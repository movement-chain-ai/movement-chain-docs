# è§†è§‰/æ‘„åƒå¤´ä¼ æ„Ÿå™¨

åŸºäºè®¡ç®—æœºè§†è§‰çš„å§¿æ€ä¼°è®¡æ˜¯é«˜å°”å¤«æŒ¥æ†åˆ†æçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡æ‘„åƒå¤´æ•æ‰å’Œ AI æ¨¡å‹å®ç°æ— æ ‡è®°éª¨éª¼è¿½è¸ªã€‚

---

## æŠ€æœ¯æ¦‚è¿°

### æˆ‘ä»¬çš„é€‰æ‹©ï¼šåˆ†é˜¶æ®µæ¼”è¿›

!!! success "é¡¹ç›®å†³ç­–"
    æ ¹æ®å¼€å‘é˜¶æ®µé€‰æ‹©æœ€é€‚åˆçš„å§¿æ€ä¼°è®¡æ¨¡å‹ï¼Œå¹³è¡¡å¼€å‘é€Ÿåº¦ä¸ç²¾åº¦éœ€æ±‚

| é˜¶æ®µ | æ¨¡å‹ | AP (COCO) | ä¸ºä»€ä¹ˆ |
|------|------|-----------|--------|
| **MVP** | MediaPipe Pose | ~70%* | `pip install mediapipe` å³ç”¨ï¼ŒGoogle æŒç»­ç»´æŠ¤ (v0.10.26) |
| Phase 2 | RTMPose-m | 75.8% | ç²¾åº¦æ›´é«˜ï¼Œéœ€é…ç½® mmpose ç”Ÿæ€ |
| Phase 3 | ViTPose++ | 81.1% | SOTAï¼Œéœ€ GPU æœåŠ¡å™¨éƒ¨ç½² |

> *MediaPipe ä½¿ç”¨ 33 å…³é”®ç‚¹ vs COCO çš„ 17 ç‚¹ï¼ŒAP ä¸ç›´æ¥å¯æ¯”

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | AP (COCO) | å»¶è¿Ÿ (ms) | æ¥æº |
|-----|-------|----------|----------|------|
| RTMPose-t | 3.3M | 68.4 | 3.2 | [arXiv:2303.07399][rtmpose-paper] |
| RTMPose-m | 13.6M | 75.8 | 5.4 | [arXiv:2303.07399][rtmpose-paper] |
| RTMPose-l | 27.7M | 76.3 | 8.1 | [arXiv:2303.07399][rtmpose-paper] |
| ViTPose++ (ViT-H) | 632M | 81.1 | GPU only | [NeurIPS'22][vitpose-paper] |
| MoveNet | 2.8M | 64.9 | 6.8 | TensorFlow Hub |
| MediaPipe | ~3M | ~70* | 8-12 | Google AI Edge |

[rtmpose-paper]: https://arxiv.org/abs/2303.07399
[vitpose-paper]: https://arxiv.org/abs/2204.12484

!!! info "ä¸ºä»€ä¹ˆç²¾åº¦ä¸åŒï¼Ÿâ€” æŠ€æœ¯æ·±åº¦è§£æ"

    **AP (Average Precision)** æ¥è‡ª [MS COCO Keypoint Benchmark](https://paperswithcode.com/sota/keypoint-detection-on-coco-test-dev)ï¼Œè¯„ä¼°é¢„æµ‹å…³é”®ç‚¹ä¸çœŸå®æ ‡æ³¨çš„è·ç¦»ã€‚

    | å› ç´  | MediaPipe | RTMPose | ViTPose++ | å½±å“ |
    |------|-----------|---------|-----------|------|
    | **Backbone** | MobileNet (åˆ†ç±»ä¼˜åŒ–) | CSPNeXt (æ£€æµ‹ä¼˜åŒ–) | ViT-Huge (Transformer) | +5-10 AP |
    | **å®šä½æ–¹æ³•** | Heatmapâ†’å›å½’ | SimCC (åæ ‡åˆ†ç±») | ç®€å•è§£ç å™¨ | +1-3 AP |
    | **æ³¨æ„åŠ›æœºåˆ¶** | æ—  | GAU (é—¨æ§æ³¨æ„åŠ›) | å…¨å±€è‡ªæ³¨æ„åŠ› | +2-4 AP |
    | **æ„Ÿå—é‡** | å±€éƒ¨ (å·ç§¯æ ¸é™åˆ¶) | ä¸­ç­‰ | å…¨å±€ (æ•´å›¾æ³¨æ„åŠ›) | +3-5 AP |
    | **è®­ç»ƒç­–ç•¥** | å•é˜¶æ®µ | ä¸¤é˜¶æ®µ+è’¸é¦ | å¤§è§„æ¨¡é¢„è®­ç»ƒ+MAE | +1-3 AP |

    **å…³é”®å·®å¼‚**:

    - **SimCC vs Heatmap**: RTMPose å°†åæ ‡é¢„æµ‹è½¬ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œé¿å… Heatmap é‡åŒ–è¯¯å·®
    - **Vision Transformer**: ViTPose ç”¨å…¨å±€æ³¨æ„åŠ›æ•æ‰é•¿è·ç¦»ä¾èµ–ï¼ŒCNN éš¾ä»¥åšåˆ°
    - **Trade-off**: ç²¾åº¦è¶Šé«˜ â†’ è®¡ç®—è¶Šé‡ â†’ ç§»åŠ¨ç«¯è¶Šéš¾éƒ¨ç½²

### éª¨éª¼å…³é”®ç‚¹

```text
                RTMPose 17 å…³é”®ç‚¹

                    0 (nose)
                      â”‚
            1 â”€â”€â”€â”€â”€â”€â”€ 2 â”€â”€â”€â”€â”€â”€ 3
          (L eye)  (R eye)  (L ear)
                      â”‚
                      4 (R ear)
                      â”‚
           5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€ 6
         (L shoulder) â”‚      (R shoulder)
              â”‚       â”‚           â”‚
              7       â”‚           8
           (L elbow)  â”‚        (R elbow)
              â”‚       â”‚           â”‚
              9       â”‚          10
          (L wrist)   â”‚       (R wrist)
                      â”‚
          11 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€ 12
         (L hip)      â”‚       (R hip)
              â”‚       â”‚           â”‚
             13       â”‚          14
          (L knee)    â”‚       (R knee)
              â”‚       â”‚           â”‚
             15       â”‚          16
         (L ankle)    â”‚       (R ankle)

        å…± 17 å…³é”®ç‚¹ Ã— 2 åæ ‡ = 34D ç‰¹å¾å‘é‡
```

---

## è§£å†³æ–¹æ¡ˆå¯¹æ¯”

### ç«¯ä¾§éƒ¨ç½²æ–¹æ¡ˆ

| æ–¹æ¡ˆ | æ¨¡å‹ | å¹³å° | ç‰¹ç‚¹ | é€‚ç”¨é˜¶æ®µ |
|-----|------|-----|------|---------|
| **MediaPipe** | BlazePose | è·¨å¹³å° | æ˜“é›†æˆã€Google ç»´æŠ¤ | âœ… **MVP æ¨è** |
| **RTMPose + ONNX** | RTMPose-m | iOS/Android | é«˜ç²¾åº¦ã€å¼€æº | Phase 2 |
| **MoveNet + TFLite** | MoveNet | iOS/Android | è°·æ­Œç”Ÿæ€ | TF é¡¹ç›® |
| **Apple Vision** | åŸç”Ÿ | iOS only | ç³»ç»Ÿé›†æˆ | iOS ä¸“å± |

### äº‘ç«¯æ–¹æ¡ˆ

| æ–¹æ¡ˆ | æä¾›å•† | ç‰¹ç‚¹ | ä»·æ ¼ |
|-----|-------|-----|------|
| **Sency AI** | Sency | è¿åŠ¨åˆ†æ | å…è´¹å±‚ + å•†ä¸š |
| **LightBuzz** | LightBuzz | å¥èº«åˆ†æ | $0.01/æ¬¡ |
| **AWS Rekognition** | Amazon | é€šç”¨è§†è§‰ | $0.001/å›¾ |

### ä¸“ä¸šåŠ¨æ•æ–¹æ¡ˆ

| æ–¹æ¡ˆ | ä»·æ ¼ | ç²¾åº¦ | é€‚ç”¨åœºæ™¯ |
|-----|------|-----|---------|
| **OptiTrack** | $10,000+ | äºšæ¯«ç±³ | ä¸“ä¸šåŠ¨æ• |
| **Vicon** | $50,000+ | äºšæ¯«ç±³ | ç ”ç©¶/å½±è§† |
| **Xsens** | $5,000+ | é«˜ | æ— æ ‡è®°åŠ¨æ• |
| **iPhone LiDAR** | $1,000 | ä¸­ç­‰ | æ¶ˆè´¹çº§ 3D |

---

## æ•°æ®è®¿é—®

### MediaPipe (MVP æ¨è)

```python
import mediapipe as mp
import cv2

mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

def process_video(video_path):
    cap = cv2.VideoCapture(video_path)

    with mp_pose.Pose(
        static_image_mode=False,
        model_complexity=1,  # 0=Lite, 1=Full, 2=Heavy
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as pose:

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # è½¬æ¢ BGR -> RGB
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(rgb_frame)

            if results.pose_landmarks:
                # 33 ä¸ªå…³é”®ç‚¹ (MediaPipe æ¯” COCO å¤š)
                landmarks = results.pose_landmarks.landmark

                # æå–é«˜å°”å¤«å…³é”®å…³èŠ‚
                left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]
                right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]
                left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW]
                right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW]

                # ç»˜åˆ¶éª¨éª¼
                mp_drawing.draw_landmarks(
                    frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

            cv2.imshow('Pose', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    cap.release()
```

### RTMPose + ONNX (Phase 2)

```python
import onnxruntime as ort
import cv2
import numpy as np

class PoseEstimator:
    def __init__(self, model_path='rtmpose-m.onnx'):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.input_shape = (256, 192)  # RTMPose è¾“å…¥å°ºå¯¸

    def preprocess(self, image):
        """å›¾åƒé¢„å¤„ç†"""
        img = cv2.resize(image, self.input_shape)
        img = img.astype(np.float32) / 255.0
        img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]
        img = img.transpose(2, 0, 1)
        return np.expand_dims(img, 0)

    def inference(self, image):
        """æ¨ç†è·å–å…³é”®ç‚¹"""
        input_data = self.preprocess(image)
        outputs = self.session.run(None, {self.input_name: input_data})

        # è¾“å‡º: [1, 17, 3] - 17 ä¸ªå…³é”®ç‚¹, æ¯ä¸ª (x, y, confidence)
        keypoints = outputs[0][0]
        return keypoints

    def get_feature_vector(self, keypoints):
        """è½¬æ¢ä¸º 34D ç‰¹å¾å‘é‡"""
        # æå– x, y åæ ‡, å¿½ç•¥ç½®ä¿¡åº¦
        return keypoints[:, :2].flatten()  # shape: (34,)

# ä½¿ç”¨ç¤ºä¾‹
estimator = PoseEstimator()
frame = cv2.imread('golf_swing.jpg')
keypoints = estimator.inference(frame)
features = estimator.get_feature_vector(keypoints)
print(f"Feature vector shape: {features.shape}")  # (34,)
```

### Flutter + ONNX Runtime (Phase 2)

```dart
import 'package:onnxruntime/onnxruntime.dart';
import 'package:camera/camera.dart';

class PoseEstimator {
  late OrtSession _session;

  Future<void> initialize() async {
    OrtEnv.instance.init();
    final sessionOptions = OrtSessionOptions();
    final modelPath = 'assets/rtmpose-m.onnx';
    _session = OrtSession.fromAsset(modelPath, sessionOptions);
  }

  Future<List<List<double>>> estimate(CameraImage image) async {
    // é¢„å¤„ç†å›¾åƒ
    final inputTensor = _preprocessImage(image);

    // æ¨ç†
    final inputs = {'input': inputTensor};
    final outputs = await _session.runAsync(inputs);

    // è§£æè¾“å‡º
    final keypoints = outputs['output']!.value as List<List<List<double>>>;
    return keypoints[0];  // 17 x 3
  }

  List<double> getFeatureVector(List<List<double>> keypoints) {
    return keypoints.expand((kp) => [kp[0], kp[1]]).toList();  // 34D
  }
}
```

---

## é«˜å°”å¤«æŒ¥æ†åº”ç”¨

### å…³é”®è§’åº¦æµ‹é‡

| è§’åº¦ | è®¡ç®—æ–¹æ³• | å…¸å‹å€¼ | æ„ä¹‰ |
|-----|---------|-------|------|
| **è„ŠæŸ±è§’åº¦** | è‚©-é«‹è¿çº¿ vs å‚ç›´ | 25-35Â° | èº«ä½“å€¾æ–œ |
| **é«‹æ—‹è½¬** | å·¦é«‹-å³é«‹è¿çº¿è§’åº¦ | 0-45Â° | è½¬ä½“å¹…åº¦ |
| **è‚©æ—‹è½¬** | å·¦è‚©-å³è‚©è¿çº¿è§’åº¦ | 0-90Â° | ä¸Šèº«æ—‹è½¬ |
| **æ‰‹è‡‚è§’åº¦** | è‚©-è‚˜-è…•å¤¹è§’ | 90-180Â° | æ‰‹è‡‚ä¼¸å±• |
| **è†ç›–å¼¯æ›²** | é«‹-è†-è¸å¤¹è§’ | 140-170Â° | ç¨³å®šæ€§ |

### è§’åº¦è®¡ç®—ä»£ç 

```python
import numpy as np

def calculate_angle(p1, p2, p3):
    """è®¡ç®—ä¸‰ç‚¹å¤¹è§’ (p2 ä¸ºé¡¶ç‚¹)"""
    v1 = np.array(p1) - np.array(p2)
    v2 = np.array(p3) - np.array(p2)

    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
    angle = np.arccos(np.clip(cos_angle, -1, 1))
    return np.degrees(angle)

def calculate_golf_angles(keypoints):
    """è®¡ç®—é«˜å°”å¤«å…³é”®è§’åº¦"""
    # keypoints ç´¢å¼• (COCO æ ¼å¼)
    L_SHOULDER, R_SHOULDER = 5, 6
    L_ELBOW, R_ELBOW = 7, 8
    L_WRIST, R_WRIST = 9, 10
    L_HIP, R_HIP = 11, 12
    L_KNEE, R_KNEE = 13, 14
    L_ANKLE, R_ANKLE = 15, 16

    angles = {}

    # å·¦è‡‚è§’åº¦ (è‚©-è‚˜-è…•)
    angles['left_arm'] = calculate_angle(
        keypoints[L_SHOULDER], keypoints[L_ELBOW], keypoints[L_WRIST])

    # å³è‡‚è§’åº¦
    angles['right_arm'] = calculate_angle(
        keypoints[R_SHOULDER], keypoints[R_ELBOW], keypoints[R_WRIST])

    # å·¦è†è§’åº¦ (é«‹-è†-è¸)
    angles['left_knee'] = calculate_angle(
        keypoints[L_HIP], keypoints[L_KNEE], keypoints[L_ANKLE])

    # é«‹æ—‹è½¬ (éœ€è¦3Dæˆ–å¤šè§†è§’)
    hip_center = (np.array(keypoints[L_HIP]) + np.array(keypoints[R_HIP])) / 2
    shoulder_center = (np.array(keypoints[L_SHOULDER]) + np.array(keypoints[R_SHOULDER])) / 2

    # è„ŠæŸ±å€¾æ–œè§’åº¦
    spine_vector = shoulder_center - hip_center
    vertical = np.array([0, -1])  # å›¾åƒåæ ‡ç³»ï¼Œy å‘ä¸‹
    angles['spine_tilt'] = calculate_angle(
        shoulder_center, hip_center, hip_center + vertical * 100)

    return angles
```

### æŒ¥æ†é˜¶æ®µæ£€æµ‹

```python
class SwingPhaseDetector:
    """åŸºäºå§¿æ€ç‰¹å¾çš„æŒ¥æ†é˜¶æ®µæ£€æµ‹"""

    PHASES = ['address', 'backswing', 'top', 'downswing', 'impact', 'follow']

    def __init__(self):
        self.prev_wrist_y = None
        self.wrist_velocities = []

    def detect_phase(self, keypoints, timestamp):
        """æ£€æµ‹å½“å‰æŒ¥æ†é˜¶æ®µ"""
        # æå–å…³é”®ç‚¹
        left_wrist = keypoints[9]
        right_wrist = keypoints[10]
        left_hip = keypoints[11]

        # è®¡ç®—æ‰‹è…•é«˜åº¦ç›¸å¯¹äºé«‹éƒ¨
        wrist_height = (left_wrist[1] + right_wrist[1]) / 2 - left_hip[1]

        # è®¡ç®—é€Ÿåº¦
        if self.prev_wrist_y is not None:
            velocity = wrist_height - self.prev_wrist_y
            self.wrist_velocities.append(velocity)

        self.prev_wrist_y = wrist_height

        # åŸºäºè§„åˆ™çš„é˜¶æ®µåˆ¤æ–­
        if len(self.wrist_velocities) < 3:
            return 'address'

        avg_velocity = np.mean(self.wrist_velocities[-3:])

        if avg_velocity < -5:  # å‘ä¸Šç§»åŠ¨ (å›¾åƒåæ ‡ y å‘ä¸‹)
            return 'backswing'
        elif avg_velocity > 5:  # å‘ä¸‹ç§»åŠ¨
            if wrist_height > 0:  # æ‰‹è…•é«˜äºé«‹éƒ¨
                return 'downswing'
            else:
                return 'follow'
        else:
            if wrist_height < -50:  # æ‰‹è…•åœ¨æœ€é«˜ç‚¹
                return 'top'
            elif abs(wrist_height) < 10:
                return 'impact'
            else:
                return 'address'
```

---

## æŠ€æœ¯è§„æ ¼

### ç¡¬ä»¶è¦æ±‚

| å¹³å° | æœ€ä½é…ç½® | æ¨èé…ç½® | å¸§ç‡ |
|-----|---------|---------|-----|
| **iOS** | A12 | A14+ | 30+ FPS |
| **Android** | Snapdragon 730 | Snapdragon 870+ | 30+ FPS |
| **æ¡Œé¢** | Core i5 | Core i7 + GPU | 60+ FPS |

### æ‘„åƒå¤´è§„æ ¼

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|-----|-------|------|
| **åˆ†è¾¨ç‡** | 1080p | è¶³å¤Ÿå§¿æ€ä¼°è®¡ |
| **å¸§ç‡** | 60 FPS | æ•æ‰å¿«é€ŸæŒ¥æ† |
| **è§†è§’** | å¹¿è§’ | å…¨èº«å…¥é•œ |
| **å¿«é—¨** | å…¨å±€/é«˜é€Ÿ | å‡å°‘è¿åŠ¨æ¨¡ç³Š |

### æ‹æ‘„å»ºè®®

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    é«˜å°”å¤«æŒ¥æ†æ‹æ‘„å¸ƒå±€                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚     è§†è§’ 1: æ­£é¢ (Face-On)                                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚     â”‚      ğŸ“·              â”‚  è·ç¦»: 3-5m                    â”‚
â”‚     â”‚       â”‚              â”‚  é«˜åº¦: 1m (è…°éƒ¨é«˜åº¦)           â”‚
â”‚     â”‚       â”‚              â”‚  ç”¨é€”: é‡å¿ƒè½¬ç§»ã€é«‹æ—‹è½¬        â”‚
â”‚     â”‚       â–¼              â”‚                                â”‚
â”‚     â”‚      ğŸŒï¸              â”‚                                â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                             â”‚
â”‚     è§†è§’ 2: åæ–¹ (Down-the-Line)                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚     â”‚  ğŸŒï¸ â†â”€â”€â”€â”€â”€â”€ ğŸ“·       â”‚  è·ç¦»: 3-5m                    â”‚
â”‚     â”‚                      â”‚  é«˜åº¦: 1m                      â”‚
â”‚     â”‚                      â”‚  ç”¨é€”: æŒ¥æ†å¹³é¢ã€æ†å¤´è·¯å¾„      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                             â”‚
â”‚     å…‰çº¿: å‡åŒ€ç…§æ˜ï¼Œé¿å…èƒŒå…‰                                 â”‚
â”‚     èƒŒæ™¯: ç®€æ´å•è‰²èƒŒæ™¯æœ€ä½³                                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä¾›åº”å•†ä¿¡æ¯

### SDK/API æä¾›å•†

| æä¾›å•† | äº§å“ | ä»·æ ¼ | ç‰¹ç‚¹ |
|-------|------|------|-----|
| **MMPose** | RTMPose | å…è´¹å¼€æº | é«˜ç²¾åº¦ |
| **Google** | MediaPipe | å…è´¹ | è·¨å¹³å° |
| **Sency AI** | Movement SDK | å…è´¹å±‚ | è¿åŠ¨åˆ†æ |
| **LightBuzz** | Pose AI | $0.01/æ¬¡ | å¥èº«ä¸“ç”¨ |

### ç¡¬ä»¶ä¾›åº”å•†

| ä¾›åº”å•† | äº§å“ | ä»·æ ¼ | ç”¨é€” |
|-------|------|------|-----|
| **GoPro** | Hero 12 | $400 | é«˜å¸§ç‡ |
| **Insta360** | One RS | $300 | 360Â° |
| **Intel** | RealSense | $200+ | æ·±åº¦ç›¸æœº |

è¯¦ç»†ä¾›åº”å•†ä¿¡æ¯è¯·å‚è§ [è§†è§‰æ–¹æ¡ˆç«å“](../../product/market-landscape/competitors/vision-based.md)

---

## ç›¸å…³èµ„æº

- [å§¿æ€ä¼°è®¡æŒ‡å—](../vision/software.md)
- [ADR-0006: ONNX Runtime éƒ¨ç½²](../../design/decisions/0006-onnx-runtime-deployment.md)
- [è§†è§‰æ–¹æ¡ˆç«å“](../../product/market-landscape/competitors/vision-based.md)

---

**æœ€åæ›´æ–°**: 2025 å¹´ 12 æœˆ 12 æ—¥
