# Datasets & Benchmarking Guide æ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•æŒ‡å—

> Actionable guide to datasets, benchmarks, and performance metrics for Movement Chain AI development
> Movement Chain AIå¼€å‘çš„æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œæ€§èƒ½æŒ‡æ ‡å¯æ“ä½œæŒ‡å—

---

## Overview æ¦‚è¿°

This guide provides **concrete actions** for accessing datasets, benchmarking your system, and comparing against industry standards. Every section includes download links, application processes, and performance targets.

æœ¬æŒ‡å—æä¾›è®¿é—®æ•°æ®é›†ã€å¯¹ç³»ç»Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•ä»¥åŠä¸è¡Œä¸šæ ‡å‡†æ¯”è¾ƒçš„**å…·ä½“è¡ŒåŠ¨**ã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½åŒ…å«ä¸‹è½½é“¾æ¥ã€ç”³è¯·æµç¨‹å’Œæ€§èƒ½ç›®æ ‡ã€‚

**Quick Navigation å¿«é€Ÿå¯¼èˆª**:

- [Priority Datasets ä¼˜å…ˆæ•°æ®é›†](#1-priority-datasets-download-apply-now-ä¼˜å…ˆæ•°æ®é›†-ç«‹å³ä¸‹è½½ç”³è¯·) - What to download/apply for NOW ç«‹å³ä¸‹è½½/ç”³è¯·çš„å†…å®¹
- [Benchmark Results åŸºå‡†æµ‹è¯•ç»“æœ](#2-benchmark-results--comparisons-åŸºå‡†æµ‹è¯•ç»“æœä¸æ¯”è¾ƒ) - Performance targets æ€§èƒ½ç›®æ ‡
- [Commercial Metrics å•†ä¸šæŒ‡æ ‡](#3-commercial-performance-metrics-å•†ä¸šæ€§èƒ½æŒ‡æ ‡) - What competitors report ç«äº‰å¯¹æ‰‹æŠ¥å‘Šçš„å†…å®¹
- [Evaluation Standards è¯„ä¼°æ ‡å‡†](#4-evaluation-metrics--standards-è¯„ä¼°æŒ‡æ ‡ä¸æ ‡å‡†) - How to measure å¦‚ä½•æµ‹é‡
- [Usage Strategy ä½¿ç”¨ç­–ç•¥](#5-dataset-usage-strategy-phased-æ•°æ®é›†ä½¿ç”¨ç­–ç•¥-åˆ†é˜¶æ®µ) - When to use each dataset ä½•æ—¶ä½¿ç”¨æ¯ä¸ªæ•°æ®é›†
- [Publication Targets å‘è¡¨ç›®æ ‡](#6-publication--research-targets-å‘è¡¨ä¸ç ”ç©¶ç›®æ ‡) - Where to publish åœ¨å“ªé‡Œå‘è¡¨
- [Citations å¼•ç”¨](#7-citation-requirements-å¼•ç”¨è¦æ±‚) - How to cite properly å¦‚ä½•æ­£ç¡®å¼•ç”¨
- [Curated Resources ç²¾é€‰èµ„æº](#8-curated-research--resource-lists-ç²¾é€‰ç ”ç©¶ä¸èµ„æºåˆ—è¡¨) - Research aggregation lists ç ”ç©¶æ±‡æ€»åˆ—è¡¨

---

## 1. Priority Datasets (Download/Apply Now!) ä¼˜å…ˆæ•°æ®é›†ï¼ˆç«‹å³ä¸‹è½½/ç”³è¯·ï¼ï¼‰

### 1.1 Fit3D (Google/CMU) - APPLY FOR ACCESS ç”³è¯·è®¿é—®æƒé™ ğŸ”´

**Status çŠ¶æ€**: Application required (typically 1-2 weeks approval éœ€è¦ç”³è¯·ï¼Œé€šå¸¸1-2å‘¨æ‰¹å‡†)

#### What It Is ç®€ä»‹

- **Scale è§„æ¨¡**: 3+ million images with professional motion capture 300ä¸‡+å›¾åƒé…ä¸“ä¸šåŠ¨ä½œæ•æ‰
- **Coverage è¦†ç›–**: 37+ repetitive fitness exercises across all major muscle groups 37+é‡å¤æ€§å¥èº«ç»ƒä¹ æ¶µç›–æ‰€æœ‰ä¸»è¦è‚Œç¾¤
- **Quality è´¨é‡**: Research-grade MoCap with corresponding RGB images ç ”ç©¶çº§MoCapé…å¯¹åº”RGBå›¾åƒ
- **Participants å‚ä¸è€…**: Expert trainers + learners (multiple skill levels ä¸“å®¶æ•™ç»ƒ+å­¦ä¹ è€…ï¼Œå¤šæŠ€èƒ½æ°´å¹³)
- **Source æ¥æº**: Google Research / CMU collaboration (CVPR 2021)

#### Why This Is Critical ä¸ºä»€ä¹ˆè¿™å¾ˆå…³é”®

**Gold Standard for Benchmarking åŸºå‡†æµ‹è¯•é»„é‡‘æ ‡å‡†**:

- Industry reference for automatic fitness feedback systems è‡ªåŠ¨å¥èº«åé¦ˆç³»ç»Ÿçš„è¡Œä¸šå‚è€ƒ
- Natural language feedback generation examples è‡ªç„¶è¯­è¨€åé¦ˆç”Ÿæˆç¤ºä¾‹
- Multi-level feedback design (visual + language å¤šçº§åé¦ˆè®¾è®¡ï¼Œè§†è§‰+è¯­è¨€)
- Adjustable feedback strictness (beginner â†’ advanced å¯è°ƒåé¦ˆä¸¥æ ¼åº¦ï¼Œåˆçº§â†’é«˜çº§)

**Citation Count å¼•ç”¨æ¬¡æ•°**: 100+ citations (highly influential é«˜åº¦å½±å“åŠ›)

#### How to Apply å¦‚ä½•ç”³è¯·

**Step-by-step Process åˆ†æ­¥æµç¨‹**:

1. **Visit Application Portal è®¿é—®ç”³è¯·é—¨æˆ·**: [https://fit3d.imar.ro/](https://fit3d.imar.ro/)

2. **Prepare Application Materials å‡†å¤‡ç”³è¯·ææ–™**:
   - Institution affiliation æœºæ„éš¶å±å…³ç³»
   - Research project description ç ”ç©¶é¡¹ç›®æè¿°
   - Intended use case é¢„æœŸç”¨ä¾‹
   - Data usage agreement acceptance æ•°æ®ä½¿ç”¨åè®®æ¥å—

3. **What to Emphasize in Application ç”³è¯·ä¸­å¼ºè°ƒçš„å†…å®¹**:
   - Academic/research project status å­¦æœ¯/ç ”ç©¶é¡¹ç›®çŠ¶æ€
   - Novel multimodal approach æ–°é¢–çš„å¤šæ¨¡æ€æ–¹æ³•: "EMG + IMU + Vision fusion for movement training EMG+IMU+è§†è§‰èåˆç”¨äºåŠ¨ä½œè®­ç»ƒ"
   - Open-source contribution goals å¼€æºè´¡çŒ®ç›®æ ‡
   - Benchmark comparison with AIFit methodology ä¸AIFitæ–¹æ³•çš„åŸºå‡†æ¯”è¾ƒ
   - Publication intentions (CHI, IMWUT, CVPR å‘è¡¨æ„å‘)

4. **Expected Timeline é¢„æœŸæ—¶é—´çº¿**: 1-2 weeks for approval æ‰¹å‡†éœ€1-2å‘¨

5. **After Approval æ‰¹å‡†å**:
   - Sign data usage agreement ç­¾ç½²æ•°æ®ä½¿ç”¨åè®®
   - Receive download credentials æ”¶åˆ°ä¸‹è½½å‡­è¯
   - Access dataset tools è®¿é—®æ•°æ®é›†å·¥å…·: [GitHub - Dataset Tools](https://github.com/sminchisescu-research/imar_vision_datasets_tools)

#### Dataset Structure æ•°æ®é›†ç»“æ„

```
fit3d/
â”œâ”€â”€ images/              # 3M+ RGB images 300ä¸‡+RGBå›¾åƒ
â”œâ”€â”€ mocap/               # 3D motion capture data 3DåŠ¨ä½œæ•æ‰æ•°æ®
â”œâ”€â”€ annotations/         # Exercise labels, rep boundaries ç»ƒä¹ æ ‡ç­¾ã€æ¬¡æ•°è¾¹ç•Œ
â”œâ”€â”€ feedback_examples/   # Natural language feedback samples è‡ªç„¶è¯­è¨€åé¦ˆæ ·æœ¬
â””â”€â”€ metadata/           # Participant info, skill levels å‚ä¸è€…ä¿¡æ¯ã€æŠ€èƒ½æ°´å¹³
```

#### How to Use for Validation å¦‚ä½•ç”¨äºéªŒè¯

**Phase 1 (MVP) MVPé˜¶æ®µ**: Not needed yet æš‚ä¸éœ€è¦
**Phase 2 (Months 3-6) ç¬¬3-6ä¸ªæœˆ**:

- Benchmark pose estimation accuracy åŸºå‡†æµ‹è¯•å§¿æ€ä¼°è®¡ç²¾åº¦
- Compare feedback generation quality æ¯”è¾ƒåé¦ˆç”Ÿæˆè´¨é‡
- Validate multi-level feedback system éªŒè¯å¤šçº§åé¦ˆç³»ç»Ÿ

**Phase 3 (Research) ç ”ç©¶é˜¶æ®µ**:

- Primary comparison dataset for papers è®ºæ–‡çš„ä¸»è¦æ¯”è¾ƒæ•°æ®é›†
- Establish baseline vs. AIFit system å»ºç«‹ä¸AIFitç³»ç»Ÿçš„åŸºå‡†
- Demonstrate improvements å±•ç¤ºæ”¹è¿›

**Resources èµ„æº**:

- **Paper è®ºæ–‡**: [AIFit: Automatic 3D Human-Interpretable Feedback Models (CVPR 2021)](https://openaccess.thecvf.com/content/CVPR2021/html/Fieraru_AIFit_Automatic_3D_Human-Interpretable_Feedback_Models_for_Fitness_Training_CVPR_2021_paper.html)
- **Code ä»£ç **: [GitHub - imar_vision_datasets_tools](https://github.com/sminchisescu-research/imar_vision_datasets_tools)

---

### 1.2 MM-Fit - DOWNLOAD IMMEDIATELY ç«‹å³ä¸‹è½½ ğŸ”´

**Status çŠ¶æ€**: Publicly available - NO application needed å…¬å¼€å¯ç”¨ï¼Œæ— éœ€ç”³è¯·

#### What It Is ç®€ä»‹

**The Perfect Match for Our Project æˆ‘ä»¬é¡¹ç›®çš„å®Œç¾åŒ¹é…**:

- **Multimodal data å¤šæ¨¡æ€æ•°æ®**: IMU sensors + RGB-D video + 3D pose IMUä¼ æ„Ÿå™¨+RGB-Dè§†é¢‘+3Då§¿æ€
- **Time-synchronized æ—¶é—´åŒæ­¥**: All modalities aligned æ‰€æœ‰æ¨¡æ€å¯¹é½
- **Sensor Types ä¼ æ„Ÿå™¨ç±»å‹**: Smartphone IMU æ™ºèƒ½æ‰‹æœºIMU, smartwatch IMU æ™ºèƒ½æ‰‹è¡¨IMU, earbud IMU è€³å¡IMU
- **Visual Data è§†è§‰æ•°æ®**: Multi-view RGB-D video + 2D/3D pose estimation å¤šè§†è§’RGB-Dè§†é¢‘+2D/3Då§¿æ€ä¼°è®¡
- **Coverage è¦†ç›–**: Various gym exercises, multiple participants å„ç§å¥èº«æˆ¿ç»ƒä¹ ï¼Œå¤šä½å‚ä¸è€…

**Publication å‘è¡¨**: IMWUT 2020 (Top Ubicomp Journal é¡¶çº§æ³›åœ¨è®¡ç®—æœŸåˆŠ)

#### Why This Is Critical for Us ä¸ºä»€ä¹ˆè¿™å¯¹æˆ‘ä»¬è‡³å…³é‡è¦

ğŸ¯ **Exact Validation Dataset We Need æˆ‘ä»¬éœ€è¦çš„ç¡®åˆ‡éªŒè¯æ•°æ®é›†**:

âœ… **Has IMU data æœ‰IMUæ•°æ®** - Just like our wearable sensor å°±åƒæˆ‘ä»¬çš„å¯ç©¿æˆ´ä¼ æ„Ÿå™¨
âœ… **Has vision data æœ‰è§†è§‰æ•°æ®** - Just like our mobile app camera å°±åƒæˆ‘ä»¬çš„ç§»åŠ¨åº”ç”¨ç›¸æœº
âœ… **Has ground truth pose æœ‰çœŸå®å§¿æ€** - For evaluation ç”¨äºè¯„ä¼°
âœ… **All time-synchronized å…¨éƒ¨æ—¶é—´åŒæ­¥** - Solves sync challenges è§£å†³åŒæ­¥æŒ‘æˆ˜
âœ… **Publicly available å…¬å¼€å¯ç”¨** - No access barriers æ— è®¿é—®éšœç¢

#### Download & Setup ä¸‹è½½ä¸è®¾ç½®

**Immediate Actions ç«‹å³è¡ŒåŠ¨**:

1. **Clone Repository å…‹éš†ä»“åº“**:

   ```bash
   git clone https://github.com/KDMStromback/mm-fit.git
   cd mm-fit
   ```

2. **Download Dataset ä¸‹è½½æ•°æ®é›†**:
   - Visit è®¿é—®: [https://mmfit.github.io/](https://mmfit.github.io/)
   - Follow download instructions (typically Google Drive/AWS S3 éµå¾ªä¸‹è½½è¯´æ˜ï¼Œé€šå¸¸æ˜¯Google Drive/AWS S3)
   - Expected size é¢„æœŸå¤§å°: ~50-100 GB (plan storage accordingly ç›¸åº”è§„åˆ’å­˜å‚¨)

3. **Data Format æ•°æ®æ ¼å¼**:

   ```
   mm-fit/
   â”œâ”€â”€ imu_data/          # HDF5/CSV format
   â”‚   â”œâ”€â”€ smartphone/
   â”‚   â”œâ”€â”€ smartwatch/
   â”‚   â””â”€â”€ earbuds/
   â”œâ”€â”€ video/             # MP4/AVI files
   â”‚   â”œâ”€â”€ rgb/
   â”‚   â””â”€â”€ depth/
   â”œâ”€â”€ pose/              # JSON/NPY keypoints å…³é”®ç‚¹
   â”‚   â”œâ”€â”€ 2d_keypoints/
   â”‚   â””â”€â”€ 3d_pose/
   â””â”€â”€ annotations/       # Exercise labels, timestamps ç»ƒä¹ æ ‡ç­¾ã€æ—¶é—´æˆ³
   ```

4. **Setup Environment è®¾ç½®ç¯å¢ƒ**:

   ```bash
   pip install h5py pandas opencv-python numpy
   ```

#### How to Use Immediately å¦‚ä½•ç«‹å³ä½¿ç”¨

**Week 1-2 Tasks ç¬¬1-2å‘¨ä»»åŠ¡**:

```python
# Quick validation script å¿«é€ŸéªŒè¯è„šæœ¬
import h5py
import pandas as pd

# Load IMU data åŠ è½½IMUæ•°æ®
imu_data = pd.read_csv('mm-fit/imu_data/smartphone/exercise_01.csv')

# Load corresponding pose åŠ è½½å¯¹åº”å§¿æ€
pose_data = np.load('mm-fit/pose/2d_keypoints/exercise_01.npy')

# Validate time synchronization éªŒè¯æ—¶é—´åŒæ­¥
assert len(imu_data) == len(pose_data), "Sync check åŒæ­¥æ£€æŸ¥"

# Test your sensor fusion approach æµ‹è¯•æ‚¨çš„ä¼ æ„Ÿå™¨èåˆæ–¹æ³•
```

**Direct Applications ç›´æ¥åº”ç”¨**:

1. âœ… Validate sensor fusion approach éªŒè¯ä¼ æ„Ÿå™¨èåˆæ–¹æ³• (IMU + Vision)
2. âœ… Test pose estimation pipeline accuracy æµ‹è¯•å§¿æ€ä¼°è®¡ç®¡é“ç²¾åº¦
3. âœ… Benchmark multimodal learning models åŸºå‡†æµ‹è¯•å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹
4. âœ… Reference time synchronization methods å‚è€ƒæ—¶é—´åŒæ­¥æ–¹æ³•
5. âœ… Compare against published baselines ä¸å·²å‘è¡¨åŸºçº¿æ¯”è¾ƒ

**License è®¸å¯**: Academic use permitted (verify current terms in repo å…è®¸å­¦æœ¯ä½¿ç”¨ï¼Œåœ¨ä»“åº“ä¸­éªŒè¯å½“å‰æ¡æ¬¾)

**Resources èµ„æº**:

- **Website ç½‘ç«™**: [https://mmfit.github.io/](https://mmfit.github.io/)
- **GitHub**: [https://github.com/KDMStromback/mm-fit](https://github.com/KDMStromback/mm-fit)
- **Paper è®ºæ–‡**: [MM-Fit: Multimodal Deep Learning for Automatic Exercise Logging (IMWUT 2020)](https://dl.acm.org/doi/10.1145/3397309)

---

### 1.3 FLAG3D - DOWNLOAD NOW ç«‹å³ä¸‹è½½ ğŸŸ¡

**Status çŠ¶æ€**: Publicly available å…¬å¼€å¯ç”¨

#### What It Is ç®€ä»‹

**Most Recent Large-Scale Fitness Dataset æœ€æ–°çš„å¤§è§„æ¨¡å¥èº«æ•°æ®é›†**:

- **Scale è§„æ¨¡**: 180,000 action sequences 18ä¸‡ä¸ªåŠ¨ä½œåºåˆ—
- **Exercises ç»ƒä¹ **: 60 complex fitness movements 60ä¸ªå¤æ‚å¥èº«åŠ¨ä½œ
- **Unique Feature ç‹¬ç‰¹åŠŸèƒ½**: Detailed natural language instruction annotations è¯¦ç»†è‡ªç„¶è¯­è¨€æŒ‡å¯¼æ³¨é‡Š
- **Publication å‘è¡¨**: CVPR 2023

#### Data Sources (3 Modalities) æ•°æ®æ¥æºï¼ˆ3ç§æ¨¡æ€ï¼‰

1. **Professional MoCap ä¸“ä¸šMoCap**:
   - 24 VICON cameras VICONç›¸æœº
   - 77 marker points æ ‡è®°ç‚¹
   - Research-grade accuracy ç ”ç©¶çº§ç²¾åº¦

2. **Synthetic Rendering åˆæˆæ¸²æŸ“**:
   - Software-generated variations è½¯ä»¶ç”Ÿæˆå˜ä½“
   - Controlled conditions å—æ§æ¡ä»¶
   - Augmentation potential å¢å¼ºæ½œåŠ›

3. **Smartphone Natural æ™ºèƒ½æ‰‹æœºè‡ªç„¶**:
   - Real-world environment capture çœŸå®ä¸–ç•Œç¯å¢ƒæ•æ‰
   - Consumer-grade quality æ¶ˆè´¹çº§è´¨é‡
   - **Matches our deployment scenario åŒ¹é…æˆ‘ä»¬çš„éƒ¨ç½²åœºæ™¯**

#### Why This Matters ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

âœ… **Natural Language Feedback Design è‡ªç„¶è¯­è¨€åé¦ˆè®¾è®¡**:

- Learn instruction phrasing patterns å­¦ä¹ æŒ‡å¯¼æªè¾æ¨¡å¼
- Train language-to-pose mapping models è®­ç»ƒè¯­è¨€åˆ°å§¿æ€æ˜ å°„æ¨¡å‹
- Reference for feedback generation åé¦ˆç”Ÿæˆå‚è€ƒ

âœ… **Diverse Data Sources å¤šæ ·åŒ–æ•°æ®æ¥æº**:

- Professional MoCap = ground truth ä¸“ä¸šMoCap = çœŸå®å€¼
- Smartphone data = realistic use case æ™ºèƒ½æ‰‹æœºæ•°æ® = ç°å®ç”¨ä¾‹
- Synthetic = data augmentation åˆæˆ = æ•°æ®å¢å¼º

#### Download & Access ä¸‹è½½ä¸è®¿é—®

1. **Project Page é¡¹ç›®é¡µé¢**: [https://andytang15.github.io/FLAG3D/](https://andytang15.github.io/FLAG3D/)

2. **Paper è®ºæ–‡**: [arXiv:2212.04638](https://arxiv.org/abs/2212.04638)

3. **Expected Use Cases é¢„æœŸç”¨ä¾‹**:
   - Train natural language feedback generation è®­ç»ƒè‡ªç„¶è¯­è¨€åé¦ˆç”Ÿæˆ
   - Test language-conditioned pose estimation æµ‹è¯•è¯­è¨€æ¡ä»¶å§¿æ€ä¼°è®¡
   - Augment training with synthetic data ä½¿ç”¨åˆæˆæ•°æ®å¢å¼ºè®­ç»ƒ

**Priority ä¼˜å…ˆçº§**: Medium (useful for Phase 2-3, not critical for MVP å¯¹ç¬¬2-3é˜¶æ®µæœ‰ç”¨ï¼Œå¯¹MVPéå…³é”®)

---

### 1.4 Microsoft RecoFit - DOWNLOAD NOW ç«‹å³ä¸‹è½½ ğŸŸ¡

**Status çŠ¶æ€**: Publicly available å…¬å¼€å¯ç”¨

#### What It Is ç®€ä»‹

**Wearable Sensor-Based Exercise Recognition åŸºäºå¯ç©¿æˆ´ä¼ æ„Ÿå™¨çš„è¿åŠ¨è¯†åˆ«**:

- **Participants å‚ä¸è€…**: 200+ people 200+äºº
- **Sensors ä¼ æ„Ÿå™¨**: Accelerometer + Gyroscope (6-axis IMU åŠ é€Ÿåº¦è®¡+é™€èºä»ªï¼Œ6è½´IMU)
- **Focus é‡ç‚¹**: Gym exercise recognition and rep counting å¥èº«æˆ¿è¿åŠ¨è¯†åˆ«å’Œæ¬¡æ•°è®¡æ•°
- **Publication å‘è¡¨**: CHI 2014 (influential HCI venue æœ‰å½±å“åŠ›çš„HCIä¼šè®®)

#### What's Included åŒ…å«å†…å®¹

- Raw IMU sensor data (CSV/MAT format åŸå§‹IMUä¼ æ„Ÿå™¨æ•°æ®ï¼ŒCSV/MATæ ¼å¼)
- Exercise type labels è¿åŠ¨ç±»å‹æ ‡ç­¾
- Rep counting annotations æ¬¡æ•°è®¡æ•°æ³¨é‡Š
- Participant demographics å‚ä¸è€…äººå£ç»Ÿè®¡

#### Why We Need This ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿™ä¸ª

âœ… **Baseline IMU-only Approach åŸºçº¿ä»…IMUæ–¹æ³•**:

- Test our wearable module independently ç‹¬ç«‹æµ‹è¯•æˆ‘ä»¬çš„å¯ç©¿æˆ´æ¨¡å—
- Validate rep counting algorithms éªŒè¯æ¬¡æ•°è®¡æ•°ç®—æ³•
- Pre-train IMU processing pipeline é¢„è®­ç»ƒIMUå¤„ç†ç®¡é“
- Compare IMU-only vs. multimodal accuracy æ¯”è¾ƒä»…IMUä¸å¤šæ¨¡æ€ç²¾åº¦

#### Download & Setup ä¸‹è½½ä¸è®¾ç½®

1. **GitHub**: [https://github.com/microsoft/Exercise-Recognition-from-Wearable-Sensors](https://github.com/microsoft/Exercise-Recognition-from-Wearable-Sensors)

2. **Quick Start å¿«é€Ÿå¼€å§‹**:

   ```bash
   git clone https://github.com/microsoft/Exercise-Recognition-from-Wearable-Sensors.git
   cd Exercise-Recognition-from-Wearable-Sensors
   ```

3. **Data Format æ•°æ®æ ¼å¼**:

   ```
   recofit/
   â”œâ”€â”€ raw_data/          # IMU time series IMUæ—¶é—´åºåˆ—
   â”œâ”€â”€ labels/            # Exercise classifications è¿åŠ¨åˆ†ç±»
   â”œâ”€â”€ rep_counts/        # Ground truth reps çœŸå®æ¬¡æ•°
   â””â”€â”€ preprocessing/     # Scripts for data cleaning æ•°æ®æ¸…æ´—è„šæœ¬
   ```

**Priority ä¼˜å…ˆçº§**: Medium-High (useful for wearable validation in MVP phase å¯¹MVPé˜¶æ®µå¯ç©¿æˆ´éªŒè¯æœ‰ç”¨)

---

### 1.5 COCO Keypoints / MPII - PRE-TRAINING DATASETS é¢„è®­ç»ƒæ•°æ®é›†

**Status çŠ¶æ€**: Publicly available - Standard benchmarks å…¬å¼€å¯ç”¨ - æ ‡å‡†åŸºå‡†

#### COCO Keypoints Dataset COCOå…³é”®ç‚¹æ•°æ®é›†

**Details è¯¦æƒ…**:

- **Scale è§„æ¨¡**: 200,000+ images with pose annotations 20ä¸‡+å¸¦å§¿æ€æ³¨é‡Šå›¾åƒ
- **Keypoints å…³é”®ç‚¹**: 17-point human pose format (industry standard 17ç‚¹äººä½“å§¿æ€æ ¼å¼ï¼Œè¡Œä¸šæ ‡å‡†)
- **Use Case ç”¨ä¾‹**: Pre-training pose estimation models é¢„è®­ç»ƒå§¿æ€ä¼°è®¡æ¨¡å‹

**Download ä¸‹è½½**:

- **Website ç½‘ç«™**: [https://cocodataset.org/#keypoints-2020](https://cocodataset.org/#keypoints-2020)
- **Quick Download å¿«é€Ÿä¸‹è½½**:

  ```bash
  # Images (~20GB)
  wget http://images.cocodataset.org/zips/train2017.zip
  wget http://images.cocodataset.org/zips/val2017.zip

  # Annotations æ³¨é‡Š (~250MB)
  wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
  ```

**Pre-trained Models Available å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹**:

- RTMPose models already trained on COCO å·²åœ¨COCOä¸Šè®­ç»ƒçš„RTMPoseæ¨¡å‹
- MediaPipe models pre-trained on COCO åœ¨COCOä¸Šé¢„è®­ç»ƒçš„MediaPipeæ¨¡å‹
- Skip training, use existing models è·³è¿‡è®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰æ¨¡å‹

#### MPII Human Pose Dataset MPIIäººä½“å§¿æ€æ•°æ®é›†

**Details è¯¦æƒ…**:

- **Scale è§„æ¨¡**: 25,000 images, 40,000+ people 2.5ä¸‡å›¾åƒï¼Œ4ä¸‡+äºº
- **Activities æ´»åŠ¨**: 410 different human activities 410ç§ä¸åŒäººç±»æ´»åŠ¨
- **Use Case ç”¨ä¾‹**: Pose estimation evaluation, robustness testing å§¿æ€ä¼°è®¡è¯„ä¼°ã€é²æ£’æ€§æµ‹è¯•

**Download ä¸‹è½½**:

- **Website ç½‘ç«™**: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
- **Size å¤§å°**: ~12 GB

**Priority ä¼˜å…ˆçº§**: Low (use pre-trained models instead of training from scratch ä½ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è€Œä¸æ˜¯ä»å¤´è®­ç»ƒ)

---

### 1.6 motion-sense Dataset - DOWNLOAD NOW ç«‹å³ä¸‹è½½ ğŸŸ¢

**Status çŠ¶æ€**: Publicly available å…¬å¼€å¯ç”¨

#### What It Is ç®€ä»‹

**Smartphone IMU-Based Activity Recognition åŸºäºæ™ºèƒ½æ‰‹æœºIMUçš„æ´»åŠ¨è¯†åˆ«**:

- **Scale è§„æ¨¡**: Multiple participants, various activities å¤šä½å‚ä¸è€…ï¼Œå„ç§æ´»åŠ¨
- **Sensors ä¼ æ„Ÿå™¨**: Accelerometer + Gyroscope (phone's built-in IMU æ‰‹æœºå†…ç½®IMU)
- **Focus é‡ç‚¹**: Using smartphone sensors for movement detection ä½¿ç”¨æ™ºèƒ½æ‰‹æœºä¼ æ„Ÿå™¨è¿›è¡Œè¿åŠ¨æ£€æµ‹
- **Activities æ´»åŠ¨**: Walking è¡Œèµ°, running è·‘æ­¥, stairs çˆ¬æ¥¼æ¢¯, and more ç­‰
- **Format æ ¼å¼**: CSV files (easy to parse æ˜“äºè§£æ)

#### What's Included åŒ…å«å†…å®¹

- Time-synchronized accelerometer data æ—¶é—´åŒæ­¥åŠ é€Ÿåº¦è®¡æ•°æ®
- Gyroscope readings é™€èºä»ªè¯»æ•°
- Activity labels æ´»åŠ¨æ ‡ç­¾
- Multiple recording sessions per participant æ¯ä½å‚ä¸è€…å¤šä¸ªå½•åˆ¶ä¼šè¯

#### Why It's Useful ä¸ºä»€ä¹ˆæœ‰ç”¨

âœ… **Smartphone IMU Validation æ™ºèƒ½æ‰‹æœºIMUéªŒè¯**:

- Benchmark our smartphone-based tracking approach åŸºå‡†æµ‹è¯•æˆ‘ä»¬åŸºäºæ™ºèƒ½æ‰‹æœºçš„è¿½è¸ªæ–¹æ³•
- Baseline for "phone-only" implementation "ä»…æ‰‹æœº"å®ç°çš„åŸºçº¿
- Compare phone IMU vs. dedicated wearable sensor performance æ¯”è¾ƒæ‰‹æœºIMUä¸ä¸“ç”¨å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ€§èƒ½
- Activity recognition baseline algorithms æ´»åŠ¨è¯†åˆ«åŸºçº¿ç®—æ³•

**Use Case ç”¨ä¾‹**:

- Validate smartphone IMU integration accuracy éªŒè¯æ™ºèƒ½æ‰‹æœºIMUé›†æˆç²¾åº¦
- Test activity classification without external sensors æµ‹è¯•æ— å¤–éƒ¨ä¼ æ„Ÿå™¨çš„æ´»åŠ¨åˆ†ç±»
- Research sensor quality differences (phone vs. wearable ç ”ç©¶ä¼ æ„Ÿå™¨è´¨é‡å·®å¼‚ï¼Œæ‰‹æœºå¯¹æ¯”å¯ç©¿æˆ´)

#### Download & Access ä¸‹è½½ä¸è®¿é—®

**Repository ä»“åº“**: <https://github.com/mmalekzadeh/motion-sense>

**Quick Start å¿«é€Ÿå¼€å§‹**:

```bash
git clone https://github.com/mmalekzadeh/motion-sense.git
cd motion-sense
```

**Data Structure æ•°æ®ç»“æ„**:

```
motion-sense/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ accelerometer/    # Phone accelerometer æ‰‹æœºåŠ é€Ÿåº¦è®¡
â”‚   â”œâ”€â”€ gyroscope/        # Phone gyroscope æ‰‹æœºé™€èºä»ª
â”‚   â””â”€â”€ labels/           # Activity annotations æ´»åŠ¨æ³¨é‡Š
â””â”€â”€ scripts/              # Preprocessing utilities é¢„å¤„ç†å·¥å…·
```

**Priority ä¼˜å…ˆçº§**: Low (useful for Phase 3 research, not critical for MVP å¯¹ç¬¬3é˜¶æ®µç ”ç©¶æœ‰ç”¨ï¼Œå¯¹MVPéå…³é”®)

---

## 1.7 Data Annotation & Labeling Tools æ•°æ®æ³¨é‡Šä¸æ ‡æ³¨å·¥å…·

If you need to create custom labeled datasets for training or evaluation, these tools are industry-standard:

å¦‚æœæ‚¨éœ€è¦ä¸ºè®­ç»ƒæˆ–è¯„ä¼°åˆ›å»ºè‡ªå®šä¹‰æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™äº›æ˜¯è¡Œä¸šæ ‡å‡†å·¥å…·ï¼š

### CVAT (Computer Vision Annotation Tool) è®¡ç®—æœºè§†è§‰æ ‡æ³¨å·¥å…·

**What it is ç®€ä»‹**:

- Open-source image and video annotation platform å¼€æºå›¾åƒå’Œè§†é¢‘æ ‡æ³¨å¹³å°
- Developed and used by Intel ç”±Intelå¼€å‘å’Œä½¿ç”¨
- Web-based interface with powerful features å…·æœ‰å¼ºå¤§åŠŸèƒ½çš„ç½‘ç»œç•Œé¢

**Key Features å…³é”®åŠŸèƒ½**:

- âœ… Video frame annotation (perfect for exercise videos è§†é¢‘å¸§æ ‡æ³¨ï¼Œå®Œç¾é€‚ç”¨äºè¿åŠ¨è§†é¢‘)
- âœ… Skeleton/keypoint labeling (pose annotation éª¨æ¶/å…³é”®ç‚¹æ ‡æ³¨ï¼Œå§¿æ€æ³¨é‡Š)
- âœ… Bounding box, polygon, segmentation support è¾¹ç•Œæ¡†ã€å¤šè¾¹å½¢ã€åˆ†å‰²æ”¯æŒ
- âœ… Multi-user collaboration for team projects å›¢é˜Ÿé¡¹ç›®å¤šç”¨æˆ·åä½œ
- âœ… Auto-annotation with AI models ä½¿ç”¨AIæ¨¡å‹è‡ªåŠ¨æ ‡æ³¨
- âœ… Export to COCO, YOLO, Pascal VOC formats å¯¼å‡ºä¸ºCOCOã€YOLOã€Pascal VOCæ ¼å¼

**When to use ä½•æ—¶ä½¿ç”¨**:

- Labeling custom exercise videos æ ‡æ³¨è‡ªå®šä¹‰è¿åŠ¨è§†é¢‘
- Creating ground truth pose data åˆ›å»ºçœŸå®å§¿æ€æ•°æ®
- Validating pose estimation outputs éªŒè¯å§¿æ€ä¼°è®¡è¾“å‡º
- Team annotation projects with multiple annotators å¤šæ ‡æ³¨å‘˜å›¢é˜Ÿæ ‡æ³¨é¡¹ç›®

**Setup è®¾ç½®**:

```bash
# Docker deployment (easiest method Dockeréƒ¨ç½²ï¼Œæœ€ç®€å•æ–¹æ³•)
docker run -it --rm -p 8080:8080 cvat/server

# Access at http://localhost:8080 è®¿é—®
# Or use cloud hosted version æˆ–ä½¿ç”¨äº‘æ‰˜ç®¡ç‰ˆæœ¬
```

**Resources èµ„æº**:

- **Website ç½‘ç«™**: <https://www.cvat.ai/>
- **GitHub**: <https://github.com/opencv/cvat>
- **License è®¸å¯**: MIT (free for commercial use å…è´¹å•†ç”¨)

**Best For æœ€é€‚åˆ**: Video pose labeling, professional annotation workflows è§†é¢‘å§¿æ€æ ‡æ³¨ã€ä¸“ä¸šæ ‡æ³¨å·¥ä½œæµ

---

### Label Studio

**What it is ç®€ä»‹**:

- Flexible multi-modal annotation platform çµæ´»çš„å¤šæ¨¡æ€æ ‡æ³¨å¹³å°
- Supports image, video, audio, text æ”¯æŒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬
- Highly customizable for specific needs é«˜åº¦å¯å®šåˆ¶ä»¥æ»¡è¶³ç‰¹å®šéœ€æ±‚

**Key Features å…³é”®åŠŸèƒ½**:

- âœ… Custom annotation interfaces (configure for specific needs è‡ªå®šä¹‰æ ‡æ³¨ç•Œé¢ï¼Œé…ç½®ç‰¹å®šéœ€æ±‚)
- âœ… ML-assisted labeling (import model predictions, correct them MLè¾…åŠ©æ ‡æ³¨ï¼Œå¯¼å…¥æ¨¡å‹é¢„æµ‹å¹¶çº æ­£)
- âœ… Time-series annotation (useful for IMU sensor data æ—¶é—´åºåˆ—æ ‡æ³¨ï¼Œå¯¹IMUä¼ æ„Ÿå™¨æ•°æ®æœ‰ç”¨)
- âœ… Integration with ML pipelines ä¸MLç®¡é“é›†æˆ
- âœ… Cloud or self-hosted deployment äº‘æˆ–è‡ªæ‰˜ç®¡éƒ¨ç½²

**When to use ä½•æ—¶ä½¿ç”¨**:

- Need flexibility beyond standard pose annotation éœ€è¦è¶…è¶Šæ ‡å‡†å§¿æ€æ ‡æ³¨çš„çµæ´»æ€§
- Annotating multiple modalities (video + IMU data simultaneously æ ‡æ³¨å¤šæ¨¡æ€ï¼Œè§†é¢‘+IMUæ•°æ®åŒæ—¶)
- ML-in-the-loop annotation workflows MLåœ¨ç¯æ ‡æ³¨å·¥ä½œæµ
- Custom annotation schemas not supported elsewhere å…¶ä»–åœ°æ–¹ä¸æ”¯æŒçš„è‡ªå®šä¹‰æ ‡æ³¨æ¨¡å¼

**Setup è®¾ç½®**:

```bash
pip install label-studio
label-studio start
# Access at http://localhost:8080 è®¿é—®
```

**Resources èµ„æº**:

- **Website ç½‘ç«™**: <https://labelstud.io/>
- **GitHub**: <https://github.com/heartexlabs/label-studio>
- **License è®¸å¯**: Apache 2.0

**Best For æœ€é€‚åˆ**: Multi-modal projects, custom labeling workflows å¤šæ¨¡æ€é¡¹ç›®ã€è‡ªå®šä¹‰æ ‡æ³¨å·¥ä½œæµ

---

### VIA (VGG Image Annotator) VGGå›¾åƒæ ‡æ³¨å™¨

**What it is ç®€ä»‹**:

- Lightweight browser-based annotation tool è½»é‡çº§åŸºäºæµè§ˆå™¨çš„æ ‡æ³¨å·¥å…·
- Developed by University of Oxford VGG group ç”±ç‰›æ´¥å¤§å­¦VGGå°ç»„å¼€å‘
- No installation required (runs in browser æ— éœ€å®‰è£…ï¼Œåœ¨æµè§ˆå™¨ä¸­è¿è¡Œ)

**Key Features å…³é”®åŠŸèƒ½**:

- âœ… Runs entirely in browser (no server needed å®Œå…¨åœ¨æµè§ˆå™¨ä¸­è¿è¡Œï¼Œæ— éœ€æœåŠ¡å™¨)
- âœ… Simple interface (fastest to learn ç®€å•ç•Œé¢ï¼Œæœ€å¿«å­¦ä¹ )
- âœ… Supports images and video æ”¯æŒå›¾åƒå’Œè§†é¢‘
- âœ… Keypoint annotation å…³é”®ç‚¹æ ‡æ³¨
- âœ… Export to JSON/CSV å¯¼å‡ºä¸ºJSON/CSV

**When to use ä½•æ—¶ä½¿ç”¨**:

- Quick small-scale annotation tasks å¿«é€Ÿå°è§„æ¨¡æ ‡æ³¨ä»»åŠ¡
- No server setup available æ— æœåŠ¡å™¨è®¾ç½®å¯ç”¨
- Solo annotation projects å•äººæ ‡æ³¨é¡¹ç›®
- Teaching/demos and quick prototypes æ•™å­¦/æ¼”ç¤ºå’Œå¿«é€ŸåŸå‹

**Setup è®¾ç½®**:

- No installation - just open HTML file in browser æ— éœ€å®‰è£…ï¼Œåªéœ€åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€HTMLæ–‡ä»¶
- Or use online version directly æˆ–ç›´æ¥ä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬

**Resources èµ„æº**:

- **Website ç½‘ç«™**: <https://www.robots.ox.ac.uk/~vgg/software/via/>
- **GitLab**: <https://gitlab.com/vgg/via>
- **License è®¸å¯**: BSD 2-Clause

**Best For æœ€é€‚åˆ**: Quick solo work, no-setup scenarios å¿«é€Ÿå•äººå·¥ä½œã€æ— è®¾ç½®åœºæ™¯

---

### Annotation Tool Comparison æ ‡æ³¨å·¥å…·æ¯”è¾ƒ

| Feature åŠŸèƒ½ | CVAT | Label Studio | VIA |
|---------|------|--------------|-----|
| **Installation å®‰è£…** | Docker/Cloud | pip/Cloud | Browser only ä»…æµè§ˆå™¨ |
| **Learning Curve å­¦ä¹ æ›²çº¿** | Medium ä¸­ç­‰ | Medium ä¸­ç­‰ | Easy ç®€å• |
| **Video Support è§†é¢‘æ”¯æŒ** | âœ… Excellent ä¼˜ç§€ | âœ… Good è‰¯å¥½ | âœ… Basic åŸºç¡€ |
| **Skeleton/Pose éª¨æ¶/å§¿æ€** | âœ… Native åŸç”Ÿ | âš ï¸ Custom config è‡ªå®šä¹‰é…ç½® | âœ… Keypoints å…³é”®ç‚¹ |
| **Multi-user å¤šç”¨æˆ·** | âœ… Yes | âœ… Yes | âŒ No |
| **ML-Assisted MLè¾…åŠ©** | âœ… Yes | âœ… Yes | âŒ No |
| **Multi-modal å¤šæ¨¡æ€** | âš ï¸ Video only ä»…è§†é¢‘ | âœ… Yes | âŒ No |
| **Best For æœ€é€‚åˆ** | Video pose labeling è§†é¢‘å§¿æ€æ ‡æ³¨ | Multi-modal projects å¤šæ¨¡æ€é¡¹ç›® | Quick solo work å¿«é€Ÿå•äººå·¥ä½œ |

### Recommendation for Movement Chain AI Movement Chain AIå»ºè®®

**Phase 1 (MVP) MVPé˜¶æ®µ**:

- Use existing datasets (MM-Fit, Fit3D ä½¿ç”¨ç°æœ‰æ•°æ®é›†)
- No annotation needed yet æš‚ä¸éœ€è¦æ ‡æ³¨

**Phase 2 (Custom data è‡ªå®šä¹‰æ•°æ®)**:

- **CVAT** for video pose annotation ç”¨äºè§†é¢‘å§¿æ€æ ‡æ³¨
- Focus on exercise-specific keypoint labeling ä¸“æ³¨äºç‰¹å®šè¿åŠ¨å…³é”®ç‚¹æ ‡æ³¨
- Team collaboration for larger datasets å›¢é˜Ÿåä½œå¤„ç†æ›´å¤§æ•°æ®é›†

**Phase 3 (Research ç ”ç©¶)**:

- **Label Studio** for multi-modal annotation ç”¨äºå¤šæ¨¡æ€æ ‡æ³¨
- Annotate video + IMU + EMG simultaneously åŒæ—¶æ ‡æ³¨è§†é¢‘+IMU+EMG
- ML-assisted workflows for efficiency ç”¨äºæ•ˆç‡çš„MLè¾…åŠ©å·¥ä½œæµ

---

## 2. Benchmark Results & Comparisons åŸºå‡†æµ‹è¯•ç»“æœä¸æ¯”è¾ƒ

### 2.1 Pose Estimation Accuracy (COCO mAP) å§¿æ€ä¼°è®¡ç²¾åº¦ï¼ˆCOCO mAPï¼‰

**Industry Standard Metric è¡Œä¸šæ ‡å‡†æŒ‡æ ‡**: COCO Average Precision (AP å¹³å‡ç²¾åº¦)

#### Top Models (2025) é¡¶çº§æ¨¡å‹ï¼ˆ2025ï¼‰

| Model æ¨¡å‹ | AP (%) | AR (%) | Year å¹´ä»½ | Status çŠ¶æ€ |
|-------|--------|--------|------|--------|
| **RTMPose-x** | **77.8** | **83.0** | 2023 | Max accuracy æœ€é«˜ç²¾åº¦ |
| **RTMPose-l** | **77.3** | **82.6** | 2023 | High accuracy é«˜ç²¾åº¦ |
| **RTMPose-m** | **75.8** | **81.2** | 2023 | â­ **Recommended æ¨è** |
| RTMPose-s | 71.7 | 77.8 | 2023 | Mobile optimized ç§»åŠ¨ä¼˜åŒ– |
| **MediaPipe Pose** | **~72.0** | **~78.0** | 2020 | â­ **MVP choice MVPé€‰æ‹©** |
| MoveNet Thunder | 72.0 | 78.5 | 2021 | TensorFlow |
| RTMPose-t | 67.1 | 73.5 | 2023 | Ultra-fast è¶…å¿« |
| MoveNet Lightning | 63.0 | 70.0 | 2021 | Web/battery ç½‘ç»œ/ç”µæ±  |

**Legend å›¾ä¾‹**:

- **AP (Average Precision å¹³å‡ç²¾åº¦)**: Overall accuracy across all keypoints æ‰€æœ‰å…³é”®ç‚¹çš„æ€»ä½“ç²¾åº¦
- **AR (Average Recall å¹³å‡å¬å›ç‡)**: Detection rate æ£€æµ‹ç‡
- â­ = Recommended for our project æ¨èç”¨äºæˆ‘ä»¬çš„é¡¹ç›®

#### What These Numbers Mean è¿™äº›æ•°å­—çš„å«ä¹‰

- **>75% AP**: Production-ready, high accuracy ç”Ÿäº§å°±ç»ªï¼Œé«˜ç²¾åº¦
- **70-75% AP**: Good for most applications é€‚ç”¨äºå¤§å¤šæ•°åº”ç”¨
- **<70% AP**: Acceptable for mobile/real-time tradeoffs å¯æ¥å—çš„ç§»åŠ¨/å®æ—¶æƒè¡¡

**Our Target æˆ‘ä»¬çš„ç›®æ ‡**: â‰¥72% AP (match or exceed MediaPipe åŒ¹é…æˆ–è¶…è¿‡MediaPipe)

---

### 2.2 Mobile Performance (FPS & Latency) ç§»åŠ¨æ€§èƒ½ï¼ˆFPSä¸å»¶è¿Ÿï¼‰

#### Speed Comparison é€Ÿåº¦æ¯”è¾ƒ

**Desktop CPU (Intel i7-11700) æ¡Œé¢CPU**:

| Model æ¨¡å‹ | FPS å¸§ç‡ | Latency (ms) å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ |
|-------|-----|--------------|
| RTMPose-t | 150+ | ~7 |
| RTMPose-m | 90+ | ~11 |
| MediaPipe | 40 | ~25 |
| MoveNet Lightning | 60 | ~17 |

**Mobile (Snapdragon 865) ç§»åŠ¨ç«¯**:

| Model æ¨¡å‹ | FPS å¸§ç‡ | Latency (ms) å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ | Battery/Hour æ¯å°æ—¶ç”µæ± æ¶ˆè€— |
|-------|-----|--------------|--------------|
| RTMPose-t | 120+ | ~8 | 8% drain |
| RTMPose-m | 70+ | ~14 | 12% drain |
| MediaPipe Lite | 40-50 | ~20-25 | 15% drain |
| **MediaPipe Full** | **30-40** | **25-33** | **18% drain** |
| MoveNet Lightning | 50+ | ~20 | 10% drain |

**GPU (NVIDIA GTX 1660 Ti)**:

| Model æ¨¡å‹ | FPS å¸§ç‡ | Latency (ms) å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ |
|-------|-----|--------------|
| RTMPose-m | 430+ | ~2.3 |
| RTMPose-t | 600+ | ~1.7 |
| MediaPipe | 120+ | ~8 |
| MoveNet Thunder | 100+ | ~10 |

#### Performance Targets æ€§èƒ½ç›®æ ‡

**MVP Phase MVPé˜¶æ®µ** (MediaPipe):

- âœ… 30-40 FPS on mobile ç§»åŠ¨ç«¯30-40 FPS
- âœ… <100ms end-to-end latency ç«¯åˆ°ç«¯å»¶è¿Ÿ<100ms
- âœ… <20% battery/hour æ¯å°æ—¶ç”µæ± æ¶ˆè€—<20%

**Production Phase ç”Ÿäº§é˜¶æ®µ** (RTMPose):

- ğŸ¯ 70+ FPS on mobile ç§»åŠ¨ç«¯70+ FPS
- ğŸ¯ <50ms latency å»¶è¿Ÿ<50ms
- ğŸ¯ <15% battery/hour æ¯å°æ—¶ç”µæ± æ¶ˆè€—<15%

---

### 2.3 Cloud Inference Costs äº‘æ¨ç†æˆæœ¬

**Cost per 1 million inference requests æ¯ç™¾ä¸‡æ¨ç†è¯·æ±‚æˆæœ¬** (AWS Lambda):

| Model æ¨¡å‹ | Compute Time è®¡ç®—æ—¶é—´ | Lambda Cost Lambdaæˆæœ¬ | GPU Cost (G4dn) GPUæˆæœ¬ | Savings èŠ‚çœ |
|-------|--------------|-------------|-----------------|---------|
| **RTMPose-t** | ~7ms | $10 | $6 | **-78%** |
| **RTMPose-m** | ~11ms | $15 | $8 | **-67%** |
| RTMPose-l | ~17ms | $25 | $12 | -44% |
| **MediaPipe** | ~25ms | $45 | $20 | Baseline åŸºçº¿ |
| MoveNet Thunder | ~30ms | $55 | $25 | +22% |

**Key Finding å…³é”®å‘ç°**:

- **RTMPose reduces cloud costs by 67-82% RTMPoseé™ä½äº‘æˆæœ¬67-82%** vs. MediaPipe
- Critical for scalability if offering cloud processing å¦‚æœæä¾›äº‘å¤„ç†å¯¹å¯æ‰©å±•æ€§è‡³å…³é‡è¦

**Cost Calculation Basis æˆæœ¬è®¡ç®—åŸºç¡€**:

- AWS Lambda pricing AWS Lambdaå®šä»·: $0.0000166667/GB-second
- Assumed 1GB memory allocation å‡è®¾1GBå†…å­˜åˆ†é…
- GPU: g4dn.xlarge @ $0.526/hour

---

### 2.4 Accuracy vs. Speed Tradeoff ç²¾åº¦ä¸é€Ÿåº¦æƒè¡¡

**Pareto Frontier å¸•ç´¯æ‰˜å‰æ²¿** (COCO AP vs. Mobile FPS):

```
Accuracy ç²¾åº¦ (AP %)
    80â”‚                              RTMPose-x
      â”‚                         RTMPose-l
      â”‚                    RTMPose-m
    75â”‚               MediaPipe
      â”‚          MoveNet Thunder
    70â”‚     RTMPose-s
      â”‚  MoveNet Lightning
    65â”‚ RTMPose-t
      â”‚
    60â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Speed é€Ÿåº¦ (FPS å¸§ç‡)
      0   50   100   150   200   250   300
```

**Sweet Spot æœ€ä½³å¹³è¡¡ç‚¹**: RTMPose-m (75.8% AP, 70+ FPS mobile ç§»åŠ¨ç«¯70+ FPS)

---

## 3. Commercial Performance Metrics å•†ä¸šæ€§èƒ½æŒ‡æ ‡

### 3.1 Peloton IQ (2025)

**Publicly Claimed Metrics å…¬å¼€å£°ç§°çš„æŒ‡æ ‡**:

- Training data è®­ç»ƒæ•°æ®: 5+ million workouts 500ä¸‡+è®­ç»ƒ, 40,000+ hours 4ä¸‡+å°æ—¶
- Exercises covered è¦†ç›–ç»ƒä¹ : 37+ movements 37+åŠ¨ä½œ
- Accuracy ç²¾åº¦: Not disclosed publicly æœªå…¬å¼€æŠ«éœ²
- Latency å»¶è¿Ÿ: "Real-time å®æ—¶" (estimated <100ms ä¼°è®¡<100ms)

**Feedback Design åé¦ˆè®¾è®¡**:

- **Confidence thresholding ç½®ä¿¡åº¦é˜ˆå€¼**: Only shows feedback when confident ä»…åœ¨æœ‰ä¿¡å¿ƒæ—¶æ˜¾ç¤ºåé¦ˆ
- **Adaptive strictness è‡ªé€‚åº”ä¸¥æ ¼åº¦**: Beginner â†’ Advanced modes åˆçº§â†’é«˜çº§æ¨¡å¼
- **Multi-level å¤šçº§**: Visual + audio + post-workout summary è§†è§‰+éŸ³é¢‘+è®­ç»ƒåæ‘˜è¦

**Technology æŠ€æœ¯**: Computer vision (pose estimation è®¡ç®—æœºè§†è§‰ï¼Œå§¿æ€ä¼°è®¡), proprietary ML models ä¸“æœ‰MLæ¨¡å‹

---

### 3.2 Tonal

**Publicly Claimed Metrics å…¬å¼€å£°ç§°çš„æŒ‡æ ‡**:

- Training database è®­ç»ƒæ•°æ®åº“: "Nearly 1 billion reps è¿‘10äº¿æ¬¡é‡å¤"
- Exercise coverage ç»ƒä¹ è¦†ç›–: 111 strength exercises 111ä¸ªåŠ›é‡ç»ƒä¹ 
- Feedback types åé¦ˆç±»å‹: Up to 6 per exercise æ¯ä¸ªç»ƒä¹ æœ€å¤š6ç§
  1. Speed é€Ÿåº¦ (tempo control èŠ‚å¥æ§åˆ¶)
  2. Range of motion è¿åŠ¨èŒƒå›´
  3. Position ä½ç½® (joint angles å…³èŠ‚è§’åº¦)
  4. Balance å¹³è¡¡ (left/right å·¦/å³)
  5. Symmetry å¯¹ç§°æ€§
  6. Smoothness æµç•…åº¦

**Technology æŠ€æœ¯**:

- Multi-sensor fusion å¤šä¼ æ„Ÿå™¨èåˆ (force sensors + rope tracking + vision åŠ›ä¼ æ„Ÿå™¨+ç»³ç´¢è¿½è¸ª+è§†è§‰)
- 60 Hz sensor sampling rate 60 Hzä¼ æ„Ÿå™¨é‡‡æ ·ç‡
- Estimated <50ms latency ä¼°è®¡å»¶è¿Ÿ<50ms

**Key Claim å…³é”®å£°ç§°**:
> "Think of Tonal vs. pure vision systems like the difference between a sportscaster and a sports science laboratory."
> "å°†Tonalä¸çº¯è§†è§‰ç³»ç»Ÿæ¯”è¾ƒï¼Œå°±åƒä½“è‚²è¯„è®ºå‘˜ä¸è¿åŠ¨ç§‘å­¦å®éªŒå®¤ä¹‹é—´çš„åŒºåˆ«ã€‚"

**Validation éªŒè¯**: Multi-sensor superiority over vision-only å¤šä¼ æ„Ÿå™¨ä¼˜äºä»…è§†è§‰

---

### 3.3 MAGIC AI Mirror

**Publicly Claimed Metrics å…¬å¼€å£°ç§°çš„æŒ‡æ ‡**:

- Movement patterns recognized è¯†åˆ«çš„è¿åŠ¨æ¨¡å¼: ~400
- Rep quality scoring é‡å¤è´¨é‡è¯„åˆ†: 0-100 numerical scale 0-100æ•°å€¼é‡è¡¨
- Accuracy ç²¾åº¦: Not disclosed æœªæŠ«éœ²
- Latency å»¶è¿Ÿ: "Real-time å®æ—¶"

**Feedback Design åé¦ˆè®¾è®¡**:

- Color-coded joint indicators (green/yellow/red é¢œè‰²ç¼–ç å…³èŠ‚æŒ‡ç¤ºå™¨ï¼Œç»¿/é»„/çº¢)
- Holographic coach overlay å…¨æ¯æ•™ç»ƒè¦†ç›–å±‚
- Movement trajectory visualization è¿åŠ¨è½¨è¿¹å¯è§†åŒ–
- Side-by-side ideal vs. actual comparison ç†æƒ³ä¸å®é™…å¹¶æ’æ¯”è¾ƒ

**Technology æŠ€æœ¯**: ReflectAIÂ® (proprietary vision system ä¸“æœ‰è§†è§‰ç³»ç»Ÿ)

---

### 3.4 Tempo Studio

**Publicly Claimed Metrics å…¬å¼€å£°ç§°çš„æŒ‡æ ‡**:

- 3D reconstruction 3Dé‡å»º: 30+ FPS
- Latency å»¶è¿Ÿ: Estimated <100ms ä¼°è®¡<100ms
- Accuracy ç²¾åº¦: Not disclosed æœªæŠ«éœ²

**Technology æŠ€æœ¯**:

- Time-of-Flight (ToF) depth sensors é£è¡Œæ—¶é—´æ·±åº¦ä¼ æ„Ÿå™¨
- True 3D pose (not 2D projection çœŸå®3Då§¿æ€ï¼Œé2DæŠ•å½±)
- Real-time joint angle calculation å®æ—¶å…³èŠ‚è§’åº¦è®¡ç®—

---

### 3.5 Form (Swimming Goggles) æ¸¸æ³³æŠ¤ç›®é•œ

**Published Scientific Validation å·²å‘è¡¨çš„ç§‘å­¦éªŒè¯**:

- **Peer-reviewed accuracy åŒè¡Œè¯„å®¡ç²¾åº¦**: Validated against video analysis é€šè¿‡è§†é¢‘åˆ†æéªŒè¯
- **Stroke count åˆ’æ°´æ¬¡æ•°**: >95% accuracy ç²¾åº¦>95%
- **Heart rate å¿ƒç‡**: Clinical-grade precision ä¸´åºŠçº§ç²¾åº¦
- **Latency å»¶è¿Ÿ**: <20ms (on-device processing è®¾å¤‡ç«¯å¤„ç†)

**Technology æŠ€æœ¯**:

- IMU-based stroke detection åŸºäºIMUçš„åˆ’æ°´æ£€æµ‹
- Optical heart rate sensor å…‰å­¦å¿ƒç‡ä¼ æ„Ÿå™¨
- AR display with <30ms latency ARæ˜¾ç¤ºå»¶è¿Ÿ<30ms

**Key Finding å…³é”®å‘ç°**: Scientific validation proves AR feedback effectiveness ç§‘å­¦éªŒè¯è¯æ˜ARåé¦ˆæœ‰æ•ˆæ€§

---

### 3.6 Apple Fitness+

**Current State (2025) å½“å‰çŠ¶æ€ï¼ˆ2025ï¼‰**:

- âŒ No real-time form correction æ— å®æ—¶åŠ¨ä½œçº æ­£
- âŒ No pose estimation æ— å§¿æ€ä¼°è®¡
- âŒ No AI coaching æ— AIæ•™ç»ƒ

**Market Observation å¸‚åœºè§‚å¯Ÿ**:

- Apple has all necessary technology (Vision Framework, ARKit, LiDAR Appleæ‹¥æœ‰æ‰€æœ‰å¿…è¦æŠ€æœ¯)
- Has not yet entered AI fitness feedback market å°šæœªè¿›å…¥AIå¥èº«åé¦ˆå¸‚åœº
- **Validates market opportunity still exists éªŒè¯å¸‚åœºæœºä¼šä»ç„¶å­˜åœ¨**

---

## 4. Evaluation Metrics & Standards è¯„ä¼°æŒ‡æ ‡ä¸æ ‡å‡†

### 4.1 Pose Estimation Metrics å§¿æ€ä¼°è®¡æŒ‡æ ‡

#### COCO Average Precision (AP) COCOå¹³å‡ç²¾åº¦

**Definition å®šä¹‰**:

- Percentage of keypoints correctly detected within threshold é˜ˆå€¼å†…æ­£ç¡®æ£€æµ‹çš„å…³é”®ç‚¹ç™¾åˆ†æ¯”
- Standard threshold æ ‡å‡†é˜ˆå€¼: Object Keypoint Similarity (OKS å¯¹è±¡å…³é”®ç‚¹ç›¸ä¼¼æ€§) > 0.5

**How to Calculate å¦‚ä½•è®¡ç®—**:

```python
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# Load ground truth and predictions åŠ è½½çœŸå®å€¼å’Œé¢„æµ‹
coco_gt = COCO('annotations/person_keypoints_val2017.json')
coco_dt = coco_gt.loadRes('results/predictions.json')

# Evaluate è¯„ä¼°
coco_eval = COCOeval(coco_gt, coco_dt, 'keypoints')
coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

# Primary metric ä¸»è¦æŒ‡æ ‡
ap = coco_eval.stats[0]  # AP at OKS=0.50:0.95
```

**Variants å˜ä½“**:

- **AP**: Average Precision @ OKS=0.50:0.95 (primary metric ä¸»è¦æŒ‡æ ‡)
- **AP50**: AP @ OKS=0.50 (easier threshold è¾ƒæ˜“é˜ˆå€¼)
- **AP75**: AP @ OKS=0.75 (strict threshold ä¸¥æ ¼é˜ˆå€¼)
- **AR**: Average Recall å¹³å‡å¬å›ç‡

**Target for Our System æˆ‘ä»¬ç³»ç»Ÿçš„ç›®æ ‡**: AP â‰¥ 72% (match MediaPipe åŒ¹é…MediaPipe)

---

#### Mean Per Joint Position Error (MPJPE) å¹³å‡æ¯å…³èŠ‚ä½ç½®è¯¯å·®

**Definition å®šä¹‰**: Average 3D distance error for each joint (millimeters å¹³å‡3Dè·ç¦»è¯¯å·®ï¼Œæ¯«ç±³)

**How to Calculate å¦‚ä½•è®¡ç®—**:

```python
import numpy as np

def mpjpe(predicted, ground_truth):
    """
    predicted: (N, 17, 3) - N frames, 17 keypoints, (x,y,z)
    ground_truth: (N, 17, 3)
    """
    return np.mean(np.sqrt(np.sum((predicted - ground_truth)**2, axis=2)))

# Example ç¤ºä¾‹
error_mm = mpjpe(pred_poses, gt_poses)
print(f"MPJPE: {error_mm:.2f} mm")
```

**Good Performance è‰¯å¥½æ€§èƒ½**:

- <50mm: Excellent ä¼˜ç§€
- 50-80mm: Good è‰¯å¥½
- 80-120mm: Acceptable å¯æ¥å—
- >120mm: Poor è¾ƒå·®

**Target ç›®æ ‡**: <80mm MPJPE for critical joints å…³é”®å…³èŠ‚<80mm (elbows è‚˜éƒ¨, knees è†éƒ¨, wrists è…•éƒ¨)

---

#### Percentage of Correct Keypoints (PCK) æ­£ç¡®å…³é”®ç‚¹ç™¾åˆ†æ¯”

**Definition å®šä¹‰**: Percentage of joints within threshold distance of ground truth çœŸå®å€¼é˜ˆå€¼è·ç¦»å†…çš„å…³èŠ‚ç™¾åˆ†æ¯”

**How to Calculate å¦‚ä½•è®¡ç®—**:

```python
def pck(predicted, ground_truth, threshold=0.05):
    """
    threshold: fraction of torso diameter (typically 0.05 = 5% èº¯å¹²ç›´å¾„çš„åˆ†æ•°)
    """
    # Calculate torso diameter (shoulder to hip distance è®¡ç®—èº¯å¹²ç›´å¾„ï¼Œè‚©åˆ°è‡€è·ç¦»)
    torso_diameter = np.linalg.norm(
        ground_truth[:, shoulder_idx] - ground_truth[:, hip_idx],
        axis=1
    )

    # Calculate distances for each joint è®¡ç®—æ¯ä¸ªå…³èŠ‚çš„è·ç¦»
    distances = np.linalg.norm(predicted - ground_truth, axis=2)

    # Count correct if within threshold * torso_diameter å¦‚æœåœ¨é˜ˆå€¼*èº¯å¹²ç›´å¾„å†…åˆ™è®¡ä¸ºæ­£ç¡®
    threshold_dist = threshold * torso_diameter[:, np.newaxis]
    correct = distances < threshold_dist

    return np.mean(correct) * 100  # Percentage ç™¾åˆ†æ¯”

# Example ç¤ºä¾‹
pck_score = pck(pred_poses, gt_poses, threshold=0.05)
print(f"PCK@0.05: {pck_score:.1f}%")
```

**Target ç›®æ ‡**: PCK@0.05 > 90%

---

### 4.2 Feedback Quality Metrics åé¦ˆè´¨é‡æŒ‡æ ‡

#### User Study Design (Standard Approach) ç”¨æˆ·ç ”ç©¶è®¾è®¡ï¼ˆæ ‡å‡†æ–¹æ³•ï¼‰

**Methodology æ–¹æ³•è®º**:

1. **Participants å‚ä¸è€…**: 20-30 users (mix of beginners and experienced åˆå­¦è€…å’Œæœ‰ç»éªŒè€…æ··åˆ)
2. **Exercises ç»ƒä¹ **: 5-10 representative movements 5-10ä¸ªä»£è¡¨æ€§åŠ¨ä½œ
3. **Conditions æ¡ä»¶**:
   - With AI feedback (our system ä½¿ç”¨AIåé¦ˆï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿ)
   - Without feedback (baseline æ— åé¦ˆï¼ŒåŸºçº¿)
   - With human trainer (gold standard, optional äººç±»æ•™ç»ƒï¼Œé»„é‡‘æ ‡å‡†ï¼Œå¯é€‰)
4. **Metrics æŒ‡æ ‡**:
   - Form improvement (pre/post scores åŠ¨ä½œæ”¹è¿›ï¼Œå‰/åè¯„åˆ†)
   - User satisfaction (Likert scale 1-7 ç”¨æˆ·æ»¡æ„åº¦ï¼Œæå…‹ç‰¹é‡è¡¨1-7)
   - Perceived usefulness æ„ŸçŸ¥æœ‰ç”¨æ€§
   - Trust in system å¯¹ç³»ç»Ÿçš„ä¿¡ä»»

**Quantitative Measurements å®šé‡æµ‹é‡**:

```python
# Form quality scoring (0-100) åŠ¨ä½œè´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰
def score_form(keypoints, exercise_type):
    """
    Based on joint angles, ROM, symmetry
    åŸºäºå…³èŠ‚è§’åº¦ã€è¿åŠ¨èŒƒå›´ã€å¯¹ç§°æ€§
    """
    scores = {
        'joint_angles': score_angles(keypoints),
        'range_of_motion': score_rom(keypoints),
        'symmetry': score_symmetry(keypoints),
        'tempo': score_tempo(keypoints)
    }
    return np.mean(list(scores.values()))
```

**Publication Requirements å‘è¡¨è¦æ±‚**:

- IRB approval for human subjects research äººä½“å—è¯•è€…ç ”ç©¶çš„IRBæ‰¹å‡†
- Pre-registration of study design ç ”ç©¶è®¾è®¡é¢„æ³¨å†Œ
- Statistical analysis (paired t-tests, effect sizes ç»Ÿè®¡åˆ†æï¼Œé…å¯¹tæ£€éªŒã€æ•ˆåº”é‡)

---

#### Feedback Accuracy Metrics åé¦ˆç²¾åº¦æŒ‡æ ‡

**False Positive Rate (FPR) å‡é˜³æ€§ç‡**:

```python
# When feedback is given but form is actually correct
# å½“ç»™å‡ºåé¦ˆä½†åŠ¨ä½œå®é™…æ­£ç¡®æ—¶
FPR = False_Positives / (False_Positives + True_Negatives)
```

**Target ç›®æ ‡**: FPR < 10% (90%+ specificity ç‰¹å¼‚æ€§>90%)

**False Negative Rate (FNR) å‡é˜´æ€§ç‡**:

```python
# When feedback is NOT given but form is actually wrong
# å½“æœªç»™å‡ºåé¦ˆä½†åŠ¨ä½œå®é™…é”™è¯¯æ—¶
FNR = False_Negatives / (False_Negatives + True_Positives)
```

**Target ç›®æ ‡**: FNR < 20% (80%+ sensitivity æ•æ„Ÿæ€§>80%)

**Precision (Positive Predictive Value) ç²¾ç¡®åº¦ï¼ˆé˜³æ€§é¢„æµ‹å€¼ï¼‰**:

```python
Precision = True_Positives / (True_Positives + False_Positives)
```

**Target ç›®æ ‡**: Precision > 80%

---

### 4.3 System Latency Metrics ç³»ç»Ÿå»¶è¿ŸæŒ‡æ ‡

#### End-to-End Latency Breakdown ç«¯åˆ°ç«¯å»¶è¿Ÿåˆ†è§£

**Target ç›®æ ‡**: <100ms total for real-time feedback å®æ—¶åé¦ˆæ€»è®¡<100ms

| Component ç»„ä»¶ | Target (ms) ç›®æ ‡ï¼ˆæ¯«ç§’ï¼‰ | Measurement Method æµ‹é‡æ–¹æ³• |
|-----------|-------------|-------------------|
| Sensor capture ä¼ æ„Ÿå™¨æ•è· | <5 | IMU/camera timestamp IMU/ç›¸æœºæ—¶é—´æˆ³ |
| Data transmission æ•°æ®ä¼ è¾“ | <10 | Network profiling ç½‘ç»œæ€§èƒ½åˆ†æ |
| Pose estimation å§¿æ€ä¼°è®¡ | <30 | Model inference time æ¨¡å‹æ¨ç†æ—¶é—´ |
| Feedback generation åé¦ˆç”Ÿæˆ | <20 | Logic execution time é€»è¾‘æ‰§è¡Œæ—¶é—´ |
| Haptic/visual output Haptic/è§†è§‰è¾“å‡º | <10 | Display/actuator delay æ˜¾ç¤º/æ‰§è¡Œå™¨å»¶è¿Ÿ |
| **Total æ€»è®¡** | **<100** | **End-to-end timestamp ç«¯åˆ°ç«¯æ—¶é—´æˆ³** |

**How to Measure å¦‚ä½•æµ‹é‡**:

```python
import time

# Full pipeline timing å®Œæ•´ç®¡é“è®¡æ—¶
start = time.perf_counter()

# 1. Sensor capture ä¼ æ„Ÿå™¨æ•è·
frame = camera.read()
imu_data = sensor.read()
t1 = time.perf_counter()

# 2. Pose estimation å§¿æ€ä¼°è®¡
keypoints = pose_estimator(frame)
t2 = time.perf_counter()

# 3. Feedback generation åé¦ˆç”Ÿæˆ
feedback = generate_feedback(keypoints, imu_data)
t3 = time.perf_counter()

# 4. Output è¾“å‡º
haptic_device.vibrate(feedback.pattern)
t4 = time.perf_counter()

# Report æŠ¥å‘Š
print(f"Capture æ•è·: {(t1-start)*1000:.1f}ms")
print(f"Pose å§¿æ€: {(t2-t1)*1000:.1f}ms")
print(f"Feedback åé¦ˆ: {(t3-t2)*1000:.1f}ms")
print(f"Output è¾“å‡º: {(t4-t3)*1000:.1f}ms")
print(f"TOTAL æ€»è®¡: {(t4-start)*1000:.1f}ms")
```

---

### 4.4 Battery Life & Power Consumption ç”µæ± ç»­èˆªä¸åŠŸè€—

**Measurement æµ‹é‡**:

```python
# Android (adb)
adb shell dumpsys batterystats --reset  # Reset stats é‡ç½®ç»Ÿè®¡
# Run workout for 1 hour è¿è¡Œ1å°æ—¶è®­ç»ƒ
adb shell dumpsys batterystats > battery_stats.txt

# iOS (Xcode Instruments)
# Use Energy Log instrument ä½¿ç”¨èƒ½é‡æ—¥å¿—å·¥å…·
# Record 1-hour workout session è®°å½•1å°æ—¶è®­ç»ƒä¼šè¯
```

**Target ç›®æ ‡**:

- <20% battery drain per hour (moderate usage æ¯å°æ—¶ç”µæ± æ¶ˆè€—<20%ï¼Œä¸­ç­‰ä½¿ç”¨)
- <15% battery drain per hour (optimized æ¯å°æ—¶ç”µæ± æ¶ˆè€—<15%ï¼Œä¼˜åŒ–å)

---

## 5. Dataset Usage Strategy (Phased) æ•°æ®é›†ä½¿ç”¨ç­–ç•¥ï¼ˆåˆ†é˜¶æ®µï¼‰

### Phase 1: MVP Development (Months 1-3) MVPå¼€å‘ï¼ˆç¬¬1-3ä¸ªæœˆï¼‰

**Goal ç›®æ ‡**: Get working prototype as fast as possible å°½å¿«è·å¾—å·¥ä½œåŸå‹

#### Priority Datasets ä¼˜å…ˆæ•°æ®é›†

| Dataset æ•°æ®é›† | Purpose ç›®çš„ | Action è¡ŒåŠ¨ | Timeline æ—¶é—´çº¿ |
|---------|---------|--------|----------|
| **MM-Fit** | Sensor fusion validation ä¼ æ„Ÿå™¨èåˆéªŒè¯ | âœ… Download NOW ç«‹å³ä¸‹è½½ | Week 1 ç¬¬1å‘¨ |
| **COCO Keypoints** | Pre-trained models é¢„è®­ç»ƒæ¨¡å‹ | âœ… Use existing models ä½¿ç”¨ç°æœ‰æ¨¡å‹ | Week 1 ç¬¬1å‘¨ |
| **RecoFit** | IMU baseline IMUåŸºçº¿ | âœ… Download NOW ç«‹å³ä¸‹è½½ | Week 2 ç¬¬2å‘¨ |

#### Specific Tasks å…·ä½“ä»»åŠ¡

**Week 1-2 ç¬¬1-2å‘¨**:

```bash
# Download MM-Fit ä¸‹è½½MM-Fit
git clone https://github.com/KDMStromback/mm-fit.git
cd mm-fit && bash download_dataset.sh

# Download RecoFit ä¸‹è½½RecoFit
git clone https://github.com/microsoft/Exercise-Recognition-from-Wearable-Sensors.git
```

**Week 3-4 ç¬¬3-4å‘¨**:

- Test pose estimation on MM-Fit video data åœ¨MM-Fitè§†é¢‘æ•°æ®ä¸Šæµ‹è¯•å§¿æ€ä¼°è®¡
- Validate IMU processing on RecoFit data åœ¨RecoFitæ•°æ®ä¸ŠéªŒè¯IMUå¤„ç†
- Measure baseline accuracy (COCO AP metric æµ‹é‡åŸºçº¿ç²¾åº¦ï¼ŒCOCO APæŒ‡æ ‡)

**Month 2-3 ç¬¬2-3ä¸ªæœˆ**:

- Benchmark sensor fusion on MM-Fit synchronized data åœ¨MM-FitåŒæ­¥æ•°æ®ä¸ŠåŸºå‡†æµ‹è¯•ä¼ æ„Ÿå™¨èåˆ
- Compare IMU-only vs. Vision-only vs. Fusion accuracy æ¯”è¾ƒä»…IMUã€ä»…è§†è§‰ä¸èåˆç²¾åº¦
- Initial latency measurements åˆå§‹å»¶è¿Ÿæµ‹é‡

**Milestone é‡Œç¨‹ç¢‘**: Functional MVP with quantified baseline performance å…·æœ‰é‡åŒ–åŸºçº¿æ€§èƒ½çš„åŠŸèƒ½MVP

---

### Phase 2: System Refinement (Months 3-6) ç³»ç»Ÿä¼˜åŒ–ï¼ˆç¬¬3-6ä¸ªæœˆï¼‰

**Goal ç›®æ ‡**: Improve accuracy, optimize performance, prepare for research æé«˜ç²¾åº¦ï¼Œä¼˜åŒ–æ€§èƒ½ï¼Œä¸ºç ”ç©¶åšå‡†å¤‡

#### Add Datasets æ·»åŠ æ•°æ®é›†

| Dataset æ•°æ®é›† | Purpose ç›®çš„ | Action è¡ŒåŠ¨ | Timeline æ—¶é—´çº¿ |
|---------|---------|--------|----------|
| **Fit3D** | Benchmark feedback system åŸºå‡†æµ‹è¯•åé¦ˆç³»ç»Ÿ | âœ… Apply for access NOW ç«‹å³ç”³è¯·è®¿é—® | Month 3 ç¬¬3ä¸ªæœˆ |
| **FLAG3D** | Language feedback design è¯­è¨€åé¦ˆè®¾è®¡ | âœ… Download ä¸‹è½½ | Month 4 ç¬¬4ä¸ªæœˆ |
| **MPII** | Pose robustness testing å§¿æ€é²æ£’æ€§æµ‹è¯• | Download if needed å¦‚éœ€ä¸‹è½½ | Month 5 ç¬¬5ä¸ªæœˆ |

#### Specific Tasks å…·ä½“ä»»åŠ¡

**Month 3-4 ç¬¬3-4ä¸ªæœˆ** (while waiting for Fit3D access ç­‰å¾…Fit3Dè®¿é—®æœŸé—´):

- Download FLAG3D dataset ä¸‹è½½FLAG3Dæ•°æ®é›†
- Analyze natural language instruction patterns åˆ†æè‡ªç„¶è¯­è¨€æŒ‡å¯¼æ¨¡å¼
- Design feedback generation templates è®¾è®¡åé¦ˆç”Ÿæˆæ¨¡æ¿
- Implement multi-level feedback (visual + audio + haptic å®ç°å¤šçº§åé¦ˆï¼Œè§†è§‰+éŸ³é¢‘+haptic)

**Month 4-5 ç¬¬4-5ä¸ªæœˆ** (if Fit3D granted å¦‚æœè·å¾—Fit3D):

- Benchmark pose estimation: Our system vs. AIFit baseline åŸºå‡†æµ‹è¯•å§¿æ€ä¼°è®¡ï¼šæˆ‘ä»¬çš„ç³»ç»Ÿå¯¹æ¯”AIFitåŸºçº¿
- Compare feedback quality æ¯”è¾ƒåé¦ˆè´¨é‡
- Measure user study metrics (if possible æµ‹é‡ç”¨æˆ·ç ”ç©¶æŒ‡æ ‡ï¼Œå¦‚å¯èƒ½)
- Identify improvement areas è¯†åˆ«æ”¹è¿›é¢†åŸŸ

**Month 6 ç¬¬6ä¸ªæœˆ**:

- Optimize based on Fit3D benchmarks åŸºäºFit3DåŸºå‡†ä¼˜åŒ–
- Reduce latency to <50ms å°†å»¶è¿Ÿé™è‡³<50ms
- Improve battery life to <15%/hour å°†ç”µæ± ç»­èˆªæé«˜è‡³æ¯å°æ—¶<15%
- Finalize feedback generation logic æœ€ç»ˆç¡®å®šåé¦ˆç”Ÿæˆé€»è¾‘

**Milestone é‡Œç¨‹ç¢‘**: Production-ready system with peer-reviewed benchmark scores å…·æœ‰åŒè¡Œè¯„å®¡åŸºå‡†åˆ†æ•°çš„ç”Ÿäº§å°±ç»ªç³»ç»Ÿ

---

### Phase 3: Research Publication (Months 6-12) ç ”ç©¶å‘è¡¨ï¼ˆç¬¬6-12ä¸ªæœˆï¼‰

**Goal ç›®æ ‡**: Publish novel contributions, collect custom dataset å‘è¡¨æ–°é¢–è´¡çŒ®ï¼Œæ”¶é›†è‡ªå®šä¹‰æ•°æ®é›†

#### Custom Dataset Collection è‡ªå®šä¹‰æ•°æ®é›†æ”¶é›†

**EMG + IMU + Vision + Haptic Dataset**:

- **Scale è§„æ¨¡**: 20-30 participants (sufficient for CHI/IMWUT å‚ä¸è€…ï¼Œè¶³å¤Ÿç”¨äºCHI/IMWUT)
- **Exercises ç»ƒä¹ **: 10-15 movements 10-15ä¸ªåŠ¨ä½œ
- **Modalities æ¨¡æ€**:
  - EMG muscle activation (our unique contribution EMGè‚Œè‚‰æ¿€æ´»ï¼Œæˆ‘ä»¬çš„ç‹¬ç‰¹è´¡çŒ®)
  - IMU kinematics IMUè¿åŠ¨å­¦
  - RGB video + 3D pose RGBè§†é¢‘+3Då§¿æ€
  - Haptic feedback timing Hapticåé¦ˆæ—¶é—´
- **Annotations æ³¨é‡Š**:
  - Ground truth form quality (expert ratings çœŸå®åŠ¨ä½œè´¨é‡ï¼Œä¸“å®¶è¯„åˆ†)
  - Muscle activation patterns è‚Œè‚‰æ¿€æ´»æ¨¡å¼
  - User feedback (qualitative ç”¨æˆ·åé¦ˆï¼Œå®šæ€§)

**IRB Approval IRBæ‰¹å‡†**: Apply in Month 6, expect 1-2 months approval ç¬¬6ä¸ªæœˆç”³è¯·ï¼Œé¢„è®¡1-2ä¸ªæœˆæ‰¹å‡†

**Data Collection æ•°æ®æ”¶é›†**: Months 8-10 ç¬¬8-10ä¸ªæœˆ

**Analysis & Writing åˆ†æä¸å†™ä½œ**: Months 10-12 ç¬¬10-12ä¸ªæœˆ

#### Comparison Strategy æ¯”è¾ƒç­–ç•¥

**Table for Paper è®ºæ–‡è¡¨æ ¼**:

| Method æ–¹æ³• | Dataset æ•°æ®é›† | AP (%) | MPJPE (mm) | Latency (ms) å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ |
|--------|---------|--------|------------|--------------|
| AIFit (baseline åŸºçº¿) | Fit3D | 74.2 | 85 | ~100 |
| Tonal (multi-sensor å¤šä¼ æ„Ÿå™¨) | Proprietary ä¸“æœ‰ | N/A | N/A | ~50 |
| MediaPipe (vision è§†è§‰) | COCO | 72.0 | 95 | 25-30 |
| RTMPose (vision è§†è§‰) | COCO | 75.8 | 78 | 11 |
| **Ours (IMU+Vision æˆ‘ä»¬çš„ï¼ŒIMU+è§†è§‰)** | **MM-Fit** | **76.5** | **72** | **45** |
| **Ours (IMU+Vision+EMG æˆ‘ä»¬çš„ï¼ŒIMU+è§†è§‰+EMG)** | **Custom è‡ªå®šä¹‰** | **77.8** | **68** | **48** |

*Hypothetical target numbers - actual results depend on implementation
*å‡è®¾ç›®æ ‡æ•°å­— - å®é™…ç»“æœå–å†³äºå®ç°

**Novel Contributions to Highlight è¦å¼ºè°ƒçš„æ–°é¢–è´¡çŒ®**:

1. âœ… EMG muscle activation for form assessment (no other system has this EMGè‚Œè‚‰æ¿€æ´»ç”¨äºåŠ¨ä½œè¯„ä¼°ï¼Œæ²¡æœ‰å…¶ä»–ç³»ç»Ÿæœ‰æ­¤åŠŸèƒ½)
2. âœ… Haptic real-time feedback (non-visual modality Hapticå®æ—¶åé¦ˆï¼Œéè§†è§‰æ¨¡æ€)
3. âœ… Low-cost (<$300 vs. $2,000-3,000 commercial ä½æˆæœ¬ï¼Œ<$300å¯¹æ¯”å•†ä¸š$2,000-3,000)
4. âœ… Open-source (reproducible research å¼€æºï¼Œå¯é‡ç°ç ”ç©¶)
5. âœ… Multi-sport generalization å¤šè¿åŠ¨æ³›åŒ–

**Milestone é‡Œç¨‹ç¢‘**: 1-2 peer-reviewed publications (CHI, IMWUT, CVPR 1-2ç¯‡åŒè¡Œè¯„å®¡å‘è¡¨)

---

### Timeline Summary æ—¶é—´çº¿æ‘˜è¦

```
Month 1-3 ç¬¬1-3ä¸ªæœˆ: MVP with MM-Fit + RecoFit validation ä½¿ç”¨MM-Fit + RecoFitéªŒè¯çš„MVP
Month 3-6 ç¬¬3-6ä¸ªæœˆ: Optimization with Fit3D benchmarking (if granted ä½¿ç”¨Fit3DåŸºå‡†ä¼˜åŒ–ï¼Œå¦‚è·æ‰¹)
Month 6-8 ç¬¬6-8ä¸ªæœˆ: IRB approval + study design IRBæ‰¹å‡†+ç ”ç©¶è®¾è®¡
Month 8-10 ç¬¬8-10ä¸ªæœˆ: Custom dataset collection è‡ªå®šä¹‰æ•°æ®é›†æ”¶é›†
Month 10-12 ç¬¬10-12ä¸ªæœˆ: Analysis + paper writing åˆ†æ+è®ºæ–‡å†™ä½œ
Month 12+ ç¬¬12ä¸ªæœˆ+: Publication submission å‘è¡¨æäº¤
```

---

## 6. Publication & Research Targets å‘è¡¨ä¸ç ”ç©¶ç›®æ ‡

### 6.1 Target Venues ç›®æ ‡ä¼šè®®/æœŸåˆŠ

#### Tier 1 (Top Venues) ä¸€çº§ï¼ˆé¡¶çº§ä¼šè®®ï¼‰

**CHI (ACM Conference on Human Factors in Computing Systems äººæœºäº¤äº’ç ”è®¨ä¼š)**:

- **Acceptance Rate å½•å–ç‡**: ~25%
- **Impact Factor å½±å“å› å­**: High (A*ranking A*æ’å)
- **Focus é‡ç‚¹**: Human-computer interaction, fitness technology UX äººæœºäº¤äº’ã€å¥èº«æŠ€æœ¯ç”¨æˆ·ä½“éªŒ
- **Typical Citations å…¸å‹å¼•ç”¨**: 20-50 for good papers å¥½è®ºæ–‡20-50
- **Deadline æˆªæ­¢æ—¥æœŸ**: Usually September (for May conference é€šå¸¸9æœˆï¼Œ5æœˆä¼šè®®)
- **Why Target ä¸ºä»€ä¹ˆç„å‡†**: Perfect fit for fitness feedback systems, user studies å®Œç¾é€‚åˆå¥èº«åé¦ˆç³»ç»Ÿã€ç”¨æˆ·ç ”ç©¶

**IMWUT (Interactive, Mobile, Wearable and Ubiquitous Technologies äº¤äº’ã€ç§»åŠ¨ã€å¯ç©¿æˆ´å’Œæ³›åœ¨æŠ€æœ¯)**:

- **Acceptance Rate å½•å–ç‡**: ~25%
- **Format æ ¼å¼**: Journal (rolling submissions æœŸåˆŠï¼Œæ»šåŠ¨æäº¤)
- **Focus é‡ç‚¹**: Wearable sensors, mobile systems, ubiquitous computing å¯ç©¿æˆ´ä¼ æ„Ÿå™¨ã€ç§»åŠ¨ç³»ç»Ÿã€æ³›åœ¨è®¡ç®—
- **Typical Citations å…¸å‹å¼•ç”¨**: 30-100 for impactful papers æœ‰å½±å“åŠ›è®ºæ–‡30-100
- **Why Target ä¸ºä»€ä¹ˆç„å‡†**: Ideal for our multimodal wearable approach é€‚åˆæˆ‘ä»¬çš„å¤šæ¨¡æ€å¯ç©¿æˆ´æ–¹æ³•

**CVPR (Conference on Computer Vision and Pattern Recognition è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«)**:

- **Acceptance Rate å½•å–ç‡**: ~25-30%
- **Impact Factor å½±å“å› å­**: Very High (A*ranking A*æ’å)
- **Focus é‡ç‚¹**: Computer vision, pose estimation algorithms è®¡ç®—æœºè§†è§‰ã€å§¿æ€ä¼°è®¡ç®—æ³•
- **Typical Citations å…¸å‹å¼•ç”¨**: 50-200 for good papers å¥½è®ºæ–‡50-200
- **Deadline æˆªæ­¢æ—¥æœŸ**: Usually November (for June conference é€šå¸¸11æœˆï¼Œ6æœˆä¼šè®®)
- **Why Target ä¸ºä»€ä¹ˆç„å‡†**: If we have novel pose estimation/fusion contribution å¦‚æœæˆ‘ä»¬æœ‰æ–°é¢–çš„å§¿æ€ä¼°è®¡/èåˆè´¡çŒ®

#### Tier 2 (Good Alternatives) äºŒçº§ï¼ˆè‰¯å¥½æ›¿ä»£ï¼‰

**UIST (User Interface Software and Technology ç”¨æˆ·ç•Œé¢è½¯ä»¶ä¸æŠ€æœ¯)**:

- Focus é‡ç‚¹: Novel interaction techniques æ–°é¢–äº¤äº’æŠ€æœ¯
- Good fit for haptic feedback innovation é€‚åˆhapticåé¦ˆåˆ›æ–°

**MobiSys (Mobile Systems, Applications, and Services ç§»åŠ¨ç³»ç»Ÿã€åº”ç”¨å’ŒæœåŠ¡)**:

- Focus é‡ç‚¹: Mobile system design ç§»åŠ¨ç³»ç»Ÿè®¾è®¡
- Good fit for low-latency mobile implementation é€‚åˆä½å»¶è¿Ÿç§»åŠ¨å®ç°

**PerCom (Pervasive Computing and Communications æ³›åœ¨è®¡ç®—ä¸é€šä¿¡)**:

- Focus é‡ç‚¹: Pervasive computing æ³›åœ¨è®¡ç®—
- Good fit for sensor fusion approach é€‚åˆä¼ æ„Ÿå™¨èåˆæ–¹æ³•

**Sports Engineering / Journal of Sports Sciences è¿åŠ¨å·¥ç¨‹/è¿åŠ¨ç§‘å­¦æœŸåˆŠ**:

- Focus é‡ç‚¹: Applied sports technology åº”ç”¨è¿åŠ¨æŠ€æœ¯
- Easier acceptance, lower citation counts æ›´æ˜“æ¥å—ï¼Œå¼•ç”¨æ¬¡æ•°è¾ƒä½

---

### 6.2 Expected Citation Projections é¢„æœŸå¼•ç”¨é¢„æµ‹

**Conservative Estimates ä¿å®ˆä¼°è®¡** (first 2 years å‰2å¹´):

| Venue ä¼šè®®/æœŸåˆŠ | Expected Citations (2yr) é¢„æœŸå¼•ç”¨ï¼ˆ2å¹´ï¼‰ | Rationale ç†ç”± |
|-------|-------------------------|-----------|
| **CHI** | 15-30 | HCI community interest in fitness tech HCIç¤¾åŒºå¯¹å¥èº«æŠ€æœ¯æ„Ÿå…´è¶£ |
| **IMWUT** | 25-50 | Wearable/ubicomp researchers will cite å¯ç©¿æˆ´/æ³›åœ¨è®¡ç®—ç ”ç©¶è€…ä¼šå¼•ç”¨ |
| **CVPR** | 40-80 | High-impact CV venue, pose estimation topic é«˜å½±å“åŠ›CVä¼šè®®ã€å§¿æ€ä¼°è®¡ä¸»é¢˜ |
| Sports Eng. è¿åŠ¨å·¥ç¨‹ | 5-15 | Smaller community è¾ƒå°ç¤¾åŒº |

**High-Impact Scenario é«˜å½±å“åœºæ™¯** (if novel contribution å¦‚æœæœ‰æ–°é¢–è´¡çŒ®):

- 50-100+ citations in 2 years 2å¹´å†…50-100+å¼•ç”¨ (e.g., if EMG+haptic proves revolutionary ä¾‹å¦‚ï¼Œå¦‚æœEMG+hapticè¯æ˜å…·æœ‰é©å‘½æ€§)
- Example ä¾‹å­: AIFit (CVPR 2021) has 100+ citations in 4 years 4å¹´å†…100+å¼•ç”¨

**Factors for High Citations é«˜å¼•ç”¨å› ç´ **:

1. âœ… Open-source code + dataset release å¼€æºä»£ç +æ•°æ®é›†å‘å¸ƒ
2. âœ… Novel sensor modality (EMG æ–°é¢–ä¼ æ„Ÿå™¨æ¨¡æ€ï¼ŒEMG)
3. âœ… Reproducible results å¯é‡ç°ç»“æœ
4. âœ… Strong user study validation å¼ºå¤§çš„ç”¨æˆ·ç ”ç©¶éªŒè¯
5. âœ… Practical deployment (not just simulation å®é™…éƒ¨ç½²ï¼Œä¸ä»…ä»…æ˜¯æ¨¡æ‹Ÿ)

---

### 6.3 Paper Structure (Standard CHI/IMWUT Format) è®ºæ–‡ç»“æ„ï¼ˆæ ‡å‡†CHI/IMWUTæ ¼å¼ï¼‰

**Title Examples æ ‡é¢˜ç¤ºä¾‹**:

- "Movement Chain AI: Multimodal Real-time Feedback for Movement Training with EMG and Haptic Guidance Movement Chain AIï¼šä½¿ç”¨EMGå’ŒHapticå¼•å¯¼çš„è¿åŠ¨è®­ç»ƒå¤šæ¨¡æ€å®æ—¶åé¦ˆ"
- "Beyond Vision: EMG-Enhanced Movement Correction for Accessible Fitness Training è¶…è¶Šè§†è§‰ï¼šç”¨äºå¯è®¿é—®å¥èº«è®­ç»ƒçš„EMGå¢å¼ºè¿åŠ¨çº æ­£"

**Abstract æ‘˜è¦** (250 words 250å­—):

- Problem é—®é¢˜: Current fitness feedback systems lack muscle activation insight å½“å‰å¥èº«åé¦ˆç³»ç»Ÿç¼ºä¹è‚Œè‚‰æ¿€æ´»æ´å¯Ÿ
- Approach æ–¹æ³•: Multimodal fusion (EMG + IMU + Vision + Haptic å¤šæ¨¡æ€èåˆ)
- Contribution è´¡çŒ®: Novel EMG integration, low-cost, open-source æ–°é¢–çš„EMGé›†æˆã€ä½æˆæœ¬ã€å¼€æº
- Evaluation è¯„ä¼°: User study (N=25), benchmarks (MM-Fit, Fit3D ç”¨æˆ·ç ”ç©¶ï¼ŒåŸºå‡†)
- Results ç»“æœ: 78% accuracy, <$300 cost, 12% improvement over vision-only 78%ç²¾åº¦ï¼Œ<$300æˆæœ¬ï¼Œæ¯”ä»…è§†è§‰æé«˜12%

**Sections ç« èŠ‚**:

1. **Introduction å¼•è¨€** (2 pages 2é¡µ)
   - Motivation åŠ¨æœº: Form correction importance åŠ¨ä½œçº æ­£çš„é‡è¦æ€§
   - Gap å·®è·: Existing systems miss muscle activation ç°æœ‰ç³»ç»Ÿç¼ºå°‘è‚Œè‚‰æ¿€æ´»
   - Contribution è´¡çŒ®: EMG + haptic innovation EMG + hapticåˆ›æ–°

2. **Related Work ç›¸å…³å·¥ä½œ** (2-3 pages 2-3é¡µ)
   - Commercial systems å•†ä¸šç³»ç»Ÿ (Peloton, Tonal, Tempo)
   - Academic research å­¦æœ¯ç ”ç©¶ (AIFit, MM-Fit, FLAG3D)
   - Pose estimation å§¿æ€ä¼°è®¡ (RTMPose, MediaPipe)
   - Gap analysis å·®è·åˆ†æ

3. **System Design ç³»ç»Ÿè®¾è®¡** (3-4 pages 3-4é¡µ)
   - Hardware ç¡¬ä»¶: EMG sensor, IMU, smartphone EMGä¼ æ„Ÿå™¨ã€IMUã€æ™ºèƒ½æ‰‹æœº
   - Software è½¯ä»¶: Pose estimation, sensor fusion, feedback generation å§¿æ€ä¼°è®¡ã€ä¼ æ„Ÿå™¨èåˆã€åé¦ˆç”Ÿæˆ
   - Architecture diagram æ¶æ„å›¾
   - Implementation details å®ç°ç»†èŠ‚

4. **Methodology æ–¹æ³•è®º** (2 pages 2é¡µ)
   - Datasets æ•°æ®é›†: MM-Fit, Fit3D, Custom è‡ªå®šä¹‰
   - Metrics æŒ‡æ ‡: COCO AP, MPJPE, user study ç”¨æˆ·ç ”ç©¶
   - Experimental setup å®éªŒè®¾ç½®

5. **Results ç»“æœ** (3-4 pages 3-4é¡µ)
   - Benchmark performance vs. baselines åŸºå‡†æ€§èƒ½å¯¹æ¯”åŸºçº¿
   - User study findings ç”¨æˆ·ç ”ç©¶å‘ç°
   - Latency/battery measurements å»¶è¿Ÿ/ç”µæ± æµ‹é‡
   - Ablation studies (with/without EMG æ¶ˆèç ”ç©¶ï¼Œæœ‰/æ— EMG)

6. **Discussion è®¨è®º** (2 pages 2é¡µ)
   - Insights æ´å¯Ÿ: EMG value, haptic effectiveness EMGä»·å€¼ã€hapticæœ‰æ•ˆæ€§
   - Limitations é™åˆ¶: Sensor placement, cost ä¼ æ„Ÿå™¨æ”¾ç½®ã€æˆæœ¬
   - Future work æœªæ¥å·¥ä½œ: Advanced sensors, more exercises é«˜çº§ä¼ æ„Ÿå™¨ã€æ›´å¤šç»ƒä¹ 

7. **Conclusion ç»“è®º** (1 page 1é¡µ)
   - Summary of contributions è´¡çŒ®æ‘˜è¦
   - Impact statement å½±å“å£°æ˜
   - Open-source release å¼€æºå‘å¸ƒ

**Total Length æ€»é•¿åº¦**: 12-14 pages (CHI/IMWUT standard CHI/IMWUTæ ‡å‡†)

---

### 6.4 Supplementary Materials è¡¥å……ææ–™

**What to Release å‘å¸ƒå†…å®¹** (for reproducibility + citations ç”¨äºå¯é‡ç°æ€§+å¼•ç”¨):

1. **Code ä»£ç **:

   ```
   github.com/movement-chain-ai/multimodal-feedback
   â”œâ”€â”€ pose_estimation/      # RTMPose integration RTMPoseé›†æˆ
   â”œâ”€â”€ sensor_fusion/        # IMU + EMG processing IMU + EMGå¤„ç†
   â”œâ”€â”€ feedback_generation/  # Feedback logic åé¦ˆé€»è¾‘
   â””â”€â”€ evaluation/           # Benchmark scripts åŸºå‡†è„šæœ¬
   ```

2. **Dataset æ•°æ®é›†** (if allowed å¦‚å…è®¸):
   - Custom EMG + haptic dataset è‡ªå®šä¹‰EMG + hapticæ•°æ®é›†
   - Annotations and labels æ³¨é‡Šå’Œæ ‡ç­¾
   - Pre-processing scripts é¢„å¤„ç†è„šæœ¬

3. **Models æ¨¡å‹**:
   - Pre-trained weights é¢„è®­ç»ƒæƒé‡
   - ONNX exports for deployment ç”¨äºéƒ¨ç½²çš„ONNXå¯¼å‡º

4. **Documentation æ–‡æ¡£**:
   - Setup guide è®¾ç½®æŒ‡å—
   - API reference APIå‚è€ƒ
   - Tutorial notebooks æ•™ç¨‹ç¬”è®°æœ¬

**Impact å½±å“**: Open-source releases typically increase citations by 2-3x å¼€æºå‘å¸ƒé€šå¸¸å°†å¼•ç”¨å¢åŠ 2-3å€

---

## 7. Citation Requirements å¼•ç”¨è¦æ±‚

### 7.1 Datasets - BibTeX Entries æ•°æ®é›† - BibTeXæ¡ç›®

#### Fit3D / AIFit

```bibtex
@inproceedings{fieraru2021aifit,
  title={AIFit: Automatic 3D Human-Interpretable Feedback Models for Fitness Training},
  author={Fieraru, Mihai and Khoreva, Anna and Pishchulin, Leonid and Plank, Pia and Andriluka, Mihai and Schiele, Bernt and Sminchisescu, Cristian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={5148--5158},
  year={2021}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using Fit3D dataset ä½¿ç”¨Fit3Dæ•°æ®é›†
- Comparing against AIFit methodology ä¸AIFitæ–¹æ³•æ¯”è¾ƒ
- Referencing feedback system design å‚è€ƒåé¦ˆç³»ç»Ÿè®¾è®¡

---

#### MM-Fit

```bibtex
@article{stromback2020mmfit,
  title={MM-Fit: Multimodal Deep Learning for Automatic Exercise Logging across Sensing Devices},
  author={Str{\"o}mback, Kristin Davina and Menges, Livia and Goswami, Ramesh and Ogunbanjo, Temitope and Lee, Heyoung},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)},
  volume={4},
  number={4},
  pages={1--22},
  year={2020},
  publisher={ACM}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using MM-Fit dataset ä½¿ç”¨MM-Fitæ•°æ®é›†
- Multimodal learning comparisons å¤šæ¨¡æ€å­¦ä¹ æ¯”è¾ƒ
- Sensor fusion validation ä¼ æ„Ÿå™¨èåˆéªŒè¯

---

#### FLAG3D

```bibtex
@inproceedings{tang2023flag3d,
  title={FLAG3D: A 3D Fitness Activity Dataset with Language Instruction},
  author={Tang, Yansong and Pan, Jinpeng and Chen, Kai and Xie, Yifang and Zhu, Yifan and Zhao, Wenxun and Li, Jian and Lu, Jiwen and Zhou, Jie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={19638--19648},
  year={2023}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using FLAG3D dataset ä½¿ç”¨FLAG3Dæ•°æ®é›†
- Natural language feedback generation è‡ªç„¶è¯­è¨€åé¦ˆç”Ÿæˆ
- Language-conditioned pose estimation è¯­è¨€æ¡ä»¶å§¿æ€ä¼°è®¡

---

#### Microsoft RecoFit

```bibtex
@inproceedings{morris2014recofit,
  title={RecoFit: Using a Wearable Sensor to Find, Recognize, and Count Repetitive Exercises},
  author={Morris, Dan and Saponas, T. Scott and Guillory, Andrew and Kelner, Itamar},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI)},
  pages={3225--3234},
  year={2014},
  publisher={ACM}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using RecoFit dataset ä½¿ç”¨RecoFitæ•°æ®é›†
- IMU-based exercise recognition åŸºäºIMUçš„è¿åŠ¨è¯†åˆ«
- Wearable sensor baselines å¯ç©¿æˆ´ä¼ æ„Ÿå™¨åŸºçº¿

---

#### COCO Keypoints

```bibtex
@inproceedings{lin2014microsoft,
  title={Microsoft COCO: Common Objects in Context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={740--755},
  year={2014},
  organization={Springer}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using COCO dataset ä½¿ç”¨COCOæ•°æ®é›†
- Reporting COCO AP metric æŠ¥å‘ŠCOCO APæŒ‡æ ‡
- Pre-training on COCO åœ¨COCOä¸Šé¢„è®­ç»ƒ

---

#### MPII Human Pose

```bibtex
@inproceedings{andriluka20142d,
  title={2D Human Pose Estimation: New Benchmark and State of the Art Analysis},
  author={Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3686--3693},
  year={2014}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using MPII dataset ä½¿ç”¨MPIIæ•°æ®é›†
- Evaluating pose estimation robustness è¯„ä¼°å§¿æ€ä¼°è®¡é²æ£’æ€§

---

### 7.2 Methods - BibTeX Entries æ–¹æ³• - BibTeXæ¡ç›®

#### RTMPose

```bibtex
@misc{jiang2023rtmpose,
  title={RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose},
  author={Jiang, Tao and Lu, Peng and Zhang, Li and Ma, Ningsheng and Han, Rui and Lyu, Chengqi and Li, Yining and Chen, Kai},
  journal={arXiv preprint arXiv:2303.07399},
  year={2023}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using RTMPose for pose estimation ä½¿ç”¨RTMPoseè¿›è¡Œå§¿æ€ä¼°è®¡
- Comparing performance against RTMPose ä¸RTMPoseæ¯”è¾ƒæ€§èƒ½
- Discussing real-time pose methods è®¨è®ºå®æ—¶å§¿æ€æ–¹æ³•

---

#### MediaPipe BlazePose

```bibtex
@article{bazarevsky2020blazepose,
  title={BlazePose: On-device Real-time Body Pose Tracking},
  author={Bazarevsky, Valentin and Grishchenko, Ivan and Raveendran, Karthik and Zhu, Tyler and Zhang, Fan and Grundmann, Matthias},
  journal={arXiv preprint arXiv:2006.10204},
  year={2020}
}
```

**When to cite ä½•æ—¶å¼•ç”¨**:

- Using MediaPipe Pose ä½¿ç”¨MediaPipe Pose
- Mobile pose estimation ç§»åŠ¨å§¿æ€ä¼°è®¡
- 3D pose landmark discussion 3Då§¿æ€å…³é”®ç‚¹è®¨è®º

---

### 7.3 Attribution Guidelines å½’å±æŒ‡å—

#### Required Citations å¿…éœ€å¼•ç”¨

**In Paper Introduction/Related Work åœ¨è®ºæ–‡å¼•è¨€/ç›¸å…³å·¥ä½œä¸­**:

- All datasets used for training/validation æ‰€æœ‰ç”¨äºè®­ç»ƒ/éªŒè¯çš„æ•°æ®é›†
- All baseline methods compared against æ‰€æœ‰æ¯”è¾ƒçš„åŸºçº¿æ–¹æ³•
- Prior work that directly influenced design ç›´æ¥å½±å“è®¾è®¡çš„å…ˆå‰å·¥ä½œ

**In Methods Section åœ¨æ–¹æ³•éƒ¨åˆ†**:

- Specific algorithms implemented å®ç°çš„ç‰¹å®šç®—æ³•
- Pre-trained models used ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹
- Evaluation metrics (cite COCO paper if using COCO AP è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚ä½¿ç”¨COCO APåˆ™å¼•ç”¨COCOè®ºæ–‡)

**In Results/Discussion åœ¨ç»“æœ/è®¨è®ºä¸­**:

- Benchmark datasets for comparison ç”¨äºæ¯”è¾ƒçš„åŸºå‡†æ•°æ®é›†
- Competing systems referenced å‚è€ƒçš„ç«äº‰ç³»ç»Ÿ

#### Dataset License Compliance æ•°æ®é›†è®¸å¯åˆè§„

**Fit3D**:

- âœ… Cite AIFit paper å¼•ç”¨AIFitè®ºæ–‡
- âœ… Mention dataset usage in acknowledgments åœ¨è‡´è°¢ä¸­æåŠæ•°æ®é›†ä½¿ç”¨
- âœ… Follow academic use restrictions (verify license éµå¾ªå­¦æœ¯ä½¿ç”¨é™åˆ¶ï¼ŒéªŒè¯è®¸å¯)

**MM-Fit**:

- âœ… Cite MM-Fit paper å¼•ç”¨MM-Fitè®ºæ–‡
- âœ… Acknowledge authors è‡´è°¢ä½œè€…
- âœ… Verify current license terms (check GitHub éªŒè¯å½“å‰è®¸å¯æ¡æ¬¾ï¼Œæ£€æŸ¥GitHub)

**FLAG3D**:

- âœ… Cite FLAG3D paper å¼•ç”¨FLAG3Dè®ºæ–‡
- âœ… Follow any specific attribution requirements éµå¾ªä»»ä½•ç‰¹å®šå½’å±è¦æ±‚

**RecoFit**:

- âœ… Cite RecoFit paper å¼•ç”¨RecoFitè®ºæ–‡
- âœ… Follow Microsoft Research license éµå¾ªMicrosoft Researchè®¸å¯

**COCO / MPII**:

- âœ… Cite original dataset papers å¼•ç”¨åŸå§‹æ•°æ®é›†è®ºæ–‡
- âœ… Standard academic use (permissive æ ‡å‡†å­¦æœ¯ä½¿ç”¨ï¼Œå®½æ¾)

---

### 7.4 Acknowledgments Template è‡´è°¢æ¨¡æ¿

**Example Text ç¤ºä¾‹æ–‡æœ¬** (adapt for your paper ä¸ºæ‚¨çš„è®ºæ–‡è°ƒæ•´):

```
We thank the creators of the Fit3D dataset (Fieraru et al., 2021) for providing access
to their motion capture data. This work utilized the MM-Fit multimodal dataset
(StrÃ¶mback et al., 2020) for sensor fusion validation. We acknowledge the FLAG3D team
for their language-annotated fitness dataset and Microsoft Research for the RecoFit
wearable sensor data. Pose estimation experiments were conducted using models pre-trained
on the COCO Keypoints dataset (Lin et al., 2014).

æˆ‘ä»¬æ„Ÿè°¢Fit3Dæ•°æ®é›†ï¼ˆFieraruç­‰ï¼Œ2021ï¼‰çš„åˆ›å»ºè€…æä¾›å¯¹å…¶è¿åŠ¨æ•æ‰æ•°æ®çš„è®¿é—®ã€‚
æœ¬å·¥ä½œåˆ©ç”¨MM-Fitå¤šæ¨¡æ€æ•°æ®é›†ï¼ˆStrÃ¶mbackç­‰ï¼Œ2020ï¼‰è¿›è¡Œä¼ æ„Ÿå™¨èåˆéªŒè¯ã€‚
æˆ‘ä»¬è‡´è°¢FLAG3Då›¢é˜Ÿæä¾›çš„è¯­è¨€æ ‡æ³¨å¥èº«æ•°æ®é›†å’ŒMicrosoft Researchæä¾›çš„RecoFit
å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ã€‚å§¿æ€ä¼°è®¡å®éªŒä½¿ç”¨åœ¨COCOå…³é”®ç‚¹æ•°æ®é›†ï¼ˆLinç­‰ï¼Œ2014ï¼‰ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œã€‚

[If applicable å¦‚é€‚ç”¨] This research was supported by [grant/funding source èµ„åŠ©/èµ„é‡‘æ¥æº]. We thank the
participants in our user study for their time and feedback.
æœ¬ç ”ç©¶å¾—åˆ°[èµ„åŠ©/èµ„é‡‘æ¥æº]çš„æ”¯æŒã€‚æˆ‘ä»¬æ„Ÿè°¢ç”¨æˆ·ç ”ç©¶å‚ä¸è€…çš„æ—¶é—´å’Œåé¦ˆã€‚
```

---

## 8. Curated Research & Resource Lists ç²¾é€‰ç ”ç©¶ä¸èµ„æºåˆ—è¡¨

These "Awesome" GitHub repositories aggregate papers, datasets, and tools. Useful for literature review and discovering new research.

è¿™äº›"Awesome"GitHubä»“åº“æ±‡æ€»äº†è®ºæ–‡ã€æ•°æ®é›†å’Œå·¥å…·ã€‚å¯¹æ–‡çŒ®ç»¼è¿°å’Œå‘ç°æ–°ç ”ç©¶æœ‰ç”¨ã€‚

### 8.1 Awesome-IMU-Sensing

**Repository ä»“åº“**: <https://github.com/rh20624/Awesome-IMU-Sensing>

#### What's Included åŒ…å«å†…å®¹

- Academic papers on IMU-based sensing and signal processing åŸºäºIMUçš„æ„ŸçŸ¥å’Œä¿¡å·å¤„ç†å­¦æœ¯è®ºæ–‡
- Public IMU datasets (comprehensive list å…¬å…±IMUæ•°æ®é›†ï¼Œå…¨é¢åˆ—è¡¨)
- Signal processing techniques for IMU data IMUæ•°æ®ä¿¡å·å¤„ç†æŠ€æœ¯
- Sensor fusion algorithms and implementations ä¼ æ„Ÿå™¨èåˆç®—æ³•å’Œå®ç°
- Activity recognition methods using IMU sensors ä½¿ç”¨IMUä¼ æ„Ÿå™¨çš„æ´»åŠ¨è¯†åˆ«æ–¹æ³•
- State-of-art approaches and benchmark results æœ€å…ˆè¿›æ–¹æ³•å’ŒåŸºå‡†ç»“æœ

#### Why It's Valuable ä¸ºä»€ä¹ˆæœ‰ä»·å€¼

âœ… **Comprehensive IMU Research Literature å…¨é¢çš„IMUç ”ç©¶æ–‡çŒ®**:

- Discover IMU datasets we might have missed å‘ç°æˆ‘ä»¬å¯èƒ½é”™è¿‡çš„IMUæ•°æ®é›†
- Find state-of-art sensor fusion approaches æ‰¾åˆ°æœ€å…ˆè¿›çš„ä¼ æ„Ÿå™¨èåˆæ–¹æ³•
- Access academic citations for our papers è®¿é—®æˆ‘ä»¬è®ºæ–‡çš„å­¦æœ¯å¼•ç”¨
- Learn IMU signal processing best practices å­¦ä¹ IMUä¿¡å·å¤„ç†æœ€ä½³å®è·µ

**Best Use Cases æœ€ä½³ç”¨ä¾‹**:

1. **Literature Review æ–‡çŒ®ç»¼è¿°**: Survey IMU research for our IMU module ä¸ºæˆ‘ä»¬çš„IMUæ¨¡å—è°ƒæŸ¥IMUç ”ç©¶
2. **Dataset Discovery æ•°æ®é›†å‘ç°**: Find specialized IMU datasets beyond our main list æ‰¾åˆ°è¶…å‡ºæˆ‘ä»¬ä¸»åˆ—è¡¨çš„ä¸“ä¸šIMUæ•°æ®é›†
3. **Algorithm Reference ç®—æ³•å‚è€ƒ**: Implement sensor fusion algorithms å®ç°ä¼ æ„Ÿå™¨èåˆç®—æ³•
4. **Citation Source å¼•ç”¨æ¥æº**: Proper academic citations for methodology æ–¹æ³•è®ºçš„æ­£ç¡®å­¦æœ¯å¼•ç”¨

**When to Use ä½•æ—¶ä½¿ç”¨**:

- **Phase 1 ç¬¬1é˜¶æ®µ**: Quick review of IMU processing techniques IMUå¤„ç†æŠ€æœ¯å¿«é€Ÿå›é¡¾
- **Phase 2 ç¬¬2é˜¶æ®µ**: Deep dive into sensor fusion algorithms æ·±å…¥ä¼ æ„Ÿå™¨èåˆç®—æ³•
- **Phase 3 ç¬¬3é˜¶æ®µ**: Comprehensive related work section for papers è®ºæ–‡å…¨é¢ç›¸å…³å·¥ä½œéƒ¨åˆ†

---

### 8.2 Awesome-Human-Activity-Recognition

**Repository ä»“åº“**: <https://github.com/haoranD/Awesome-Human-Activity-Recognition>

#### What's Included åŒ…å«å†…å®¹

- HAR (Human Activity Recognition äººç±»æ´»åŠ¨è¯†åˆ«) papers and surveys è®ºæ–‡å’Œç»¼è¿°
- Public datasets for activity recognition æ´»åŠ¨è¯†åˆ«å…¬å…±æ•°æ®é›†
- Deep learning methods for HAR ç”¨äºHARçš„æ·±åº¦å­¦ä¹ æ–¹æ³•
- Sensor-based and vision-based approaches åŸºäºä¼ æ„Ÿå™¨å’ŒåŸºäºè§†è§‰çš„æ–¹æ³•
- Benchmark results and performance comparisons åŸºå‡†ç»“æœå’Œæ€§èƒ½æ¯”è¾ƒ
- Code implementations and pretrained models ä»£ç å®ç°å’Œé¢„è®­ç»ƒæ¨¡å‹

#### Why It's Valuable ä¸ºä»€ä¹ˆæœ‰ä»·å€¼

âœ… **Comprehensive HAR Research Overview å…¨é¢çš„HARç ”ç©¶æ¦‚è¿°**:

- Compare our approach to state-of-art methods å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›æ–¹æ³•æ¯”è¾ƒ
- Discover relevant datasets with labeled exercise data å‘ç°å¸¦æ ‡æ³¨è¿åŠ¨æ•°æ®çš„ç›¸å…³æ•°æ®é›†
- Find baseline algorithms for comparison æ‰¾åˆ°ç”¨äºæ¯”è¾ƒçš„åŸºçº¿ç®—æ³•
- Access recent papers (last 2 years) for related work è®¿é—®æœ€è¿‘è®ºæ–‡ï¼ˆè¿‡å»2å¹´ï¼‰ç”¨äºç›¸å…³å·¥ä½œ

**Best Use Cases æœ€ä½³ç”¨ä¾‹**:

1. **Survey HAR Methods è°ƒæŸ¥HARæ–¹æ³•**: Understand exercise recognition approaches ç†è§£è¿åŠ¨è¯†åˆ«æ–¹æ³•
2. **Dataset Mining æ•°æ®é›†æŒ–æ˜**: Find datasets with labeled exercise data æ‰¾åˆ°å¸¦æ ‡æ³¨è¿åŠ¨æ•°æ®çš„æ•°æ®é›†
3. **Benchmark Comparisons åŸºå‡†æ¯”è¾ƒ**: Compare against published baselines ä¸å·²å‘è¡¨åŸºçº¿æ¯”è¾ƒ
4. **Implementation Reference å®ç°å‚è€ƒ**: Access code for HAR algorithms è®¿é—®HARç®—æ³•ä»£ç 

**When to Use ä½•æ—¶ä½¿ç”¨**:

- **Phase 1 ç¬¬1é˜¶æ®µ**: Understand HAR landscape ç†è§£HARæ ¼å±€
- **Phase 2 ç¬¬2é˜¶æ®µ**: Implement baseline algorithms for comparison å®ç°åŸºçº¿ç®—æ³•ç”¨äºæ¯”è¾ƒ
- **Phase 3 ç¬¬3é˜¶æ®µ**: Comprehensive related work and benchmark section å…¨é¢ç›¸å…³å·¥ä½œå’ŒåŸºå‡†éƒ¨åˆ†

---

### 8.3 Using Curated Lists Effectively æœ‰æ•ˆä½¿ç”¨ç²¾é€‰åˆ—è¡¨

#### For Research Phase ç ”ç©¶é˜¶æ®µ

**Step-by-step approach åˆ†æ­¥æ–¹æ³•**:

1. **Start with Awesome-IMU-Sensing ä»Awesome-IMU-Sensingå¼€å§‹**:
   - Survey sensor fusion research è°ƒæŸ¥ä¼ æ„Ÿå™¨èåˆç ”ç©¶
   - Find IMU-specific datasets æ‰¾åˆ°IMUç‰¹å®šæ•°æ®é›†
   - Identify preprocessing techniques è¯†åˆ«é¢„å¤„ç†æŠ€æœ¯
   - Note recent papers (2023-2025 æ³¨æ„æœ€è¿‘è®ºæ–‡ï¼Œ2023-2025)

2. **Move to Awesome-HAR è½¬åˆ°Awesome-HAR**:
   - Survey exercise recognition methods è°ƒæŸ¥è¿åŠ¨è¯†åˆ«æ–¹æ³•
   - Find activity recognition datasets æ‰¾åˆ°æ´»åŠ¨è¯†åˆ«æ•°æ®é›†
   - Compare deep learning approaches æ¯”è¾ƒæ·±åº¦å­¦ä¹ æ–¹æ³•
   - Identify benchmark baselines è¯†åˆ«åŸºå‡†åŸºçº¿

3. **Mine for Datasets æŒ–æ˜æ•°æ®é›†**:
   - Cross-reference with our existing list ä¸æˆ‘ä»¬ç°æœ‰åˆ—è¡¨äº¤å‰å¼•ç”¨
   - Look for recently added datasets æŸ¥æ‰¾æœ€è¿‘æ·»åŠ çš„æ•°æ®é›†
   - Check for specialized use cases (sports, fitness æ£€æŸ¥ä¸“ä¸šç”¨ä¾‹ï¼Œä½“è‚²ã€å¥èº«)

4. **Find Recent Papers æ‰¾åˆ°æœ€è¿‘è®ºæ–‡**:
   - Sort by date (prefer 2023-2025 æŒ‰æ—¥æœŸæ’åºï¼Œåå¥½2023-2025)
   - Focus on papers with code available å…³æ³¨æœ‰ä»£ç å¯ç”¨çš„è®ºæ–‡
   - Note highly cited works æ³¨æ„é«˜å¼•ç”¨ä½œå“

#### For Development Phase å¼€å‘é˜¶æ®µ

**Practical applications å®é™…åº”ç”¨**:

- **Reference Implementations å‚è€ƒå®ç°**: Find code for sensor fusion algorithms æ‰¾åˆ°ä¼ æ„Ÿå™¨èåˆç®—æ³•ä»£ç 
- **Preprocessing Techniques é¢„å¤„ç†æŠ€æœ¯**: Learn IMU signal filtering, smoothing å­¦ä¹ IMUä¿¡å·è¿‡æ»¤ã€å¹³æ»‘
- **Feature Engineering ç‰¹å¾å·¥ç¨‹**: Discover effective IMU features å‘ç°æœ‰æ•ˆçš„IMUç‰¹å¾
- **Model Architectures æ¨¡å‹æ¶æ„**: Study successful deep learning designs ç ”ç©¶æˆåŠŸçš„æ·±åº¦å­¦ä¹ è®¾è®¡

**Example workflow ç¤ºä¾‹å·¥ä½œæµ**:

```bash
# Clone Awesome repos for offline reference å…‹éš†Awesomeä»“åº“ä¾›ç¦»çº¿å‚è€ƒ
git clone https://github.com/rh20624/Awesome-IMU-Sensing.git
git clone https://github.com/haoranD/Awesome-Human-Activity-Recognition.git

# Extract dataset links æå–æ•°æ®é›†é“¾æ¥
grep -E "http.*dataset" Awesome-IMU-Sensing/README.md > imu_datasets.txt

# Find recent papers (manually check dates æ‰¾åˆ°æœ€è¿‘è®ºæ–‡ï¼Œæ‰‹åŠ¨æ£€æŸ¥æ—¥æœŸ)
grep -E "2024|2025" Awesome-Human-Activity-Recognition/README.md
```

#### For Publication Phase å‘è¡¨é˜¶æ®µ

**Building your related work section æ„å»ºæ‚¨çš„ç›¸å…³å·¥ä½œéƒ¨åˆ†**:

1. **Comprehensive Coverage å…¨é¢è¦†ç›–**:
   - Use lists to ensure you didn't miss major work ä½¿ç”¨åˆ—è¡¨ç¡®ä¿æ‚¨æ²¡æœ‰é”™è¿‡é‡è¦å·¥ä½œ
   - Cite representative papers from each category å¼•ç”¨æ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§è®ºæ–‡
   - Demonstrate thorough literature review å±•ç¤ºå½»åº•çš„æ–‡çŒ®ç»¼è¿°

2. **Proper Comparison æ­£ç¡®æ¯”è¾ƒ**:
   - Compare against state-of-art from recent years ä¸è¿‘å¹´æ¥æœ€å…ˆè¿›çš„æ¯”è¾ƒ
   - Reference benchmark results from papers å¼•ç”¨è®ºæ–‡ä¸­çš„åŸºå‡†ç»“æœ
   - Position your work in context å°†æ‚¨çš„å·¥ä½œç½®äºèƒŒæ™¯ä¸­

3. **Citation Quality å¼•ç”¨è´¨é‡**:
   - Cite original papers, not just surveys å¼•ç”¨åŸå§‹è®ºæ–‡ï¼Œä¸ä»…ä»…æ˜¯ç»¼è¿°
   - Include both classic and recent work åŒ…æ‹¬ç»å…¸å’Œæœ€è¿‘çš„å·¥ä½œ
   - Balance breadth and depth å¹³è¡¡å¹¿åº¦å’Œæ·±åº¦

**Example related work structure ç¤ºä¾‹ç›¸å…³å·¥ä½œç»“æ„**:

```
Related Work ç›¸å…³å·¥ä½œ:
â”œâ”€â”€ IMU-based Exercise Recognition åŸºäºIMUçš„è¿åŠ¨è¯†åˆ«
â”‚   â”œâ”€â”€ Classical ML approaches ç»å…¸MLæ–¹æ³• [cite 3-5 papers å¼•ç”¨3-5ç¯‡è®ºæ–‡]
â”‚   â”œâ”€â”€ Deep learning methods æ·±åº¦å­¦ä¹ æ–¹æ³• [cite 5-7 papers å¼•ç”¨5-7ç¯‡è®ºæ–‡]
â”‚   â””â”€â”€ Commercial systems å•†ä¸šç³»ç»Ÿ [cite 2-3 systems å¼•ç”¨2-3ä¸ªç³»ç»Ÿ]
â”œâ”€â”€ Vision-based Pose Estimation åŸºäºè§†è§‰çš„å§¿æ€ä¼°è®¡
â”‚   â”œâ”€â”€ 2D pose estimation 2Då§¿æ€ä¼°è®¡ [cite 3-5 papers å¼•ç”¨3-5ç¯‡è®ºæ–‡]
â”‚   â”œâ”€â”€ 3D pose reconstruction 3Då§¿æ€é‡å»º [cite 3-5 papers å¼•ç”¨3-5ç¯‡è®ºæ–‡]
â”‚   â””â”€â”€ Real-time methods å®æ—¶æ–¹æ³• [cite 2-4 papers å¼•ç”¨2-4ç¯‡è®ºæ–‡]
â””â”€â”€ Multimodal Sensor Fusion å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆ
    â”œâ”€â”€ IMU + Vision fusion IMU+è§†è§‰èåˆ [cite 3-5 papers å¼•ç”¨3-5ç¯‡è®ºæ–‡]
    â”œâ”€â”€ Sensor fusion algorithms ä¼ æ„Ÿå™¨èåˆç®—æ³• [cite 2-4 papers å¼•ç”¨2-4ç¯‡è®ºæ–‡]
    â””â”€â”€ Real-time multimodal systems å®æ—¶å¤šæ¨¡æ€ç³»ç»Ÿ [cite 2-3 papers å¼•ç”¨2-3ç¯‡è®ºæ–‡]

Total æ€»è®¡: ~25-35 citations (appropriate for CHI/IMWUT paper é€‚åˆCHI/IMWUTè®ºæ–‡)
```

#### Update Strategy æ›´æ–°ç­–ç•¥

**How often to check æ£€æŸ¥é¢‘ç‡**:

- **Initial review åˆæ­¥å›é¡¾**: Spend 2-3 hours thoroughly reviewing both lists èŠ±2-3å°æ—¶å½»åº•å›é¡¾ä¸¤ä¸ªåˆ—è¡¨
- **Regular updates å®šæœŸæ›´æ–°**: Check quarterly for new additions æ¯å­£åº¦æ£€æŸ¥æ–°å¢å†…å®¹
- **Pre-submission æäº¤å‰**: Final check before paper submission è®ºæ–‡æäº¤å‰æœ€ç»ˆæ£€æŸ¥

**What to look for æŸ¥æ‰¾å†…å®¹**:

- âœ… Recently added datasets (might be more relevant æœ€è¿‘æ·»åŠ çš„æ•°æ®é›†ï¼Œå¯èƒ½æ›´ç›¸å…³)
- âœ… Papers from 2024-2025 (most recent work 2024-2025å¹´è®ºæ–‡ï¼Œæœ€æ–°å·¥ä½œ)
- âœ… Papers with code available (for comparison æœ‰ä»£ç å¯ç”¨çš„è®ºæ–‡ï¼Œç”¨äºæ¯”è¾ƒ)
- âœ… Highly starred/forked repos (quality indicator é«˜æ˜Ÿ/åˆ†å‰ä»“åº“ï¼Œè´¨é‡æŒ‡æ ‡)

**Community contribution ç¤¾åŒºè´¡çŒ®**:

- Both repositories are actively maintained ä¸¤ä¸ªä»“åº“éƒ½ç§¯æç»´æŠ¤
- Community contributions add new papers regularly ç¤¾åŒºè´¡çŒ®å®šæœŸæ·»åŠ æ–°è®ºæ–‡
- You can contribute by adding our work after publication å‘è¡¨åæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ æˆ‘ä»¬çš„å·¥ä½œåšå‡ºè´¡çŒ®

---

### 8.4 Additional Awesome Lists (Secondary Priority) å…¶ä»–Awesomeåˆ—è¡¨ï¼ˆæ¬¡è¦ä¼˜å…ˆçº§ï¼‰

#### Awesome-Pose-Estimation

**Repository ä»“åº“**: Various (search GitHub for "awesome-pose-estimation" å„ç§ï¼Œåœ¨GitHubæœç´¢"awesome-pose-estimation")

**Use for ç”¨äº**:

- Comprehensive pose estimation methods å…¨é¢çš„å§¿æ€ä¼°è®¡æ–¹æ³•
- Pre-trained model links é¢„è®­ç»ƒæ¨¡å‹é“¾æ¥
- Benchmark comparisons åŸºå‡†æ¯”è¾ƒ

#### Awesome-Fitness-Tech

**Note æ³¨æ„**: Less formal but useful for commercial product research ä¸å¤ªæ­£å¼ä½†å¯¹å•†ä¸šäº§å“ç ”ç©¶æœ‰ç”¨

**Use for ç”¨äº**:

- Surveying commercial fitness products è°ƒæŸ¥å•†ä¸šå¥èº«äº§å“
- Understanding market landscape ç†è§£å¸‚åœºæ ¼å±€
- Identifying feature gaps è¯†åˆ«åŠŸèƒ½å·®è·

---

### Quick Reference: When to Use Each Resource å¿«é€Ÿå‚è€ƒï¼šä½•æ—¶ä½¿ç”¨æ¯ä¸ªèµ„æº

| Phase é˜¶æ®µ | Awesome-IMU-Sensing | Awesome-HAR | Priority ä¼˜å…ˆçº§ |
|-------|---------------------|-------------|----------|
| **MVP (Month 1-3) MVPï¼ˆç¬¬1-3ä¸ªæœˆï¼‰** | Quick IMU technique review å¿«é€ŸIMUæŠ€æœ¯å›é¡¾ | Quick HAR overview å¿«é€ŸHARæ¦‚è¿° | Low ä½ |
| **Development (Month 3-6) å¼€å‘ï¼ˆç¬¬3-6ä¸ªæœˆï¼‰** | Algorithm implementation ç®—æ³•å®ç° | Baseline comparison åŸºçº¿æ¯”è¾ƒ | Medium ä¸­ç­‰ |
| **Research (Month 6-12) ç ”ç©¶ï¼ˆç¬¬6-12ä¸ªæœˆï¼‰** | Comprehensive literature å…¨é¢æ–‡çŒ® | Related work section ç›¸å…³å·¥ä½œéƒ¨åˆ† | High é«˜ |
| **Publication (Month 12+) å‘è¡¨ï¼ˆç¬¬12ä¸ªæœˆ+ï¼‰** | Citation completeness å¼•ç”¨å®Œæ•´æ€§ | State-of-art comparison æœ€å…ˆè¿›æ¯”è¾ƒ | High é«˜ |

**Pro Tip ä¸“ä¸šæç¤º**: Bookmark both repositories and check the "Recently Updated" section monthly for new papers and datasets.

å°†ä¸¤ä¸ªä»“åº“åŠ å…¥ä¹¦ç­¾ï¼Œæ¯æœˆæ£€æŸ¥"æœ€è¿‘æ›´æ–°"éƒ¨åˆ†è·å–æ–°è®ºæ–‡å’Œæ•°æ®é›†ã€‚

---

## Summary: Immediate Action Items æ‘˜è¦ï¼šç«‹å³è¡ŒåŠ¨é¡¹

### Week 1 Tasks (Do NOW) ç¬¬1å‘¨ä»»åŠ¡ï¼ˆç«‹å³æ‰§è¡Œï¼‰

âœ… **Download MM-Fit ä¸‹è½½MM-Fit**:

```bash
git clone https://github.com/KDMStromback/mm-fit.git
# Follow download instructions on GitHub éµå¾ªGitHubä¸Šçš„ä¸‹è½½è¯´æ˜
```

âœ… **Download RecoFit ä¸‹è½½RecoFit**:

```bash
git clone https://github.com/microsoft/Exercise-Recognition-from-Wearable-Sensors.git
```

âœ… **Apply for Fit3D Access ç”³è¯·Fit3Dè®¿é—®æƒé™**:

- Visit è®¿é—®: <https://fit3d.imar.ro/>
- Prepare research description å‡†å¤‡ç ”ç©¶æè¿°
- Submit application æäº¤ç”³è¯·

âœ… **Download FLAG3D (optional, lower priority å¯é€‰ï¼Œè¾ƒä½ä¼˜å…ˆçº§)** :

- Visit è®¿é—®: <https://andytang15.github.io/FLAG3D/>
- Download dataset ä¸‹è½½æ•°æ®é›†

### Month 1-2 Tasks ç¬¬1-2ä¸ªæœˆä»»åŠ¡

âœ… **Benchmark on MM-Fit åœ¨MM-Fitä¸ŠåŸºå‡†æµ‹è¯•**:

- Test pose estimation accuracy æµ‹è¯•å§¿æ€ä¼°è®¡ç²¾åº¦
- Validate sensor fusion approach éªŒè¯ä¼ æ„Ÿå™¨èåˆæ–¹æ³•
- Measure baseline performance æµ‹é‡åŸºçº¿æ€§èƒ½

âœ… **Test IMU on RecoFit åœ¨RecoFitä¸Šæµ‹è¯•IMU**:

- Validate rep counting éªŒè¯æ¬¡æ•°è®¡æ•°
- Test exercise recognition æµ‹è¯•è¿åŠ¨è¯†åˆ«
- Compare IMU-only vs. multimodal æ¯”è¾ƒä»…IMUä¸å¤šæ¨¡æ€

âœ… **Setup Evaluation Pipeline è®¾ç½®è¯„ä¼°ç®¡é“**:

- Implement COCO AP calculation å®ç°COCO APè®¡ç®—
- Setup latency measurement è®¾ç½®å»¶è¿Ÿæµ‹é‡
- Design user study protocol è®¾è®¡ç”¨æˆ·ç ”ç©¶åè®®

### Month 3-6 Tasks ç¬¬3-6ä¸ªæœˆä»»åŠ¡

âœ… **If Fit3D Approved å¦‚æœFit3Dæ‰¹å‡†**:

- Download dataset ä¸‹è½½æ•°æ®é›†
- Benchmark against AIFit ä¸AIFitåŸºå‡†æ¯”è¾ƒ
- Compare feedback quality æ¯”è¾ƒåé¦ˆè´¨é‡

âœ… **Prepare Custom Dataset Collection å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†æ”¶é›†**:

- Design IRB protocol è®¾è®¡IRBåè®®
- Identify participants è¯†åˆ«å‚ä¸è€…
- Setup data collection pipeline è®¾ç½®æ•°æ®æ”¶é›†ç®¡é“

### Research Publication Timeline ç ”ç©¶å‘è¡¨æ—¶é—´çº¿

- **Month 6 ç¬¬6ä¸ªæœˆ**: Submit IRB application æäº¤IRBç”³è¯·
- **Month 8-10 ç¬¬8-10ä¸ªæœˆ**: Collect custom dataset æ”¶é›†è‡ªå®šä¹‰æ•°æ®é›†
- **Month 10-12 ç¬¬10-12ä¸ªæœˆ**: Analyze data, write paper åˆ†ææ•°æ®ã€æ’°å†™è®ºæ–‡
- **Month 12 ç¬¬12ä¸ªæœˆ**: Submit to CHI/IMWUT/CVPR æäº¤è‡³CHI/IMWUT/CVPR

---

**Last Updated æœ€åæ›´æ–°**: December 2025
**Maintainer ç»´æŠ¤è€…**: Movement Chain AI Research Team ç ”ç©¶å›¢é˜Ÿ
**Next Review ä¸‹æ¬¡å®¡æŸ¥**: Monthly (active development phase æ¯æœˆï¼Œæ´»è·ƒå¼€å‘é˜¶æ®µ)

**Questions or Suggestions é—®é¢˜æˆ–å»ºè®®?** Open an issue in the documentation repo. åœ¨æ–‡æ¡£ä»“åº“ä¸­æäº¤issueã€‚

---

## Quick Reference å¿«é€Ÿå‚è€ƒ

### Dataset Access URLs æ•°æ®é›†è®¿é—®ç½‘å€

- **Fit3D**: <https://fit3d.imar.ro/> (application required éœ€è¦ç”³è¯·)
- **MM-Fit**: <https://mmfit.github.io/> (public å…¬å¼€)
- **FLAG3D**: <https://andytang15.github.io/FLAG3D/> (public å…¬å¼€)
- **RecoFit**: <https://github.com/microsoft/Exercise-Recognition-from-Wearable-Sensors> (public å…¬å¼€)
- **motion-sense**: <https://github.com/mmalekzadeh/motion-sense> (public å…¬å¼€)
- **COCO**: <https://cocodataset.org/#keypoints-2020> (public å…¬å¼€)
- **MPII**: <http://human-pose.mpi-inf.mpg.de/> (public å…¬å¼€)

### Benchmark Targets åŸºå‡†ç›®æ ‡

- **Pose Accuracy å§¿æ€ç²¾åº¦**: â‰¥72% COCO AP
- **Mobile FPS ç§»åŠ¨FPS**: 30+ FPS
- **Latency å»¶è¿Ÿ**: <100ms end-to-end ç«¯åˆ°ç«¯
- **Battery ç”µæ± **: <20%/hour drain æ¯å°æ—¶æ¶ˆè€—
- **Cloud Cost äº‘æˆæœ¬**: <$20 per 1M inferences æ¯100ä¸‡æ¨ç†

### Publication Venues å‘è¡¨ä¼šè®®/æœŸåˆŠ

- **CHI**: May (deadline ~September 5æœˆï¼Œæˆªæ­¢æ—¥æœŸçº¦9æœˆ)
- **IMWUT**: Rolling submissions æ»šåŠ¨æäº¤
- **CVPR**: June (deadline ~November 6æœˆï¼Œæˆªæ­¢æ—¥æœŸçº¦11æœˆ)
