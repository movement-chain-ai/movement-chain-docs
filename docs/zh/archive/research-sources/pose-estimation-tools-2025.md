# å§¿æ€ä¼°è®¡å·¥å…·ä¸åº“ (Pose Estimation Tools & Libraries) (2025)

> Movement Chain AI æœ€å…ˆè¿›å§¿æ€ä¼°è®¡è§£å†³æ–¹æ¡ˆçš„ç»¼åˆæŒ‡å— (Comprehensive guide to state-of-the-art pose estimation solutions for Movement Chain AI)

---

## æ‰§è¡Œæ‘˜è¦ (Executive Summary)

æœ¬æ–‡æ¡£è¯„ä¼°äº†2025å¹´å¯ç”¨çš„å§¿æ€ä¼°è®¡å¼•æ“ï¼Œå¹¶æ ¹æ®æœ€æ–°æ€§èƒ½åŸºå‡†æä¾›äº†æ›´æ–°çš„å»ºè®®ã€‚**å…³é”®å‘ç° (Key finding)**: RTMPose ç°åœ¨åœ¨ç”Ÿäº§éƒ¨ç½²ä¸­æä¾›äº†ä¼˜äº MediaPipe çš„æ€§èƒ½ï¼Œè€Œ MediaPipe ç”±äºç”Ÿæ€ç³»ç»Ÿæˆç†Ÿåº¦ä»ç„¶æœ€é€‚åˆå¿«é€ŸåŸå‹å¼€å‘ã€‚

### å¿«é€Ÿæ¨è (Quick Recommendation)

| ä½¿ç”¨åœºæ™¯ Use Case | æ¨èå·¥å…· Recommended Tool | å¤‡é€‰æ–¹æ¡ˆ Runner-up |
|----------|------------------|-----------|
| **MVP / å¿«é€ŸåŸå‹å¼€å‘ Rapid Prototyping** | MediaPipe Pose | MoveNet Lightning |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆé«˜æ€§èƒ½ï¼‰Production (High Performance)** | **RTMPose-m** | MediaPipe Pose |
| **ç§»åŠ¨ç«¯ï¼ˆç”µæ± ä¼˜åŒ–ï¼‰Mobile (Battery Optimized)** | MoveNet Lightning | RTMPose-t (tiny) |
| **ç ”ç©¶/åŸºå‡†æµ‹è¯• Research / Benchmarking** | MMPose (toolbox) | ViTPose |
| **iOS åŸç”Ÿ iOS Native** | Apple Vision Framework | MediaPipe |
| **Web æµè§ˆå™¨ Web Browser** | PoseNet (TF.js) | MoveNet (TF.js) |

---

## 1. RTMPose - æ–°æ€§èƒ½é¢†å¯¼è€… (New Performance Leader) (2023-2025)

**æœ€é‡è¦çš„æ›´æ–° (Most Important Update)**: RTMPose åœ¨é€Ÿåº¦å’Œç²¾åº¦ä¸Šéƒ½è¶…è¶Šäº† MediaPipeã€‚

### æ¦‚è¿° (Overview)

- **å¼€å‘è€… Developer**: OpenMMLab (Open-source research lab)
- **å‘å¸ƒæ—¶é—´ Release**: March 2023 (CVPR 2023 Workshop)
- **çŠ¶æ€ Status**: Production-ready, actively maintained
- **è®¸å¯è¯ License**: Apache 2.0 (commercial-friendly)

### æ€§èƒ½æŒ‡æ ‡ (Performance Metrics)

**RTMPose-m (Medium variant)**:
```
Accuracy (COCO AP): 75.8%
Speed (CPU - Intel i7-11700): 90+ FPS
Speed (GPU - GTX 1660 Ti): 430+ FPS
Speed (Mobile - Snapdragon 865): 70+ FPS
```

**vs. MediaPipe BlazePose**:
```
                RTMPose-m    MediaPipe
Accuracy (AP):    75.8%        ~72%
CPU FPS:          90+          30-40
GPU FPS:          430+         120+
Mobile FPS:       70+          30+
```

**ç»“è®º (Verdict)**: RTMPose åœ¨æ‰€æœ‰å¹³å°ä¸Šéƒ½**æ›´å¿«ä¸”æ›´å‡†ç¡® (faster and more accurate)**ã€‚

### éƒ¨ç½²é€‰é¡¹ (Deployment Options)

#### é€‰é¡¹ 1 (Option 1): rtmlib (è½»é‡çº§ Lightweight - æ¨è Recommended)

**ä¸ºä»€ä¹ˆé€‰æ‹© rtmlib (Why rtmlib)**:
- **é›¶é‡å‹ä¾èµ– Zero heavy dependencies**: ä¸éœ€è¦ mmcv, mmpose, mmdet (No mmcv, mmpose, mmdet required)
- **ç®€å•å®‰è£… Simple install**: ä»…éœ€ numpy, opencv, onnxruntime (Just numpy, opencv, onnxruntime)
- **å°å ç”¨ç©ºé—´ Small footprint**: æœ€å°ç£ç›˜ç©ºé—´ (Minimal disk space)
- **å¿«é€Ÿé›†æˆ Fast integration**: å‡ å°æ—¶å³å¯æŠ•å…¥ç”Ÿäº§ (Production-ready in hours)

**å®‰è£… Installation**:
```bash
pip install rtmlib
```

**åŸºæœ¬ä½¿ç”¨ Basic Usage**:
```python
from rtmlib import PoseTracker

tracker = PoseTracker(
    model='rtmpose-m',
    backend='onnxruntime',  # or 'openvino', 'tensorrt'
    device='cpu'  # or 'cuda'
)

keypoints, scores = tracker(image)
```

**GitHub**: [https://github.com/Tau-J/rtmlib](https://github.com/Tau-J/rtmlib)

#### é€‰é¡¹ 2 (Option 2): MMPose (å®Œæ•´ç ”ç©¶å·¥å…·åŒ… Full Research Toolkit)

**ä½•æ—¶ä½¿ç”¨ When to use**:
- éœ€è¦è®¿é—®å¤šä¸ªæ¨¡å‹ (Need access to multiple models)
- è¿›è¡Œå¯¹æ¯”ç ”ç©¶ (Doing comparative research)
- è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹ (Training custom models)
- ä¸åŸºçº¿è¿›è¡ŒåŸºå‡†æµ‹è¯• (Benchmark against baselines)

**ç¼ºç‚¹ Disadvantages**:
- é‡å‹ä¾èµ– (Heavy dependencies) (mmcv, mmdet, mmengine)
- å¤æ‚å®‰è£… (Complex installation)
- æ›´å¤§å ç”¨ç©ºé—´ (Larger footprint)
- æ›´é™¡å³­çš„å­¦ä¹ æ›²çº¿ (Steeper learning curve)

### æ¨¡å‹å˜ä½“ (Model Variants)

| æ¨¡å‹ Model | AP (COCO) | å‚æ•°é‡ Params | CPU FPS | Mobile FPS | ä½¿ç”¨åœºæ™¯ Use Case |
|-------|-----------|--------|---------|------------|----------|
| RTMPose-t | 67.1% | 3.3M | 150+ | 120+ | è¶…å¿«é€Ÿç§»åŠ¨ç«¯ Ultra-fast mobile |
| RTMPose-s | 71.7% | 5.5M | 120+ | 90+ | å¹³è¡¡å‹ç§»åŠ¨ç«¯ Balanced mobile |
| **RTMPose-m** | **75.8%** | **13.6M** | **90+** | **70+** | **æ¨è Recommended** |
| RTMPose-l | 77.3% | 27.8M | 60+ | 40+ | é«˜ç²¾åº¦ High accuracy |
| RTMPose-x | 77.8% | 49.4M | 40+ | 20+ | æœ€é«˜ç²¾åº¦ Maximum accuracy |

**æ¨è Recommendation**: ä» **RTMPose-m** å¼€å§‹ - æœ€ä½³ç²¾åº¦/é€Ÿåº¦æƒè¡¡ (best accuracy/speed trade-off)ã€‚

### å…³é”®ç‚¹æ ¼å¼ (Keypoint Format)

**17 COCO keypoints**:
```
0: Nose
1-2: Eyes (left, right)
3-4: Ears (left, right)
5-6: Shoulders (left, right)
7-8: Elbows (left, right)
9-10: Wrists (left, right)
11-12: Hips (left, right)
13-14: Knees (left, right)
15-16: Ankles (left, right)
```

**å¯é€‰çš„ 133 å…³é”®ç‚¹å…¨èº«æ¨¡å‹ (Optional 133-keypoint whole-body model) (RTMW)**:
- 17 èº«ä½“ body + 6 è„šéƒ¨ feet + 10 é¢éƒ¨ face + 40 æ¯åªæ‰‹ hands per hand
- é€‚ç”¨äºéœ€è¦æ‰‹éƒ¨/é¢éƒ¨ç»†èŠ‚çš„åº”ç”¨ (For applications needing hand/face detail)

### æŠ€æœ¯åˆ›æ–° (Technical Innovation)

**SimCC (Simple Coordinate Classification)**:
- å°†å§¿æ€ä¼°è®¡é‡æ–°æ„æƒ³ä¸ºåˆ†ç±»è€Œéå›å½’ (Reconceptualizes pose estimation as classification vs. regression)
- æ¯”åŸºäºçƒ­å›¾çš„æ–¹æ³•æ›´å¿«æ¨ç† (Faster inference than heatmap-based methods)
- æ¯”ç›´æ¥å›å½’æ›´å‡†ç¡® (More accurate than direct regression)
- é«˜æ•ˆç”¨äºç§»åŠ¨ç«¯éƒ¨ç½² (Efficient for mobile deployment)

**è®ºæ–‡ Paper**: [arXiv:2303.07399](https://arxiv.org/abs/2303.07399)

### éƒ¨ç½²åç«¯ (Deployment Backends)

```python
# ONNX Runtime (cross-platform)
tracker = PoseTracker(backend='onnxruntime', device='cpu')

# CUDA (NVIDIA GPU)
tracker = PoseTracker(backend='onnxruntime', device='cuda')

# TensorRT (NVIDIA optimized)
tracker = PoseTracker(backend='tensorrt')

# OpenVINO (Intel optimized)
tracker = PoseTracker(backend='openvino')
```

### ä½•æ—¶é€‰æ‹© RTMPose (When to Choose RTMPose)

âœ… **é€‰æ‹© RTMPose å¦‚æœ (Choose RTMPose if)**:
- æ€§èƒ½è‡³å…³é‡è¦ (Performance is critical) (FPS, latency)
- ç²¾åº¦å¾ˆé‡è¦ (Accuracy matters) (competitive benchmark scores)
- åœ¨æœåŠ¡å™¨/è¾¹ç¼˜è®¾å¤‡ä¸Šç”Ÿäº§éƒ¨ç½² (Production deployment on servers/edge devices)
- æœ‰ GPU å¯ç”¨ (You have GPU available)
- éœ€è¦æœ€å°åŒ–è®¡ç®—æˆæœ¬ (Need to minimize compute costs)

âŒ **é€‰æ‹©å…¶ä»–æ–¹æ¡ˆå¦‚æœ (Choose something else if)**:
- éœ€è¦ 3D å§¿æ€ä¼°è®¡ (Need 3D pose estimation) (ä½¿ç”¨ use MediaPipe)
- éƒ¨ç½²åˆ° web æµè§ˆå™¨ (Deploying to web browsers) (ä½¿ç”¨ use MoveNet/PoseNet)
- éœ€è¦å¤§é‡æ‰‹éƒ¨/é¢éƒ¨åœ°æ ‡ (Need extensive hand/face landmarks) (ä½¿ç”¨ use MediaPipe)
- æ›´å–œæ¬¢æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿæ–‡æ¡£ (Prefer more mature ecosystem documentation)

---

## 2. MediaPipe Pose - ç”Ÿæ€ç³»ç»Ÿé¢†å¯¼è€… (Ecosystem Leader)

**æœ€é€‚åˆ (Best for)**: å¿«é€ŸåŸå‹å¼€å‘ï¼Œç§»åŠ¨ä¼˜å…ˆï¼Œ3D å§¿æ€ï¼Œç»¼åˆç”Ÿæ€ç³»ç»Ÿ (Rapid prototyping, mobile-first, 3D pose, comprehensive ecosystem)ã€‚

### æ¦‚è¿° (Overview)

- **å¼€å‘è€… Developer**: Google AI Edge
- **æœ€æ–°ç‰ˆæœ¬ Latest Version**: v0.10.19 (actively maintained 2025)
- **è®¸å¯è¯ License**: Apache 2.0
- **å¹³å°æ”¯æŒ Platform Support**: Android, iOS, Web, Python, Desktop

### æ ¸å¿ƒæŠ€æœ¯ (Core Technology)

**BlazePose Architecture**:
- **33 3D landmarks** (vs. RTMPose's 17 2D)
- çœŸå®ä¸–ç•Œ 3D åæ ‡ï¼ˆç±³ä¸ºå•ä½ï¼‰(Real-world 3D coordinates in meters)
- **å¯è§æ€§å’Œå­˜åœ¨æ€§åˆ†æ•° (Visibility and presence scores)** æ¯ä¸ªå…³é”®ç‚¹ (per keypoint)
- ä¼˜åŒ–ç”¨äºå•äººè¿½è¸ª (Optimized for single-person tracking)

### æ€§èƒ½ (Performance)

**ä¸‰ç§æ¨¡å‹å˜ä½“ (Three model variants)**:
```
Model      Accuracy    Size    Mobile FPS    Use Case
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Lite       Medium      4MB     40-50         Budget devices
Full       High        6MB     30-40         Standard
Heavy      Highest     30MB    15-20         Maximum accuracy
```

**å»¶è¿Ÿ Latency**:
- Mobile (Snapdragon 865): 30-40 FPS
- Desktop (i7 CPU): 40-60 FPS
- Desktop (GPU): 120+ FPS

### ç‹¬ç‰¹åŠŸèƒ½ (Unique Features)

**3D å§¿æ€ä¼°è®¡ (3D Pose Estimation)**:
```python
# MediaPipe provides 3D coordinates
for landmark in results.pose_world_landmarks.landmark:
    x, y, z = landmark.x, landmark.y, landmark.z  # Meters
    visibility = landmark.visibility  # 0.0 to 1.0
    presence = landmark.presence  # 0.0 to 1.0
```

**ç½®ä¿¡åº¦æŒ‡æ ‡ (Confidence Metrics)**:
- **Visibility**: å…³é”®ç‚¹æ˜¯å¯è§è¿˜æ˜¯è¢«é®æŒ¡ï¼Ÿ(Is keypoint visible or occluded?)
- **Presence**: å…³é”®ç‚¹æ˜¯å¦åœ¨ç”»é¢å†…ï¼Ÿ(Is keypoint within frame?)
- **Detection confidence**: æ•´ä½“å§¿æ€ç½®ä¿¡åº¦ (Overall pose confidence)

### ç§»åŠ¨ç«¯é›†æˆ (Mobile Integration)

**iOS**:
```swift
import MediaPipeTasksVision

let options = PoseLandmarkerOptions()
options.baseOptions.modelAssetPath = "pose_landmarker_full.task"
let poseLandmarker = try PoseLandmarker(options: options)
```

**Android**:
```kotlin
import com.google.mediapipe.tasks.vision.poselandmarker

val options = PoseLandmarker.PoseLandmarkerOptions.builder()
    .setBaseOptions(BaseOptions.builder().setModelAssetPath("pose_landmarker_full.task").build())
    .build()
val poseLandmarker = PoseLandmarker.createFromOptions(context, options)
```

**React Native**: é€šè¿‡åŸç”Ÿæ¨¡å—æˆ– QuickPose SDK åŒ…è£…å™¨ (Via native modules or QuickPose SDK wrapper)

### ä¼˜åŠ¿ (Strengths)

âœ… **æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿ (Mature ecosystem)**: å¹¿æ³›çš„æ–‡æ¡£ã€æ•™ç¨‹ã€ç¤¾åŒº (Extensive documentation, tutorials, community)
âœ… **3D è¾“å‡º (3D output)**: ç”¨äºç”Ÿç‰©åŠ›å­¦çš„çœŸå®ä¸–ç•Œåæ ‡ (Real-world coordinates for biomechanics)
âœ… **å¤šå¹³å° (Multi-platform)**: iOS/Android/Web çš„å•ä¸€ä»£ç åº“ (Single codebase for iOS/Android/Web)
âœ… **å¯è§æ€§åˆ†æ•° (Visibility scores)**: çŸ¥é“å…³é”®ç‚¹ä½•æ—¶è¢«é®æŒ¡ (Know when keypoints are occluded)
âœ… **Google æ”¯æŒ (Google backing)**: ä¿è¯é•¿æœŸæ”¯æŒ (Long-term support guaranteed)

### å±€é™æ€§ (Limitations)

âŒ **ä»…é™å•äºº (Single-person only)**: æ— æ³•è¿½è¸ªå¤šäºº (Can't track multiple people)
âŒ **æ¯” RTMPose æ…¢ (Slower than RTMPose)**: 30-40 FPS vs. 90+ FPS
âŒ **ç²¾åº¦è¾ƒä½ (Less accurate)**: ~72% AP vs. 75.8% AP

### ä½•æ—¶é€‰æ‹© MediaPipe (When to Choose MediaPipe)

âœ… **é€‰æ‹© MediaPipe å¦‚æœ (Choose MediaPipe if)**:
- éœ€è¦ 3D å§¿æ€ä¼°è®¡ (Need 3D pose estimation)
- éƒ¨ç½²åˆ°ç§»åŠ¨ç«¯ (Deploying to mobile) (iOS/Android)
- æƒ³è¦ç»¼åˆç”Ÿæ€ç³»ç»Ÿ (Want comprehensive ecosystem) (docs, examples)
- éœ€è¦å¯è§æ€§/å­˜åœ¨æ€§ç½®ä¿¡åº¦åˆ†æ•° (Need visibility/presence confidence scores)
- å¿«é€ŸåŸå‹å¼€å‘å’Œ MVP å¼€å‘ (Rapid prototyping and MVP development)
- è·¨å¹³å°ä¸€è‡´æ€§å¾ˆé‡è¦ (Cross-platform consistency matters)

---

## 3. MoveNet - TensorFlow è½»é‡çº§ (TensorFlow Lightweight)

**æœ€é€‚åˆ (Best for)**: Web éƒ¨ç½²ï¼Œç”µæ± å—é™çš„ç§»åŠ¨ç«¯ï¼ŒTensorFlow ç”Ÿæ€ç³»ç»Ÿ (Web deployment, battery-constrained mobile, TensorFlow ecosystem)ã€‚

### æ¦‚è¿° (Overview)

- **å¼€å‘è€… Developer**: Google TensorFlow
- **å˜ä½“ Variants**: Lightning (é€Ÿåº¦ speed) å’Œ Thunder (ç²¾åº¦ accuracy)
- **è®¸å¯è¯ License**: Apache 2.0
- **å¹³å° Platform**: TensorFlow Lite (mobile, web, edge)

### æ€§èƒ½ (Performance)

```
Model           AP (COCO)    Mobile FPS    Latency
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Lightning       63.0%        50+           <30ms
Thunder         72.0%        25+           <50ms
```

### éƒ¨ç½² (Deployment)

**TensorFlow.js (Web)**:
```javascript
import * as poseDetection from '@tensorflow-models/pose-detection';

const detector = await poseDetection.createDetector(
  poseDetection.SupportedModels.MoveNet,
  {modelType: poseDetection.movenet.modelType.LIGHTNING}
);

const poses = await detector.estimatePoses(video);
```

**TensorFlow Lite (Mobile)**:
```python
import tensorflow as tf

interpreter = tf.lite.Interpreter(model_path="movenet_lightning.tflite")
interpreter.allocate_tensors()
```

### ä½•æ—¶é€‰æ‹© MoveNet (When to Choose MoveNet)

âœ… **Web åº”ç”¨ (Web applications)**: æœ€ä½³ TF.js æ”¯æŒ (Best TF.js support)
âœ… **ç”µæ± æ•æ„Ÿ (Battery-sensitive)**: è¶…é«˜æ•ˆçš„ Lightning å˜ä½“ (Ultra-efficient Lightning variant)
âœ… **TensorFlow ç”Ÿæ€ç³»ç»Ÿ (TensorFlow ecosystem)**: ä¸ç°æœ‰ TF æµç¨‹é›†æˆ (Integrate with existing TF pipelines)

---

## 4. Apple Vision Framework (iOS åŸç”Ÿ iOS Native)

**æœ€é€‚åˆ (Best for)**: æƒ³è¦åŸç”Ÿé›†æˆçš„ iOS ç‹¬å åº”ç”¨ (iOS-exclusive apps wanting native integration)ã€‚

### æ¦‚è¿° (Overview)

- **å¼€å‘è€… Developer**: Apple
- **å¯ç”¨æ€§ Availability**: iOS 14+, iPadOS 14+, macOS 11+
- **è®¸å¯è¯ License**: Free (éœ€è¦ Apple Developer è´¦æˆ· requires Apple Developer account)
- **é›†æˆ Integration**: Native Swift/Objective-C

### åŠŸèƒ½ (Features)

**èº«ä½“å§¿æ€æ£€æµ‹ (Body Pose Detection)**:
- **19 landmarks**: å…¨èº«å…³é”®ç‚¹ (Full body keypoints)
- A12 Bionic+ èŠ¯ç‰‡ä¸Šçš„å®æ—¶å¤„ç† (Real-time on A12 Bionic+ chips)
- åˆ©ç”¨ç¥ç»å¼•æ“åŠ é€Ÿ (Leverages Neural Engine acceleration)

**ç²¾åº¦ Precision**:
```swift
import Vision

let request = VNDetectHumanBodyPoseRequest { request, error in
    guard let observations = request.results as? [VNHumanBodyPoseObservation] else { return }

    for observation in observations {
        let jointPoints = try? observation.recognizedPoints(.all)
        // Access individual joints
        if let rightElbow = jointPoints?[.rightElbow] {
            let confidence = rightElbow.confidence
            let location = rightElbow.location
        }
    }
}
```

### æ€§èƒ½ (Performance)

- **30-60 FPS** on iPhone 12+
- **<20ms latency** on iPhone 15 Pro
- é’ˆå¯¹ Apple Silicon ä¼˜åŒ– (Optimized for Apple Silicon)

### ä½•æ—¶é€‰æ‹© Apple Vision (When to Choose Apple Vision)

âœ… **ä»… iOS åº”ç”¨ (iOS-only app)**: ä¸éœ€è¦ Android (No Android needed)
âœ… **åŸç”Ÿæ€§èƒ½ (Native performance)**: åœ¨ Apple è®¾å¤‡ä¸Šæœ€å¿« (Fastest on Apple devices)
âœ… **ç³»ç»Ÿé›†æˆ (System integration)**: åˆ©ç”¨ iOS API (Leverage iOS APIs)
âœ… **éšç§ (Privacy)**: ä¿è¯è®¾å¤‡ç«¯å¤„ç† (On-device processing guaranteed)

---

## 5. OpenPose - ç ”ç©¶æ ‡å‡†ï¼ˆè¡°è½ä¸­ï¼‰(Research Standard - Declining)

**çŠ¶æ€ Status**: é—ç•™ç³»ç»Ÿï¼Œä¸æ¨èç”¨äºæ–°é¡¹ç›® (Legacy system, not recommended for new projects)ã€‚

### ä¸ºä»€ä¹ˆ OpenPose æ­£åœ¨è¡°è½ (Why OpenPose is Declining)

âŒ **å…¼å®¹æ€§é—®é¢˜ (Compatibility issues)**: ä¸æœ€æ–° CUDA/cuDNN ä¸å…¼å®¹ (Not compatible with latest CUDA/cuDNN)
âŒ **æœ€åæ›´æ–° (Last update)**: November 2020 (5+ å¹´å‰ years old)
âŒ **é‡å‹è¦æ±‚ (Heavy requirements)**: éœ€è¦å¼ºå¤§çš„ GPU (Needs powerful GPU)
âŒ **éƒ¨ç½²æŒ‘æˆ˜ (Deployment challenges)**: å¤æ‚è®¾ç½® (Complex setup)

### ä»ç„¶ç›¸å…³äº (Still Relevant For)

- **å¤šäººæ£€æµ‹ (Multi-person detection)**: å”¯ä¸€çš„ç»å…¸äººç¾¤è§£å†³æ–¹æ¡ˆ (Only classic solution for crowds)
- **å­¦æœ¯å¯¹æ¯” (Academic comparisons)**: å†å²åŸºçº¿ (Historical baseline)
- **ç ”ç©¶è®ºæ–‡ (Research papers)**: å‚è€ƒå®ç° (Reference implementation)

### OpenPose çš„æ›¿ä»£æ–¹æ¡ˆ (Alternatives to OpenPose)

- **å¤šäºº Multi-person**: ä½¿ç”¨ RTMPose + äººä½“æ£€æµ‹å™¨ (Use RTMPose + person detector)
- **ç²¾åº¦ Accuracy**: RTMPose-x è¶…è¶Š OpenPose (RTMPose-x surpasses OpenPose)
- **é€Ÿåº¦ Speed**: RTMPose å¿« 10 å€ (RTMPose 10x faster)

---

## è¯¦ç»†å¯¹æ¯”è¡¨ (Detailed Comparison Table)

### ç²¾åº¦å¯¹æ¯” (Accuracy Comparison) (COCO Dataset)

| æ¨¡å‹ Model | AP (%) | AR (%) | å¹´ä»½ Year | çŠ¶æ€ Status |
|-------|--------|--------|------|--------|
| **RTMPose-m** | **75.8** | **81.2** | 2023 | â­ æ¨è Recommended |
| RTMPose-l | 77.3 | 82.6 | 2023 | Production |
| MoveNet Thunder | 72.0 | 78.5 | 2021 | Production |
| **MediaPipe Pose** | **~72** | **~78** | 2020 | â­ æ¨è Recommended |
| MoveNet Lightning | 63.0 | 70.0 | 2021 | Production |
| OpenPose | ~70 | ~75 | 2017 | Legacy |
| PoseNet | ~60 | ~68 | 2018 | Legacy |

### é€Ÿåº¦å¯¹æ¯” (Speed Comparison) (FPS)

| æ¨¡å‹ Model | CPU (i7) | GPU (GTX 1660 Ti) | Mobile (SD865) |
|-------|----------|-------------------|----------------|
| **RTMPose-m** | **90+** | **430+** | **70+** |
| RTMPose-t | 150+ | 600+ | 120+ |
| MediaPipe Full | 40 | 120 | 35 |
| MoveNet Lightning | 60 | 200 | 50 |
| MoveNet Thunder | 30 | 100 | 25 |
| OpenPose | 15 | 60 | N/A |

### åŠŸèƒ½çŸ©é˜µ (Feature Matrix)

| åŠŸèƒ½ Feature | RTMPose | MediaPipe | MoveNet | Apple Vision | OpenPose |
|---------|---------|-----------|---------|--------------|----------|
| **å…³é”®ç‚¹ Keypoints** | 17 (133 whole-body) | 33 | 17 | 19 | 25 |
| **3D å§¿æ€ 3D Pose** | âŒ | âœ… | âŒ | âŒ | âœ… |
| **å¤šäºº Multi-Person** | âš ï¸ (éœ€è¦æ£€æµ‹å™¨ needs detector) | âŒ | âŒ | âŒ | âœ… |
| **ç§»åŠ¨ç«¯ä¼˜åŒ– Mobile Optimized** | âœ… | âœ… | âœ… | âœ… | âŒ |
| **Web æµè§ˆå™¨ Web Browser** | âš ï¸ (ONNX.js) | âœ… | âœ… | âŒ | âŒ |
| **ç½®ä¿¡åº¦åˆ†æ•° Confidence Scores** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **æ·±åº¦/Zè½´ Depth/Z-axis** | âŒ | âœ… (meters) | âŒ | âŒ | âœ… (relative) |
| **æ´»è·ƒå¼€å‘ Active Development** | âœ… 2025 | âœ… 2025 | âš ï¸ Stable | âœ… 2025 | âŒ 2020 |

---

## Movement Chain AI æ¨èç­–ç•¥ (Recommended Strategy for Movement Chain AI)

### é˜¶æ®µ 1 (Phase 1): MVP (å½“å‰ Current)

**ä¸»è¦é€‰æ‹© (Primary)**: **MediaPipe Pose**

**ç†ç”± Rationale**:
- æœ€å¿«ä¸Šå¸‚æ—¶é—´ (Fastest time to market)
- è·¨å¹³å° (Cross-platform) (iOS/Android)
- ç”¨äºç”Ÿç‰©åŠ›å­¦çš„ 3D å§¿æ€ (3D pose for biomechanics)
- å¤§é‡æ•™ç¨‹/ç¤ºä¾‹ (Extensive tutorials/examples)
- ç”Ÿäº§ç¯å¢ƒéªŒè¯ (Proven in production)

**å®ç° Implementation**:
```dart
// Flutter + MediaPipe
import 'package:google_ml_kit/google_ml_kit.dart';

final poseDetector = GoogleMlKit.vision.poseDetector();
final poses = await poseDetector.processImage(inputImage);
```

### é˜¶æ®µ 2 (Phase 2): ä¼˜åŒ– (Optimization) (ç¬¬ 3-6 ä¸ªæœˆ Month 3-6)

**å‡çº§åˆ° (Upgrade to)**: **RTMPose-m**

**ç†ç”± Rationale**:
- æ¨ç†é€Ÿåº¦å¿« 2-3 å€ (2-3x faster inference)
- æ›´é«˜ç²¾åº¦ (Higher accuracy) (75.8% vs ~72%)
- äº‘ç«¯è®¡ç®—æˆæœ¬æ›´ä½ (Lower compute costs in cloud)
- ç§»åŠ¨ç«¯ç”µæ± å¯¿å‘½æ›´é•¿ (Better battery life on mobile)

**è¿ç§»è·¯å¾„ (Migration Path)**:
```python
# Switch from MediaPipe to rtmlib
from rtmlib import PoseTracker

tracker = PoseTracker(model='rtmpose-m', backend='onnxruntime')
keypoints, scores = tracker(frame)

# Keypoint mapping: RTMPose (17) â†’ MediaPipe (33)
# Add custom interpolation for missing landmarks
```

**æŒ‘æˆ˜ Challenges**:
- RTMPose æœ‰ 17 ä¸ªå…³é”®ç‚¹ vs. MediaPipe çš„ 33 ä¸ª (RTMPose has 17 keypoints vs. MediaPipe's 33)
- éœ€è¦æ’å€¼ç¼ºå¤±çš„åœ°æ ‡ (Need to interpolate missing landmarks) (hands, feet, face)
- æˆ–æ¥å—é™ä½çš„ç²’åº¦ (Or accept reduced granularity)

### é˜¶æ®µ 3 (Phase 3): ç ”ç©¶ (Research) (ç¬¬ 6 ä¸ªæœˆä»¥ä¸Š Month 6+)

**ä½¿ç”¨ MMPose Toolbox ç”¨äº (Use MMPose Toolbox for)**:
- å¯¹æ¯”åŸºå‡†æµ‹è¯• (Comparative benchmarks)
- åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹ (Custom model training on our dataset)
- A/B æµ‹è¯•ä¸åŒæ¶æ„ (A/B testing different architectures)
- å­¦æœ¯å‡ºç‰ˆç‰© (Academic publications)

---

## é›†æˆä»£ç ç¤ºä¾‹ (Integration Code Examples)

### RTMPose (rtmlib)

```python
from rtmlib import PoseTracker
import cv2

# Initialize tracker
tracker = PoseTracker(
    model='rtmpose-m',
    backend='onnxruntime',
    device='cpu'
)

# Process video
cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Get keypoints
    keypoints, scores = tracker(frame)

    # Filter by confidence
    valid_kpts = keypoints[scores > 0.5]

    # Draw skeleton
    tracker.visualize(frame, keypoints, scores)
    cv2.imshow('Pose', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```

### MediaPipe (Python)

```python
import mediapipe as mp
import cv2

mp_pose = mp.solutions.pose
pose = mp_pose.Pose(
    static_image_mode=False,
    model_complexity=1,  # 0=Lite, 1=Full, 2=Heavy
    enable_segmentation=False,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    results = pose.process(frame_rgb)

    if results.pose_landmarks:
        # Access 3D landmarks
        for idx, landmark in enumerate(results.pose_world_landmarks.landmark):
            print(f"Landmark {idx}: ({landmark.x:.2f}, {landmark.y:.2f}, {landmark.z:.2f})")

    cv2.imshow('MediaPipe Pose', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```

### MediaPipe (Flutter)

```dart
import 'package:google_mlkit_pose_detection/google_mlkit_pose_detection.dart';

final options = PoseDetectorOptions(
  model: PoseDetectionModel.accurate,
  mode: PoseDetectionMode.stream,
);
final poseDetector = PoseDetector(options: options);

// Process camera frame
final inputImage = InputImage.fromBytes(
  bytes: cameraImage.planes[0].bytes,
  metadata: InputImageMetadata(...),
);

final poses = await poseDetector.processImage(inputImage);

for (final pose in poses) {
  for (final landmark in pose.landmarks.values) {
    print('${landmark.type}: (${landmark.x}, ${landmark.y}, ${landmark.z})');
    print('Likelihood: ${landmark.likelihood}');
  }
}
```

---

## æˆæœ¬æ•ˆç›Šåˆ†æ (Cost-Benefit Analysis)

### äº‘ç«¯æ¨ç†æˆæœ¬ (Cloud Inference Costs) (1M è¯·æ±‚/æœˆ requests/month)

| æ¨¡å‹ Model | è®¡ç®—æ—¶é—´ Compute Time | AWS Lambda æˆæœ¬ Cost | vs MediaPipe èŠ‚çœ Savings |
|-------|--------------|-----------------|----------------------|
| RTMPose-m (CPU) | ~11ms | $15 | **-67%** (baseline) |
| MediaPipe (CPU) | ~25ms | $45 | +200% |
| RTMPose-t (CPU) | ~7ms | $10 | **-78%** |
| RTMPose-m (GPU) | ~2ms | $8 (G4dn) | **-82%** |

**ç»“è®º Conclusion**: RTMPose å‡å°‘äº‘æˆæœ¬ 67-82% (reduces cloud costs by 67-82%)ã€‚

### ç§»åŠ¨ç«¯ç”µæ± å½±å“ (Mobile Battery Impact) (1 å°æ—¶è¿ç»­ä½¿ç”¨ hour continuous use)

| æ¨¡å‹ Model | ç”µæ± æ¶ˆè€— Battery Drain | æ‰‹æœºæ¸©åº¦ Phone Temperature |
|-------|---------------|-------------------|
| RTMPose-t | 8% | +2Â°C |
| RTMPose-m | 12% | +3Â°C |
| MediaPipe Lite | 15% | +3.5Â°C |
| MediaPipe Full | 18% | +4Â°C |

**ç»“è®º Conclusion**: RTMPose å»¶é•¿ç”µæ± å¯¿å‘½ 30-40% (extends battery life by 30-40%)ã€‚

---

## è¿ç§»æŒ‡å— (Migration Guide): MediaPipe â†’ RTMPose

### æ­¥éª¤ 1 (Step 1): å®‰è£… rtmlib (Install rtmlib)

```bash
pip install rtmlib onnxruntime
# or for GPU: pip install rtmlib onnxruntime-gpu
```

### æ­¥éª¤ 2 (Step 2): æ›´æ–°ä»£ç  (Update Code)

```python
# Before (MediaPipe)
import mediapipe as mp
mp_pose = mp.solutions.pose
pose = mp_pose.Pose()
results = pose.process(frame_rgb)
landmarks = results.pose_landmarks

# After (RTMPose)
from rtmlib import PoseTracker
tracker = PoseTracker(model='rtmpose-m')
keypoints, scores = tracker(frame)
```

### æ­¥éª¤ 3 (Step 3): å¤„ç†å…³é”®ç‚¹å·®å¼‚ (Handle Keypoint Differences)

**MediaPipe: 33 landmarks**
**RTMPose: 17 landmarks**

**æ˜ å°„ç­–ç•¥ (Mapping Strategy)**:
```python
def mediapipe_to_rtmpose(mp_landmarks):
    """Convert MediaPipe 33 â†’ RTMPose 17"""
    rtm_map = {
        0: 0,   # Nose â†’ Nose
        1: None, # Left eye inner (not in RTMPose)
        2: 1,   # Left eye â†’ Left eye
        # ... continue mapping
    }
    return rtm_keypoints

def rtmpose_to_mediapipe(rtm_keypoints):
    """Interpolate RTMPose 17 â†’ MediaPipe-like 33"""
    # Use body landmarks only, skip hand/face details
    # Or interpolate missing points
```

### æ­¥éª¤ 4 (Step 4): è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ (Adjust Confidence Thresholds)

```python
# MediaPipe uses visibility scores
if landmark.visibility > 0.5:
    use_keypoint()

# RTMPose uses detection scores
if score > 0.3:  # Lower threshold often works
    use_keypoint()
```

---

## æœªæ¥å±•æœ› (Future Outlook) (2025-2026)

### æ–°å…´è¶‹åŠ¿ (Emerging Trends)

**1. åŸºäº Transformer çš„æ¨¡å‹ (Transformer-Based Models)**
- ViTPose: Vision Transformer for pose (81.1 AP)
- æ›´æ…¢ä½†æ›´å‡†ç¡® (Slower but more accurate)
- é¢„è®¡ä¼šæœ‰ç§»åŠ¨ç«¯ä¼˜åŒ–ç‰ˆæœ¬ (Expect mobile-optimized versions)

**2. åŸºç¡€æ¨¡å‹ (Foundation Models)**
- SAM (Segment Anything) é›†æˆ (integration)
- å¤šä»»åŠ¡æ¨¡å‹ (Multi-task models) (pose + segmentation + depth)
- é›¶æ ·æœ¬å§¿æ€ä¼°è®¡ (Zero-shot pose estimation)

**3. å®æ—¶ 3D (Real-time 3D)**
- RGB-D åˆ° 3D ç½‘æ ¼é‡å»º (to 3D mesh reconstruction)
- åŸºäº NeRF çš„å§¿æ€ç»†åŒ– (NeRF-based pose refinement)
- LiDAR é›†æˆ (integration) (iPhone Pro)

### å…³æ³¨åˆ—è¡¨ (Watch List)

- **RTMPose v2**: ä¼ è¨€ 2025 å¹´å‘å¸ƒï¼Œæ”¯æŒ 3D (Rumored 2025 release with 3D support)
- **MediaPipe v0.11**: æ½œåœ¨æ€§èƒ½æ”¹è¿› (Potential performance improvements)
- **Apple Vision Pro**: ç”¨äºç©ºé—´å§¿æ€çš„æ–° API (New APIs for spatial pose)

---

## æ€»ç»“å»ºè®® (Summary Recommendations)

### é’ˆå¯¹ Movement Chain AI é¡¹ç›® (For Movement Chain AI Project)

**å½“å‰ (Current) (MVP)**: âœ… **MediaPipe Pose**
- åŸå›  Reason: æœ€å¿«å¼€å‘é€Ÿåº¦ï¼Œ3D æ”¯æŒï¼Œæˆç†Ÿç”Ÿæ€ç³»ç»Ÿ (Fastest development, 3D support, mature ecosystem)

**3-6 ä¸ªæœˆ (3-6 Months)**: â« **è¿ç§»åˆ° RTMPose-m (Migrate to RTMPose-m)**
- åŸå›  Reason: å¿« 2-3 å€ï¼Œæ›´é«˜ç²¾åº¦ï¼Œæ›´ä½æˆæœ¬ (2-3x faster, higher accuracy, lower costs)

**ç ”ç©¶ (Research)**: ğŸ”¬ **MMPose Toolbox**
- åŸå›  Reason: å¯¹æ¯”æ¨¡å‹ï¼Œè®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹ï¼Œå‘è¡¨è®ºæ–‡ (Compare models, train custom, publish papers)

### å¿«é€Ÿå†³ç­–çŸ©é˜µ (Quick Decision Matrix)

**é€‰æ‹© RTMPose å¦‚æœ (Choose RTMPose if)**:
- âœ… ç”Ÿäº§éƒ¨ç½² (Production deployment) (æœåŠ¡å™¨/äº‘ç«¯ server/cloud)
- âœ… æ€§èƒ½å¾ˆé‡è¦ (Performance matters) (FPS, latency, cost)
- âœ… GPU å¯ç”¨ (GPU available)
- âœ… 2D å§¿æ€è¶³å¤Ÿ (2D pose sufficient)

**é€‰æ‹© MediaPipe å¦‚æœ (Choose MediaPipe if)**:
- âœ… å¿«é€ŸåŸå‹å¼€å‘ (Rapid prototyping) (MVP)
- âœ… éœ€è¦ 3D åæ ‡ (Need 3D coordinates)
- âœ… ç§»åŠ¨ä¼˜å…ˆ (Mobile-first) (iOS/Android)
- âœ… æ›´å–œæ¬¢æˆç†Ÿç”Ÿæ€ç³»ç»Ÿ (Prefer mature ecosystem)

**é€‰æ‹© MoveNet å¦‚æœ (Choose MoveNet if)**:
- âœ… Web åº”ç”¨ (Web application) (TensorFlow.js)
- âœ… ç”µæ± å—é™è®¾å¤‡ (Battery-constrained device)
- âœ… ç°æœ‰ TF æµç¨‹ (Existing TF pipeline)

**é€‰æ‹© Apple Vision å¦‚æœ (Choose Apple Vision if)**:
- âœ… ä»… iOS åº”ç”¨ (iOS-exclusive app)
- âœ… æƒ³è¦åŸç”Ÿé›†æˆ (Want native integration)
- âœ… åˆ©ç”¨ç¥ç»å¼•æ“ (Leverage Neural Engine)

---

**æœ€åæ›´æ–° (Last Updated)**: December 2025
**ä¸‹æ¬¡å®¡æ ¸ (Next Review)**: Q2 2026 (æ£€æŸ¥ RTMPose v2, MediaPipe v0.11 check RTMPose v2, MediaPipe v0.11)
**ç»´æŠ¤è€… (Maintained By)**: Movement Chain AI ML Team

---

## å…¶ä»–èµ„æº (Additional Resources)

### å®˜æ–¹æ–‡æ¡£ (Official Documentation)
- **RTMPose**: [GitHub - OpenMMLab/MMPose](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose)
- **rtmlib**: [GitHub - Tau-J/rtmlib](https://github.com/Tau-J/rtmlib)
- **MediaPipe**: [Google AI Edge - MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker)
- **MoveNet**: [TensorFlow Hub](https://tfhub.dev/google/movenet/)
- **Apple Vision**: [Vision Framework - Apple Developer](https://developer.apple.com/documentation/vision)

### ç ”ç©¶è®ºæ–‡ (Research Papers)
- **RTMPose**: [arXiv:2303.07399](https://arxiv.org/abs/2303.07399) - RTMPose: Real-Time Multi-Person Pose Estimation
- **BlazePose**: [arXiv:2006.10204](https://arxiv.org/abs/2006.10204) - BlazePose: On-device Real-time Body Pose tracking
- **OpenPose**: [arXiv:1812.08008](https://arxiv.org/abs/1812.08008) - OpenPose: Realtime Multi-Person 2D Pose Estimation

### ç¤¾åŒºèµ„æº (Community Resources)
- **MMPose Discord**: åŠ å…¥è·å–æŠ€æœ¯æ”¯æŒ (Join for technical support)
- **MediaPipe Google Group**: æ´»è·ƒçš„ç¤¾åŒºè®¨è®º (Active community discussions)
- **Reddit r/computervision**: å§¿æ€ä¼°è®¡è®¨è®º (Pose estimation discussions)
