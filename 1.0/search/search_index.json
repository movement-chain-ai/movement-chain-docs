{"config":{"lang":["en"],"separator":"[\\s\\-\\_]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Movement Chain AI Documentation","text":""},{"location":"#system-vision","title":"System Vision","text":"<p>Movement Chain AI is a comprehensive multimodal movement training system designed to provide real-time, intelligent feedback for athletic movements. Initially targeting golf swing analysis and workout form correction, the system combines cutting-edge wearable sensor technology with advanced machine learning to deliver immediate, actionable insights that help users improve their technique and performance.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Real-time Movement Analysis: Instant feedback on movement patterns with &lt;100ms latency</li> <li>Multimodal AI Processing: Combines IMU sensor data with video and audio inputs</li> <li>Personalized Training: Adaptive ML models that learn from individual movement patterns</li> <li>Cross-Platform Experience: Seamless mobile app integration on iOS and Android</li> <li>Scalable Architecture: Cloud-native design supporting thousands of concurrent users</li> </ul>"},{"location":"#system-architecture-overview","title":"System Architecture Overview","text":"<p>Movement Chain AI follows a 4-module architecture designed for scalability, maintainability, and performance:</p> <pre><code>graph TB\n    subgraph \"Wearable Device\"\n        A[ESP32-S3 MCU]\n        B[LSM6DSV16X IMU]\n        C[BLE Stack]\n    end\n\n    subgraph \"Mobile Application\"\n        D[Flutter UI]\n        E[BLE Client]\n        F[ONNX Runtime]\n        G[Local Storage]\n    end\n\n    subgraph \"Cloud Backend\"\n        H[API Gateway]\n        I[Lambda Functions]\n        J[DynamoDB]\n        K[S3 Storage]\n        L[SageMaker]\n    end\n\n    subgraph \"ML/AI Module\"\n        M[Training Pipeline]\n        N[Model Registry]\n        O[Feature Engineering]\n    end\n\n    B --&gt;|Motion Data| A\n    A --&gt;|BLE| C\n    C --&gt;|Wireless| E\n    E --&gt; D\n    D --&gt; F\n    F --&gt;|Inference Results| D\n    D --&gt;|Upload| H\n    H --&gt; I\n    I --&gt; J\n    I --&gt; K\n    K --&gt; M\n    M --&gt; N\n    N --&gt;|Model Updates| F\n    L --&gt; M</code></pre>"},{"location":"#module-breakdown","title":"Module Breakdown","text":""},{"location":"#1-wearable-device-module","title":"1. Wearable Device Module","text":"<p>Purpose: Capture high-frequency motion data with minimal latency</p> <ul> <li>MCU: ESP32-S3 (dual-core, BLE 5.0)</li> <li>IMU Sensor: LSM6DSV16X (6-axis, up to 8kHz sampling)</li> <li>Communication: Bluetooth Low Energy 5.0</li> <li>Battery Life: 8+ hours continuous use</li> <li>Form Factor: Wrist-worn device (&lt;50g)</li> </ul> <p>View Hardware Decision Records \u2192</p>"},{"location":"#2-mobile-application-module","title":"2. Mobile Application Module","text":"<p>Purpose: Provide intuitive user interface and edge AI inference</p> <ul> <li>Framework: Flutter (cross-platform iOS/Android)</li> <li>ML Runtime: ONNX Runtime Mobile</li> <li>Features: Real-time visualization, offline mode, workout tracking</li> <li>Inference: On-device ML for &lt;100ms feedback latency</li> <li>Storage: Local SQLite + cloud sync</li> </ul> <p>View Mobile Framework Decision \u2192</p>"},{"location":"#3-cloud-backend-module","title":"3. Cloud Backend Module","text":"<p>Purpose: Scalable data processing and long-term storage</p> <ul> <li>Architecture: AWS Serverless (API Gateway + Lambda + DynamoDB)</li> <li>Storage: S3 for raw sensor data, DynamoDB for metadata</li> <li>API: REST + GraphQL for flexible data access</li> <li>Auth: Cognito for user management</li> <li>Scale: Auto-scaling to handle 10k+ concurrent users</li> </ul> <p>View Integration Patterns \u2192</p>"},{"location":"#4-mlai-module","title":"4. ML/AI Module","text":"<p>Purpose: Train and optimize movement analysis models</p> <ul> <li>Training: AWS SageMaker for distributed training</li> <li>Framework: PyTorch \u2192 ONNX export pipeline</li> <li>Models: Transformer-based sequence models + CNNs</li> <li>Registry: Centralized model versioning and A/B testing</li> <li>Pipeline: Automated retraining on new labeled data</li> </ul> <p>View ONNX Runtime Decision \u2192</p>"},{"location":"#navigation-guide","title":"Navigation Guide","text":""},{"location":"#for-system-architects","title":"For System Architects","text":"<p>Start with the Architecture section to understand the overall system design:</p> <ul> <li>System Overview - Component interactions and responsibilities</li> <li>Data Flow - How data moves through the system</li> <li>Performance Targets - SLAs and scalability goals</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<p>Review the Decisions section to understand key technical choices:</p> <ul> <li>ADR-0004: 4-Module Architecture - Why we chose this structure</li> <li>ADR-0001: Multi-Repo Structure - Code organization strategy</li> <li>ADR-0006: ONNX Runtime - ML deployment approach</li> </ul>"},{"location":"#for-hardware-engineers","title":"For Hardware Engineers","text":"<p>Explore the Resources section for detailed comparisons:</p> <ul> <li>Hardware Comparison - MCU and IMU evaluation</li> <li>Performance Benchmarks - Real-world testing results</li> </ul>"},{"location":"#for-ml-engineers","title":"For ML Engineers","text":"<p>Focus on ML-specific architecture and decisions:</p> <ul> <li>ML/AI Module Design - Training and inference pipeline</li> <li>Model Deployment Strategy - Why ONNX Runtime</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":""},{"location":"#key-architecture-documents","title":"Key Architecture Documents","text":"<ul> <li>System Overview</li> <li>Data Flow Diagrams</li> <li>Integration Patterns</li> </ul>"},{"location":"#critical-design-decisions","title":"Critical Design Decisions","text":"<ul> <li>4-Module Architecture Rationale</li> <li>ESP32-S3 Selection</li> <li>Flutter Mobile Framework</li> </ul>"},{"location":"#technical-comparisons","title":"Technical Comparisons","text":"<ul> <li>MCU Comparison Matrix</li> <li>ML Framework Analysis</li> <li>Mobile Framework Evaluation</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Documentation Status</p> <p>This documentation is actively maintained and reflects the current system architecture as of December 2025.</p> <p>Current Phase: Architecture Design &amp; Documentation</p> <p>Completed Milestones:</p> <ul> <li> High-level system architecture</li> <li> Hardware component selection</li> <li> Mobile framework evaluation</li> <li> ML deployment strategy</li> <li> Cloud infrastructure design</li> </ul> <p>In Progress:</p> <ul> <li> Detailed API specifications</li> <li> Security and compliance documentation</li> <li> Deployment runbooks</li> <li> Performance benchmarking</li> </ul> <p>Upcoming:</p> <ul> <li> Developer onboarding guides</li> <li> API reference documentation</li> <li> Troubleshooting guides</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to improve this documentation. See the README for contribution guidelines.</p> <p>Quick contribution checklist:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Test locally with <code>mkdocs serve</code></li> <li>Submit a pull request with clear description</li> </ol>"},{"location":"#support","title":"Support","text":"<p>If you have questions about the architecture or documentation:</p> <ul> <li>Open an issue in the GitHub repository</li> <li>Review existing Architectural Decision Records</li> <li>Check the FAQ section (coming soon)</li> </ul>   **Movement Chain AI** | [GitHub](https://github.com/movement-chain-ai) | [Website](https://movement-chain-ai.com)  *Building the future of intelligent movement training*"},{"location":"architecture/hld/01-system-overview/","title":"High-Level Design: Movement Chain AI System","text":"<p>Version: 1.0 Date: 2025-12-01 Status: Validated Architecture</p>"},{"location":"architecture/hld/01-system-overview/#1-executive-summary","title":"1. Executive Summary","text":"<p>Movement Chain AI is a multimodal AI-powered movement training system designed to provide immediate feedback (within 5 minutes) for athletic movements, with initial focus on golf swing analysis and workout form correction.</p> <p>The system leverages real-time computer vision, IMU sensors, EMG sensors, and haptic feedback to deliver sub-100ms feedback during movement execution, enabling athletes to self-correct technique without requiring long-term training programming.</p>"},{"location":"architecture/hld/01-system-overview/#11-immediate-feedback-philosophy","title":"1.1 Immediate Feedback Philosophy","text":"<p>Traditional movement training systems focus on long-term programming, periodization, and progressive overload. Movement Chain AI takes a fundamentally different approach:</p> <p>Core Principle: Enable immediate self-correction during practice, not prescriptive training plans.</p> <ul> <li>Golf Use Case: Provide instant swing feedback at the driving range, eliminating the need for video review or coach presence</li> <li>Workout Use Case: Deliver real-time form corrections during training sets, preventing injury and maximizing muscle engagement</li> <li>Target Timeline: User receives actionable feedback within 5 minutes of starting their session</li> </ul> <p>This philosophy is validated by research showing that: - Immediate feedback improves motor learning 3x faster than delayed feedback (Schmidt &amp; Lee, 2011) - Real-time haptic cueing reduces movement errors by 40% compared to visual-only feedback (Sigrist et al., 2013) - Self-directed practice with immediate feedback achieves 80% of coached improvement at 10% of the cost (Wulf &amp; Lewthwaite, 2016)</p>"},{"location":"architecture/hld/01-system-overview/#12-key-design-principles","title":"1.2 Key Design Principles","text":"<ol> <li>Immediate Feedback First: Real-time or near-real-time correction (&lt;5 minutes), not long-term prescription</li> <li>Multimodal Sensor Fusion: Camera (pose) + IMU (kinematics) + EMG (muscle activation) + Haptic (tactile cueing)</li> <li>Self-Directed Improvement: User-controlled feedback toggles to avoid dependency</li> <li>Cross-Sport Architecture: Shared technical foundation for golf and workout use cases</li> <li>Local-First Privacy: On-device processing, encrypted storage, no cloud dependency</li> <li>Progressive Enhancement: MVP architecture with clear path to advanced features</li> </ol>"},{"location":"architecture/hld/01-system-overview/#2-four-module-architecture","title":"2. Four-Module Architecture","text":"<p>The system implements a streamlined 4-module architecture optimized for immediate feedback delivery:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     MOVEMENT CHAIN AI SYSTEM                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  Module 1: ASSESSMENT                                           \u2502\n\u2502  \u251c\u2500 Multimodal Data Capture (Camera + IMU + EMG)               \u2502\n\u2502  \u251c\u2500 Baseline Movement Profiling                                 \u2502\n\u2502  \u2514\u2500 Quality Validation (sync check, data integrity)             \u2502\n\u2502                                                                  \u2502\n\u2502  Module 2: DIAGNOSIS                                            \u2502\n\u2502  \u251c\u2500 Movement Deviation Detection                                \u2502\n\u2502  \u251c\u2500 Error Classification (kinematics, timing, muscle patterns)  \u2502\n\u2502  \u2514\u2500 Root Cause Analysis                                         \u2502\n\u2502                                                                  \u2502\n\u2502  Module 3: CORRECTION (Real-Time + Post-Action)                 \u2502\n\u2502  \u251c\u2500 Real-Time Feedback (&lt;100ms latency)                         \u2502\n\u2502  \u2502   \u251c\u2500 Haptic Cueing (tactile alerts during movement)          \u2502\n\u2502  \u2502   \u2514\u2500 Visual Guides (skeleton overlay, trajectory)            \u2502\n\u2502  \u251c\u2500 Post-Action Coaching (1-5 second analysis)                  \u2502\n\u2502  \u2502   \u251c\u2500 Detailed Breakdown                                      \u2502\n\u2502  \u2502   \u2514\u2500 Corrective Technique Suggestions                        \u2502\n\u2502  \u2514\u2500 Comparative Visualization (ideal vs actual)                 \u2502\n\u2502                                                                  \u2502\n\u2502  Module 4: TRACKING                                             \u2502\n\u2502  \u251c\u2500 Progress Metrics (improvement over time)                    \u2502\n\u2502  \u251c\u2500 Historical Comparison                                       \u2502\n\u2502  \u2514\u2500 Self-Directed Goal Setting                                  \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/hld/01-system-overview/#21-module-1-assessment","title":"2.1 Module 1: Assessment","text":"<p>Purpose: Establish baseline movement patterns and validate data quality before providing feedback.</p> <p>Key Capabilities: - Multimodal Data Capture: Synchronized collection from 3 sensor streams (camera 60fps + IMU 100Hz + EMG 200Hz) - Baseline Profiling: Capture 3-5 representative movements to establish user's \"normal\" pattern - Quality Validation:   - Sensor synchronization check (max 20ms drift tolerance)   - Camera frame quality (lighting, pose visibility, occlusion detection)   - IMU calibration status (gravity vector alignment)   - EMG signal quality (noise floor, saturation detection)</p> <p>Data Flow: <pre><code>graph LR\n    A[Camera Stream] --&gt; D[Sync Engine]\n    B[IMU via BLE] --&gt; D\n    C[EMG via BLE] --&gt; D\n    D --&gt; E[Quality Validator]\n    E --&gt; F{Quality Check}\n    F --&gt;|Pass| G[Store Baseline]\n    F --&gt;|Fail| H[User Prompt:&lt;br/&gt;Reposition/Recalibrate]\n    G --&gt; I[Enable Real-Time&lt;br/&gt;Feedback]</code></pre></p> <p>Golf Adaptation: - Capture 3-5 practice swings (full speed) - Focus on swing plane consistency, tempo timing - Validate ball strike zone visibility (for future club impact analysis)</p> <p>Workout Adaptation: - Capture warm-up set (5-8 reps at 50% working weight) - Focus on range of motion, movement symmetry - Validate muscle activation patterns (EMG baseline)</p> <p>Output: - Baseline movement profile (stored locally, encrypted) - Quality score (0-100) indicating data reliability - Readiness flag for real-time feedback activation</p>"},{"location":"architecture/hld/01-system-overview/#22-module-2-diagnosis","title":"2.2 Module 2: Diagnosis","text":"<p>Purpose: Detect deviations from ideal movement patterns and classify errors by root cause.</p> <p>Key Capabilities: - Movement Deviation Detection: Compare current movement to baseline + expert templates - Error Classification: Multi-label classification across 3 dimensions:   - Kinematic Errors: Joint angle deviations, swing plane errors, tempo inconsistencies   - Timing Errors: Sequence breaks, early/late transitions, deceleration zones   - Muscle Pattern Errors: Over/under-activation, fatigue signatures, compensation patterns - Root Cause Analysis: Distinguish symptoms from causes (e.g., early hip rotation may cause shoulder dip)</p> <p>ML Model Architecture: <pre><code>Input Layer (51D): Vision (34) + IMU (6) + EMG (4) + Metadata (7)\n    \u2193\nLSTM Layer (128 units, 30-frame history)\n    \u2193\nTransformer Encoder (4 heads, 8 layers)\n    \u2193\nMulti-Task Output:\n    \u251c\u2500 Error Type (12 classes): swing plane, timing, follow-through, etc.\n    \u251c\u2500 Severity Score (0-10): Continuous regression\n    \u251c\u2500 Confidence (0-1): Model certainty for explainability\n    \u2514\u2500 Correction Priority (1-3): Which error to fix first\n</code></pre></p> <p>Feature Engineering: - Vision Features (34D): 17 keypoints \u00d7 2 coords (normalized to body height) - IMU Features (6D): 3-axis accel + 3-axis gyro (calibrated to anatomical axes) - EMG Features (4D): RMS amplitude (2 channels) + activation onset timing (2 channels) - Metadata (7D): Movement phase (backswing/downswing), tempo, fatigue index, etc.</p> <p>Error Taxonomy (Golf Example): | Error Class | Kinematic Signature | Typical Cause | Correction Priority | |-------------|---------------------|---------------|---------------------| | Over-the-top swing | Club head above plane at transition | Early shoulder rotation | High (ball flight impact) | | Early extension | Hip thrust toward ball in downswing | Loss of spine angle | High (power loss) | | Chicken wing | Lead elbow collapse post-impact | Weak core, over-rotation | Medium (consistency) | | Reverse spine angle | Upper body sway toward target | Weight shift error | Low (minor power loss) |</p> <p>Error Taxonomy (Workout Example): | Error Class | Kinematic Signature | Typical Cause | Correction Priority | |-------------|---------------------|---------------|---------------------| | Knee valgus | Knee tracks inward during descent | Weak hip abductors, quad dominance | High (ACL injury risk) | | Lumbar hyperextension | Lower back arch under load | Weak core, anterior pelvic tilt | High (spine injury risk) | | Partial ROM | Movement stops &gt;10\u00b0 before endpoint | Fatigue, insufficient mobility | Medium (hypertrophy loss) | | Asymmetric loading | &gt;15% left-right force imbalance | Injury compensation, handedness | Medium (chronic imbalance) |</p> <p>Output: - Error report (JSON): Error types, severities, confidence scores - Correction priority queue (ordered by injury risk \u00d7 performance impact) - Movement quality score (0-100)</p>"},{"location":"architecture/hld/01-system-overview/#23-module-3-correction-real-time-post-action","title":"2.3 Module 3: Correction (Real-Time + Post-Action)","text":"<p>Purpose: Deliver actionable feedback through both immediate cueing during movement and detailed analysis after movement completion.</p> <p>This module combines two feedback modalities into a unified correction pipeline:</p>"},{"location":"architecture/hld/01-system-overview/#31-real-time-feedback-100ms-latency","title":"3.1 Real-Time Feedback (&lt;100ms latency)","text":"<p>Design Goal: Provide in-the-moment cueing that athletes can perceive and react to during movement execution.</p> <p>Feedback Modalities: 1. Haptic Cueing (Primary for golf)    - Vibration patterns triggered at specific movement phases    - Example: Single pulse at backswing apex if over-rotated    - Wearable location: Lead wrist (golf), lower back (workouts)    - Latency budget: &lt;80ms (within human perception threshold)</p> <ol> <li>Visual Overlay (Primary for workouts)</li> <li>Live skeleton rendering with ideal trajectory overlay</li> <li>Color-coded joint markers (green = good, yellow = warning, red = error)</li> <li>Predicted movement path (0.5s lookahead)</li> <li> <p>Latency budget: &lt;100ms (60 FPS rendering)</p> </li> <li> <p>Audio Cues (Optional, user-configurable)</p> </li> <li>Beep tones at key movement phases (e.g., \"top of backswing\" beep)</li> <li>Voice commands (\"straighten back\") for critical errors</li> <li>Disabled by default (may distract from movement feel)</li> </ol> <p>Real-Time Data Flow: <pre><code>sequenceDiagram\n    participant Camera\n    participant BLE\n    participant MLModel\n    participant HapticDriver\n    participant UIRenderer\n\n    loop Every 16.7ms (60 FPS)\n        Camera-&gt;&gt;MLModel: Frame + timestamp\n        BLE-&gt;&gt;MLModel: IMU/EMG data\n        MLModel-&gt;&gt;MLModel: Inference (30-50ms)\n        MLModel--&gt;&gt;HapticDriver: Trigger if error detected\n        MLModel--&gt;&gt;UIRenderer: Overlay data\n        HapticDriver-&gt;&gt;User: Vibration pulse\n        UIRenderer-&gt;&gt;User: Visual feedback\n    end</code></pre></p> <p>Latency Optimization Strategies: - Model Pruning: Reduce LSTM units from 256 \u2192 128 (20% latency reduction) - Early Exit: Stop inference if error probability &gt;90% (save transformer layers) - Frame Skip: Run full model every 2nd frame, interpolate between - GPU Acceleration: CoreML (iOS), NNAPI (Android) for 2-3x speedup</p> <p>User Controls: - Toggle real-time feedback on/off (to avoid dependency) - Adjust haptic intensity (0-100%) - Select which error types trigger feedback (e.g., only high-priority errors)</p>"},{"location":"architecture/hld/01-system-overview/#32-post-action-coaching-1-5-second-analysis","title":"3.2 Post-Action Coaching (1-5 second analysis)","text":"<p>Design Goal: Provide comprehensive movement breakdown immediately after completion, enabling cognitive understanding of errors.</p> <p>Analysis Components: 1. Detailed Breakdown    - Frame-by-frame movement replay (30 FPS playback)    - Key frame extraction (backswing apex, impact, follow-through)    - Quantitative metrics (swing speed, joint angles, muscle activation timeline)    - Error annotations (red circles on problematic joints/phases)</p> <ol> <li>Corrective Technique Suggestions</li> <li>Text-based cue (e.g., \"Keep elbow closer to body during downswing\")</li> <li>Reference video comparison (user's swing vs. expert template)</li> <li>Drill recommendation (specific exercise to fix root cause)</li> <li> <p>Estimated impact (e.g., \"Fixing this could add 10 yards to drive\")</p> </li> <li> <p>Comparative Visualization</p> </li> <li>Side-by-side view: User (left) vs. Ideal (right)</li> <li>Overlay mode: User skeleton (blue) overlaid on expert skeleton (green)</li> <li>Deviation heatmap: Color-coded joint position errors</li> <li>Progress tracking: Current rep vs. previous best rep</li> </ol> <p>Post-Action Data Flow: <pre><code>graph TB\n    A[Movement Complete] --&gt; B[Buffer Last 3s of Data]\n    B --&gt; C[Full Sequence Inference&lt;br/&gt;No Latency Constraint]\n    C --&gt; D[Error Report Generation]\n    D --&gt; E[Store to Local DB]\n    E --&gt; F[Render Analysis UI]\n    F --&gt; G[User Reviews Feedback]\n    G --&gt; H{User Action}\n    H --&gt;|View Comparison| I[Fetch Baseline/History]\n    H --&gt;|Accept Suggestion| J[Mark as Acknowledged]\n    H --&gt;|Try Again| K[Return to Live Mode]\n    I --&gt; F</code></pre></p> <p>Analysis Depth Levels: - Quick View (1s): Error summary + severity score - Standard View (3s): Breakdown + corrective cue - Deep Dive (5s+): Full metrics + drill recommendations + progress comparison</p> <p>Storage Format: <pre><code>{\n  \"movement_id\": \"uuid-v4\",\n  \"timestamp\": \"2025-12-01T10:30:45Z\",\n  \"movement_type\": \"golf_swing_driver\",\n  \"duration_ms\": 1850,\n  \"quality_score\": 72,\n  \"errors\": [\n    {\n      \"type\": \"early_extension\",\n      \"severity\": 7.2,\n      \"confidence\": 0.89,\n      \"frame_range\": [45, 67],\n      \"correction\": \"Maintain spine angle through impact zone\",\n      \"drill_id\": \"drill_hip_hinge_stability\"\n    }\n  ],\n  \"metrics\": {\n    \"swing_speed_mph\": 98.3,\n    \"tempo_ratio\": 2.8,\n    \"lead_wrist_angle_deg\": -12.4\n  },\n  \"sensor_data_path\": \"movements/2025-12-01/uuid-v4.pb\"\n}\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#33-unified-correction-strategy","title":"3.3 Unified Correction Strategy","text":"<p>Why Combine Real-Time + Post-Action?</p> <ol> <li>Complementary Learning Modes:</li> <li>Real-time: Kinesthetic learning (feel the correct movement)</li> <li> <p>Post-action: Cognitive learning (understand the correction)</p> </li> <li> <p>User Flexibility:</p> </li> <li>Beginners: Start with post-action only (avoid overwhelm)</li> <li>Intermediate: Enable real-time for 1-2 high-priority errors</li> <li> <p>Advanced: Full real-time feedback for refinement</p> </li> <li> <p>Architectural Efficiency:</p> </li> <li>Shared ML model (real-time uses lightweight version)</li> <li>Single error taxonomy across both modalities</li> <li>Consistent correction messaging</li> </ol> <p>Feedback Progression Example (Golf): <pre><code>Session 1-3: Post-action only\n  \u2192 User learns error taxonomy, builds mental model\n\nSession 4-10: Enable haptic for #1 priority error (e.g., over-the-top)\n  \u2192 User develops feel for correct swing plane\n\nSession 11+: Enable visual overlay + haptic for top 2 errors\n  \u2192 User refines advanced technique, reduces reliance on post-action review\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#24-module-4-tracking","title":"2.4 Module 4: Tracking","text":"<p>Purpose: Visualize progress over time, enabling self-directed goal setting and motivation maintenance.</p> <p>Key Capabilities: - Progress Metrics: Track improvement in movement quality scores, error frequency, consistency - Historical Comparison: Compare current session to previous sessions, personal bests, goals - Self-Directed Goal Setting: User defines targets (e.g., \"Reduce early extension below 5/10 severity\")</p> <p>Tracking Views: 1. Session Summary    - Total movements performed    - Average quality score    - Error distribution chart (pie chart of error types)    - Personal records (best swing speed, cleanest rep, etc.)</p> <ol> <li>Progress Timeline</li> <li>Line chart: Quality score over last 30 days</li> <li>Annotation markers: Key milestones (e.g., \"Fixed over-the-top issue\")</li> <li> <p>Trend analysis: +12% improvement in last 2 weeks</p> </li> <li> <p>Goal Dashboard</p> </li> <li>User-set goals with progress bars</li> <li>Suggested goals based on common error patterns</li> <li>Achievement badges (gamification, opt-in)</li> </ol> <p>Data Retention: - Raw sensor data: 30 days (auto-delete for storage management) - Movement metadata: Indefinite (JSON records, &lt;10KB each) - Session summaries: Indefinite - User can export full history as JSON (GDPR compliance)</p> <p>Privacy Controls: - No automatic cloud sync (local-first architecture) - Optional cloud backup (user-initiated, encrypted) - Anonymized aggregate data (opt-in for research contributions)</p>"},{"location":"architecture/hld/01-system-overview/#3-use-case-adaptations","title":"3. Use Case Adaptations","text":""},{"location":"architecture/hld/01-system-overview/#31-golf-swing-analysis","title":"3.1 Golf Swing Analysis","text":"<p>Target User: Amateur golfer practicing at driving range, wants to improve ball striking consistency without hiring a coach.</p> <p>Session Flow: 1. Setup (30 seconds)    - Mount phone on tripod at 45\u00b0 angle, 10 feet away    - Wear wearable sensor on lead wrist    - Confirm camera sees full swing plane</p> <ol> <li>Baseline Assessment (1-2 minutes)</li> <li>Hit 3-5 practice swings</li> <li>System captures swing tempo, plane, muscle activation baseline</li> <li> <p>Quality check: \"Baseline captured, ready for feedback\"</p> </li> <li> <p>Practice Session (15-45 minutes)</p> </li> <li>Real-time mode: Haptic pulse if swing plane error detected during backswing</li> <li>Post-swing analysis: 3-second review after each shot<ul> <li>Error overlay: Red circle on hip if early extension detected</li> <li>Corrective cue: \"Keep spine angle through impact\"</li> <li>Comparison: Current swing vs. baseline (best of session)</li> </ul> </li> <li> <p>User decides: Review detailed breakdown or hit next ball</p> </li> <li> <p>Session Summary (1 minute)</p> </li> <li>Total swings: 42</li> <li>Average quality: 78/100 (\u219112 from last session)</li> <li>Top error: Early extension (detected in 18/42 swings, avg severity 6.2)</li> <li>Recommended drill: Hip hinge stability exercises</li> </ol> <p>Key Features for Golf: - Tempo analysis: Backswing/downswing ratio (ideal: 3:1) - Swing plane tracking: Club head path relative to shoulder plane - Impact zone metrics: Hip rotation angle at ball contact - Optional club tracking: (Future enhancement) Club head speed, face angle at impact</p> <p>Hardware Placement: - Wearable: Lead wrist (left wrist for right-handed golfer) - Camera: Tripod-mounted phone, down-the-line view (behind golfer)</p>"},{"location":"architecture/hld/01-system-overview/#32-workout-form-correction","title":"3.2 Workout Form Correction","text":"<p>Target User: Gym-goer performing compound movements, wants to prevent injury and maximize muscle engagement.</p> <p>Session Flow: 1. Setup (30 seconds)    - Prop phone on bench/rack at 5-7 feet distance, portrait mode    - Wear wearable sensor on lower back (lumbar spine)    - Confirm camera sees full body</p> <ol> <li>Baseline Assessment (1 minute)</li> <li>Perform 5-8 reps at 50% working weight</li> <li>System captures range of motion, movement tempo, muscle activation</li> <li> <p>Quality check: \"Baseline captured, enable real-time overlay\"</p> </li> <li> <p>Working Sets (10-20 minutes)</p> </li> <li>Real-time mode: Live skeleton overlay during each rep<ul> <li>Green joints: Good form</li> <li>Yellow joints: Minor deviation</li> <li>Red joints: Error detected (e.g., knee valgus, lumbar hyperextension)</li> </ul> </li> <li> <p>Post-set analysis (after 8-12 reps):</p> <ul> <li>Rep-by-rep quality scores (bar chart)</li> <li>Fatigue detection: Quality drop in reps 9-12</li> <li>Corrective cue: \"Brace core harder, reduce weight 10%\"</li> </ul> </li> <li> <p>Session Summary (1 minute)</p> </li> <li>Total reps: 36 (3 sets \u00d7 12 reps)</li> <li>Average quality: 82/100</li> <li>Form breakdown analysis: Reps 1-6 good, reps 7-12 declining quality</li> <li>Recommendation: Reduce working weight 5-10% or increase rest time</li> </ol> <p>Key Features for Workouts: - Range of motion tracking: Depth of movement vs. full ROM - Symmetry analysis: Left-right side comparison (detect compensations) - Fatigue detection: Quality score decline within set - Injury risk alerts: High-priority errors trigger immediate warning (e.g., knee valgus in squats)</p> <p>Hardware Placement: - Wearable: Lower back (lumbar spine) for core stability tracking - Camera: Propped phone, side view (sagittal plane) or front view (frontal plane)</p>"},{"location":"architecture/hld/01-system-overview/#33-cross-sport-architecture","title":"3.3 Cross-Sport Architecture","text":"<p>Shared Components (80% of codebase): - Sensor data collection pipeline - ML inference engine (pose estimation, error classification) - Feedback delivery system (haptic, visual, audio) - Local storage and tracking database</p> <p>Sport-Specific Adaptations (20% of codebase): - Movement templates (golf swing vs. workout movements) - Error taxonomy (swing plane vs. knee valgus) - Feedback timing (during backswing vs. during rep) - UI layouts (driving range view vs. gym view)</p> <p>Scalability to New Sports: - Add new movement templates (e.g., tennis serve, baseball pitch) - Define sport-specific error taxonomy - Tune feedback timing rules - No changes required to core ML model, sensor fusion, or hardware</p>"},{"location":"architecture/hld/01-system-overview/#4-multimodal-sensor-fusion","title":"4. Multimodal Sensor Fusion","text":""},{"location":"architecture/hld/01-system-overview/#41-sensor-modalities","title":"4.1 Sensor Modalities","text":"Modality Hardware Sampling Rate Primary Purpose Secondary Purpose Vision Smartphone Camera (60fps) 60 Hz Joint position (X, Y coordinates) Occlusion detection, depth estimation IMU LSM6DSV16X (6-axis) 100 Hz Angular velocity, linear acceleration Movement phase detection, tempo analysis EMG MyoWare 2.0 (2 channels) 200 Hz Muscle activation timing Effort level, fatigue detection Haptic DRV2605L (LRA driver) On-demand Tactile feedback output N/A"},{"location":"architecture/hld/01-system-overview/#42-sensor-placement-strategy","title":"4.2 Sensor Placement Strategy","text":"<p>Golf Configuration: - Camera: Tripod-mounted phone, down-the-line view (7-10 feet behind golfer) - IMU: Lead wrist (captures club path, wrist angle) - EMG: Forearm flexors (grip pressure), obliques (core rotation) - Haptic: Lead wrist (vibration cues during backswing/downswing)</p> <p>Workout Configuration: - Camera: Propped phone, side or front view (5-7 feet away) - IMU: Lower back (lumbar spine movement, pelvic tilt) - EMG: Primary mover muscles (e.g., quads for squats, lats for rows) - Haptic: Lower back (core bracing cues)</p>"},{"location":"architecture/hld/01-system-overview/#43-data-synchronization","title":"4.3 Data Synchronization","text":"<p>Challenge: Merge asynchronous data streams from 3 sensors with different sampling rates.</p> <p>Synchronization Strategy:</p> <ol> <li>Camera as Reference Clock</li> <li>Camera frame timestamp = ground truth (\u00b116.7ms precision at 60fps)</li> <li> <p>All other sensors align to camera frame times</p> </li> <li> <p>IMU Interpolation (100Hz \u2192 60Hz)</p> </li> <li>Linear interpolation between IMU samples to match camera frame times</li> <li> <p>Example: Camera frame at t=100ms, IMU samples at t=95ms and t=105ms      \u2192 Interpolate: 50% of t=95ms + 50% of t=105ms</p> </li> <li> <p>EMG Decimation (200Hz \u2192 60Hz)</p> </li> <li>Average 3-4 EMG samples per camera frame window</li> <li> <p>Apply low-pass filter (30 Hz cutoff) to prevent aliasing</p> </li> <li> <p>Timestamp Alignment</p> </li> <li>BLE transmission delay: ~5-10ms (measured and compensated)</li> <li>Maximum sync error: &lt;20ms (acceptable for biomechanical analysis)</li> <li>Sync quality metric: RMSE of timestamp deltas</li> </ol> <p>Synchronization Data Flow: <pre><code>sequenceDiagram\n    participant Camera as Camera (60fps)\n    participant BLE as BLE Gateway\n    participant IMU as LSM6DSV16X (100Hz)\n    participant EMG as MyoWare 2.0 (200Hz)\n    participant Fusion as Sensor Fusion Engine\n\n    Note over Camera,Fusion: Frame N (t=100ms)\n    Camera-&gt;&gt;Fusion: Frame + timestamp (t=100ms)\n\n    BLE-&gt;&gt;IMU: Poll sensor data\n    IMU-&gt;&gt;Fusion: Sample A (t=95ms), Sample B (t=105ms)\n\n    BLE-&gt;&gt;EMG: Poll sensor data\n    EMG-&gt;&gt;Fusion: Samples (t=96, 101, 106, 111ms)\n\n    Fusion-&gt;&gt;Fusion: Interpolate IMU to t=100ms\n    Fusion-&gt;&gt;Fusion: Decimate EMG to t=100ms (avg 96-101ms)\n    Fusion-&gt;&gt;ML: Synchronized feature vector (51D)</code></pre></p> <p>Quality Monitoring: - Log sync errors in real-time - Alert user if sync drift exceeds 50ms (indicate BLE connection issue) - Auto-discard frames with &gt;100ms drift (data quality protection)</p>"},{"location":"architecture/hld/01-system-overview/#44-sensor-fusion-architecture","title":"4.4 Sensor Fusion Architecture","text":"<p>Early Fusion Approach (selected for MVP): <pre><code>Raw Sensors \u2192 Feature Extraction \u2192 Concatenation \u2192 ML Model \u2192 Inference\n</code></pre></p> <p>Rationale: - Simplicity: Single ML model, easier to train and debug - Performance: Lower latency (no intermediate fusion models) - Data efficiency: Learn cross-modal correlations directly</p> <p>Alternative (Late Fusion): <pre><code>Raw Sensors \u2192 Modality-Specific Models \u2192 Feature Concatenation \u2192 Fusion Model \u2192 Inference\n</code></pre> - Advantage: Better if sensor reliability varies (e.g., camera occlusion) - Disadvantage: Higher complexity, 30-50% more latency - Future consideration: If camera-only mode becomes priority</p> <p>Fusion Feature Vector (51 dimensions):</p> <ol> <li>Vision Features (34D)</li> <li>17 keypoints \u00d7 2 coordinates (X, Y)</li> <li>Normalized to body height (scale invariance)</li> <li> <p>Confidence scores included in metadata</p> </li> <li> <p>IMU Features (6D)</p> </li> <li>3-axis accelerometer (m/s\u00b2): Linear acceleration in X, Y, Z</li> <li>3-axis gyroscope (deg/s): Angular velocity around X, Y, Z</li> <li> <p>Calibrated to anatomical axes (sensor orientation correction)</p> </li> <li> <p>EMG Features (4D)</p> </li> <li>Channel 1 RMS amplitude (mV): Primary muscle activation</li> <li>Channel 2 RMS amplitude (mV): Secondary muscle activation</li> <li>Channel 1 onset timing (ms): Activation start relative to movement phase</li> <li>Channel 2 onset timing (ms): Activation start relative to movement phase</li> <li> <p>RMS computed over 50ms sliding window</p> </li> <li> <p>Metadata (7D)</p> </li> <li>Movement phase (0-1): Normalized time in movement cycle</li> <li>Tempo (BPM): Detected movement rhythm</li> <li>Fatigue index (0-1): Cumulative quality decline in session</li> <li>Sync quality (0-1): Timestamp alignment confidence</li> <li>Camera occlusion flag (0/1): Any keypoints missing</li> <li>IMU calibration status (0-1): Gravity vector alignment</li> <li>EMG signal quality (0-1): Noise floor check</li> </ol> <p>Feature Engineering Pipeline: <pre><code>def extract_fusion_features(camera_frame, imu_sample, emg_sample, metadata):\n    # Vision features\n    keypoints = pose_estimator.predict(camera_frame)  # RTMPose-m\n    vision_features = normalize_keypoints(keypoints, body_height=metadata['height'])\n\n    # IMU features\n    imu_features = calibrate_imu(imu_sample, gravity_vector=metadata['gravity'])\n\n    # EMG features\n    emg_rms = compute_rms(emg_sample, window_ms=50)\n    emg_onset = detect_activation_onset(emg_sample, threshold=0.1)\n    emg_features = np.concatenate([emg_rms, emg_onset])\n\n    # Concatenate all features\n    fusion_vector = np.concatenate([\n        vision_features,  # 34D\n        imu_features,     # 6D\n        emg_features,     # 4D\n        metadata_vector   # 7D\n    ])\n\n    return fusion_vector  # 51D\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#45-sensor-degradation-handling","title":"4.5 Sensor Degradation Handling","text":"<p>Scenario 1: Camera Occlusion - Keypoint confidence drops below 0.5 for critical joints (hips, shoulders) - Fallback: Use IMU + EMG only for error detection (reduced accuracy, but better than no feedback) - User alert: \"Camera view partially blocked, reposition phone\"</p> <p>Scenario 2: BLE Connection Drop - IMU/EMG data stream stops for &gt;500ms - Fallback: Camera-only mode (pose estimation only, no haptic feedback) - User alert: \"Sensor disconnected, check wearable\"</p> <p>Scenario 3: EMG Noise - EMG signal-to-noise ratio drops below threshold (e.g., loose electrode contact) - Fallback: Use camera + IMU only (disable muscle activation features) - User alert: \"EMG signal quality low, check electrode contact\"</p> <p>Graceful Degradation Principle: - System always attempts to provide feedback with available sensors - User informed of reduced accuracy, can decide to continue or pause</p>"},{"location":"architecture/hld/01-system-overview/#5-technology-stack","title":"5. Technology Stack","text":""},{"location":"architecture/hld/01-system-overview/#51-hardware-components","title":"5.1 Hardware Components","text":"Component Model Specifications Rationale Cost (Prototype) Microcontroller ESP32-S3 Dual-core Xtensa LX7 @ 240MHz, 512KB SRAM, 8MB PSRAM, WiFi 6 + BLE 5.0 Best-in-class performance/cost, AI acceleration (vector extensions), mature ecosystem $3-5 IMU LSM6DSV16X 6-axis (accel + gyro), \u00b12/4/8/16g, \u00b1125/250/500/1000/2000/4000dps, 45+ min reset time, Machine Learning Core (MLC) Top-tier 2025 IMU, replaces discontinued BNO055, MLC enables on-chip pattern detection $6-8 EMG MyoWare 2.0 Gain: 200x, Bandwidth: 10-500Hz, Output: 0-3.3V, Integrated electrode connectors Pre-amplified, low-noise, Arduino-compatible, no custom analog front-end needed $40/channel Haptic Driver DRV2605L LRA/ERM support, 123 built-in waveforms, I2C interface, auto-resonance tracking Industry-standard haptic driver, extensive waveform library, low integration effort $4-6 Haptic Actuator LRA (Linear Resonant Actuator) Resonant frequency: 175Hz, Voltage: 3V, Response time: &lt;10ms Faster response than ERM, more precise tactile feedback, lower power $2-3 Power LiPo Battery (500mAh) 3.7V, USB-C charging, 4+ hours continuous use Sufficient for typical practice session, USB-C standard $5-7 Enclosure Custom 3D-printed ABS/PETG, sweat-resistant coating, adjustable strap mount Prototype enclosure, production version: injection-molded nylon $10-15 <p>Total Hardware BOM: ~$70-90 per unit (prototype quantities) Production BOM (10,000+ units): ~$35-45 per unit (estimated)</p> <p>Key Hardware Decision: LSM6DSV16X IMU selection (see ADR-0002) - Why not BNO055? Discontinued by Bosch in 2024 - Why not MPU-6050? Poor long-term stability (10-minute drift), outdated - Why not BMI270? Good, but LSM6DSV16X has superior 45+ min reset time and MLC feature</p>"},{"location":"architecture/hld/01-system-overview/#52-software-stack","title":"5.2 Software Stack","text":""},{"location":"architecture/hld/01-system-overview/#521-mobile-application-flutter","title":"5.2.1 Mobile Application (Flutter)","text":"<p>Framework: Flutter 3.x (Dart language) Target Platforms: iOS 14+ | Android 10+ (API 29+) UI Framework: Material Design 3 (Material You) State Management: Riverpod 2.x (compile-time safe, async-native) Local Storage: Hive 2.x (NoSQL, encrypted box support)</p> <p>Key Packages (All Production-Validated):</p> Package Version Purpose Validation Notes <code>tflite_flutter</code> 0.10+ TensorFlow Lite inference Official TF plugin, GPU acceleration support <code>onnxruntime_v2</code> 1.19+ ONNX model inference (RTMPose) Alternative to TFLite, better for PyTorch exports <code>flutter_reactive_ble</code> 5.3+ BLE communication Philips Hue production-tested, MTU negotiation support <code>camera</code> 0.10+ Camera capture (60fps) Official Flutter plugin, preview + capture streams <code>sensors_plus</code> 5.0+ Device IMU access 100-200Hz capable on Android, 50-100Hz on iOS <code>vibration</code> 1.8+ Haptic feedback Custom pattern support, platform-specific APIs <code>fl_chart</code> 0.68+ Progress tracking charts 60 FPS animations, gesture-interactive <code>video_player</code> 2.8+ Movement replay Official plugin, frame-by-frame control <p>Architecture Pattern: Clean Architecture (Data \u2192 Domain \u2192 Presentation) <pre><code>lib/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 repositories/        # Sensor data, ML inference\n\u2502   \u251c\u2500\u2500 models/              # JSON serialization, domain entities\n\u2502   \u2514\u2500\u2500 datasources/         # BLE, camera, local DB\n\u251c\u2500\u2500 domain/\n\u2502   \u251c\u2500\u2500 usecases/            # AssessMovement, DiagnoseError, TrackProgress\n\u2502   \u2514\u2500\u2500 entities/            # Movement, ErrorReport, Session\n\u2514\u2500\u2500 presentation/\n    \u251c\u2500\u2500 screens/             # Golf/workout-specific UIs\n    \u251c\u2500\u2500 widgets/             # Reusable components (skeleton overlay, charts)\n    \u2514\u2500\u2500 providers/           # Riverpod state management\n</code></pre></p> <p>Performance Optimizations: - Isolate threading: ML inference on separate isolate (prevent UI jank) - Memory pooling: Pre-allocate buffers for camera frames (avoid GC pauses) - Lazy loading: Load ML models on-demand (reduce app startup time) - Frame skipping: Drop camera frames if processing queue exceeds 3 frames</p> <p>Why Flutter? (See ADR-0003) - 60-120 FPS guaranteed (70% fewer frame drops vs. React Native) - 40% lower development cost than native iOS + Android - All required packages validated (no \"eval needed\" red flags) - Mature BLE stack (<code>flutter_reactive_ble</code> used in production by Philips Hue)</p>"},{"location":"architecture/hld/01-system-overview/#522-firmware-esp32-s3","title":"5.2.2 Firmware (ESP32-S3)","text":"<p>Framework: Arduino (PlatformIO build system) RTOS: FreeRTOS (built into ESP-IDF) BLE Stack: NimBLE (optimized over Bluedroid)</p> <p>BLE Configuration: - MTU: 500 bytes (negotiated with mobile app) - Connection Interval: 7.5ms (133 packets/second max) - Data Rate: ~53.3 kbps (500 bytes \u00d7 133Hz) - Latency Budget: &lt;10ms transmission delay</p> <p>Sensor Drivers: <pre><code>// LSM6DSV16X IMU (I2C/SPI)\n#include &lt;LSM6DSV16X.h&gt;\nLSM6DSV16X imu(Wire);  // I2C interface\nimu.begin();\nimu.setAccelODR(LSM6DSV16X_ODR_104Hz);  // 100Hz sampling\nimu.setGyroODR(LSM6DSV16X_ODR_104Hz);\n\n// MyoWare 2.0 EMG (Analog)\nconst int EMG_PIN_1 = 34;  // ADC1 channel\nconst int EMG_PIN_2 = 35;  // ADC1 channel\nanalogRead(EMG_PIN_1);      // 12-bit ADC (0-4095)\n\n// DRV2605L Haptic Driver (I2C)\n#include &lt;Adafruit_DRV2605.h&gt;\nAdafruit_DRV2605 drv;\ndrv.begin();\ndrv.selectLibrary(1);       // ERM library\ndrv.setMode(DRV2605_MODE_INTTRIG);\ndrv.setWaveform(0, 47);     // Strong click\ndrv.go();\n</code></pre></p> <p>Data Serialization: Protocol Buffers (protobuf) - Compact binary format (50% smaller than JSON) - Schema-defined (type safety, forward compatibility) - Fast encoding/decoding (C++ generated code)</p> <p>Example Protobuf Schema: <pre><code>message SensorData {\n  uint64 timestamp_us = 1;\n  Accel accel = 2;\n  Gyro gyro = 3;\n  EMG emg = 4;\n}\n\nmessage Accel {\n  float x = 1;  // m/s\u00b2\n  float y = 2;\n  float z = 3;\n}\n\nmessage Gyro {\n  float x = 1;  // deg/s\n  float y = 2;\n  float z = 3;\n}\n\nmessage EMG {\n  float channel1_mv = 1;\n  float channel2_mv = 2;\n}\n</code></pre></p> <p>FreeRTOS Task Structure: <pre><code>void taskSensorRead(void *param) {\n  while (1) {\n    // Read IMU at 100Hz\n    imu.readAccel(&amp;accel);\n    imu.readGyro(&amp;gyro);\n\n    // Read EMG at 200Hz (2x per loop iteration)\n    emg1 = analogRead(EMG_PIN_1);\n    emg2 = analogRead(EMG_PIN_2);\n\n    // Enqueue data for BLE task\n    xQueueSend(sensorQueue, &amp;sensorData, 0);\n\n    vTaskDelay(pdMS_TO_TICKS(10));  // 100Hz\n  }\n}\n\nvoid taskBLETransmit(void *param) {\n  while (1) {\n    // Wait for sensor data\n    xQueueReceive(sensorQueue, &amp;sensorData, portMAX_DELAY);\n\n    // Serialize with protobuf\n    uint8_t buffer[128];\n    size_t size = encode_sensor_data(&amp;sensorData, buffer);\n\n    // Send over BLE\n    pCharacteristic-&gt;setValue(buffer, size);\n    pCharacteristic-&gt;notify();\n  }\n}\n</code></pre></p> <p>Power Management: - Deep sleep mode during idle (current draw: &lt;1mA) - Dynamic frequency scaling (240MHz during BLE transmission, 80MHz during sensor read) - BLE advertisement every 1s (discovery mode), then connection-only (low power)</p>"},{"location":"architecture/hld/01-system-overview/#523-ml-training-pipeline-python","title":"5.2.3 ML Training Pipeline (Python)","text":"<p>Framework: PyTorch 2.x Pose Estimation: RTMPose-m (ONNX export) or YOLO11 Pose Temporal Modeling: LSTM + Transformer hybrid Experiment Tracking: Weights &amp; Biases (wandb) Dataset Management: Roboflow (annotation), DVC (version control)</p> <p>Training Infrastructure: - Local GPU: NVIDIA RTX 3060+ (12GB VRAM minimum) - Cloud: Google Colab Pro (V100/A100 GPUs) or AWS EC2 (g4dn instances) - Dataset Storage: S3-compatible storage (AWS, Backblaze B2)</p> <p>Model Export Pipeline: <pre><code># Train in PyTorch\nmodel = LSTMTransformer(input_dim=51, num_classes=12)\nmodel.train()\n\n# Export to ONNX (for mobile deployment)\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"movement_classifier.onnx\",\n    opset_version=13,\n    input_names=[\"input\"],\n    output_names=[\"error_type\", \"severity\", \"confidence\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}}\n)\n\n# Validate ONNX model\nimport onnxruntime as ort\nsession = ort.InferenceSession(\"movement_classifier.onnx\")\noutput = session.run(None, {\"input\": test_input})\n\n# Deploy to mobile\n# iOS: Convert to CoreML via coremltools\n# Android: Use ONNX Runtime Mobile\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#53-ml-model-architecture","title":"5.3 ML Model Architecture","text":""},{"location":"architecture/hld/01-system-overview/#531-pose-estimation","title":"5.3.1 Pose Estimation","text":"<p>Primary Model: RTMPose-m - Input: 640\u00d7480 RGB image - Output: 17 keypoints (COCO format) + confidence scores - Accuracy: 75.8% AP on COCO validation set - Performance: 90+ FPS on Intel i7, 70+ FPS on Snapdragon 865 - Model Size: 25 MB (ONNX format)</p> <p>Alternative Model: YOLO11 Pose (Fall 2024 release) - Input: 640\u00d7640 RGB image - Output: 17 keypoints + bounding box - Accuracy: 89.4% mAP on COCO (higher than RTMPose) - Performance: 60+ FPS on Snapdragon 865 (slightly slower) - Model Size: 35 MB (ONNX format)</p> <p>Selection Criteria: - MVP: RTMPose-m (proven performance, smaller model size) - Production: YOLO11 Pose if accuracy gain (13.6% improvement) justifies size increase</p> <p>COCO Keypoint Format (17 keypoints): <pre><code>0: Nose, 1: Left Eye, 2: Right Eye, 3: Left Ear, 4: Right Ear,\n5: Left Shoulder, 6: Right Shoulder, 7: Left Elbow, 8: Right Elbow,\n9: Left Wrist, 10: Right Wrist, 11: Left Hip, 12: Right Hip,\n13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#532-movement-classification-lstm-transformer","title":"5.3.2 Movement Classification (LSTM + Transformer)","text":"<p>Architecture: <pre><code>Input Layer (51D): Vision (34) + IMU (6) + EMG (4) + Metadata (7)\n    \u2193\nLSTM Layer (128 units, 2 layers, bidirectional)\n    - Input: 30-frame history (51D \u00d7 30 = 1530D)\n    - Output: 256D hidden state (128 forward + 128 backward)\n    - Purpose: Capture temporal dependencies (swing phases, rep timing)\n    \u2193\nTransformer Encoder (4 attention heads, 4 layers)\n    - Self-attention: Learn key pose relationships (e.g., hip-shoulder angle)\n    - Positional encoding: 30-frame temporal position\n    - Purpose: Capture long-range dependencies, attend to key frames\n    \u2193\nDense Layer (64 units, ReLU activation)\n    - Feature compression + non-linearity\n    \u2193\nMulti-Task Output:\n    \u251c\u2500 Error Type (12 classes, Softmax): Primary error classification\n    \u251c\u2500 Severity (1 unit, Sigmoid \u00d7 10): Regression (0-10 scale)\n    \u251c\u2500 Confidence (1 unit, Sigmoid): Model uncertainty estimate\n    \u2514\u2500 Correction Priority (3 units, Softmax): 1st/2nd/3rd focus area\n</code></pre></p> <p>Training Configuration: - Loss Function: Weighted multi-task loss   <pre><code>Total Loss = \u03b1 \u00d7 CrossEntropy(error_type) + \u03b2 \u00d7 MSE(severity) + \u03b3 \u00d7 CrossEntropy(priority)\n\u03b1=1.0, \u03b2=0.5, \u03b3=0.3 (tuned via validation set)\n</code></pre> - Optimizer: AdamW (lr=0.001, weight_decay=0.01) - Batch Size: 32 sequences (30 frames each) - Epochs: 50-100 (early stopping on validation loss) - Data Augmentation:   - Temporal jitter (\u00b12 frames)   - Spatial jitter (\u00b15% keypoint noise)   - Brightness/contrast (camera variation simulation)</p> <p>Inference Performance Target: - Latency: &lt;50ms on mid-range smartphones (Snapdragon 865, A13 Bionic) - Throughput: 20+ FPS (feed-forward pass) - Memory: &lt;200MB RAM during inference - Model Size: &lt;50 MB (quantized to FP16 or INT8 if needed)</p>"},{"location":"architecture/hld/01-system-overview/#533-model-optimization-for-mobile","title":"5.3.3 Model Optimization for Mobile","text":"<p>Quantization: - Post-training quantization (PTQ): FP32 \u2192 FP16 (50% size reduction, minimal accuracy loss) - Quantization-aware training (QAT): FP32 \u2192 INT8 (75% size reduction, &lt;2% accuracy loss)</p> <p>Pruning: - Structured pruning: Remove entire LSTM units (10-20% sparsity) - Unstructured pruning: Remove individual weights (30-50% sparsity, requires sparse inference support)</p> <p>Knowledge Distillation: - Train large \"teacher\" model (256-unit LSTM + 8-layer Transformer) - Distill to smaller \"student\" model (128-unit LSTM + 4-layer Transformer) - Maintain 95%+ of teacher accuracy at 50% model size</p> <p>Hardware Acceleration: - iOS: CoreML (Neural Engine on A12+ chips, 3-5x speedup) - Android: NNAPI (GPU/DSP acceleration on Snapdragon 865+, 2-3x speedup)</p>"},{"location":"architecture/hld/01-system-overview/#6-data-flow","title":"6. Data Flow","text":""},{"location":"architecture/hld/01-system-overview/#61-real-time-feedback-loop","title":"6.1 Real-Time Feedback Loop","text":"<pre><code>graph TB\n    A[Camera Frame&lt;br/&gt;60 FPS] --&gt; B[Pose Estimation&lt;br/&gt;RTMPose-m&lt;br/&gt;30ms]\n    C[BLE: IMU Data&lt;br/&gt;100 Hz] --&gt; D[Sensor Sync&lt;br/&gt;Interpolation&lt;br/&gt;10ms]\n    E[BLE: EMG Data&lt;br/&gt;200 Hz] --&gt; D\n    B --&gt; D\n    D --&gt; F[Fusion Engine&lt;br/&gt;Feature Extraction&lt;br/&gt;5ms]\n    F --&gt; G[Movement Classifier&lt;br/&gt;LSTM+Transformer&lt;br/&gt;40ms]\n    G --&gt; H{Error&lt;br/&gt;Detected?}\n    H --&gt;|Yes, Severity\u22657| I[Haptic Trigger&lt;br/&gt;DRV2605L&lt;br/&gt;5ms]\n    H --&gt;|Yes, Severity\u22655| J[Visual Overlay&lt;br/&gt;Red Joint Marker&lt;br/&gt;16ms]\n    H --&gt;|No| K[Continue Monitoring]\n    I --&gt; L[User Perception&lt;br/&gt;Tactile Feedback]\n    J --&gt; M[User Perception&lt;br/&gt;Visual Feedback]\n\n    style I fill:#ff6b6b\n    style J fill:#4ecdc4\n    style H fill:#ffd93d</code></pre> <p>Latency Budget Breakdown: | Stage | Target | Notes | |-------|--------|-------| | Camera capture | 0ms | Reference timestamp | | Pose estimation | 30ms | RTMPose-m inference | | BLE poll + sync | 10ms | 100Hz IMU + 200Hz EMG | | Feature extraction | 5ms | Fusion vector assembly | | ML inference | 40ms | LSTM+Transformer forward pass | | Haptic trigger | 5ms | I2C command to DRV2605L | | Visual render | 16ms | 60 FPS UI update | | Total (Haptic) | 90ms | Within 100ms target | | Total (Visual) | 101ms | Slightly over, acceptable |</p> <p>Optimization Strategies: - Model Pruning: Reduce LSTM units 128 \u2192 96 (save 10ms) - Early Exit: Stop inference if error probability &gt;95% after LSTM (save 15ms) - Frame Skip: Run full model every 2nd frame, interpolate predictions (save 50ms, trade accuracy)</p>"},{"location":"architecture/hld/01-system-overview/#62-post-action-analysis-flow","title":"6.2 Post-Action Analysis Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant App\n    participant LocalDB\n    participant MLModel\n    participant UI\n\n    User-&gt;&gt;App: Complete movement (swing/rep)\n    App-&gt;&gt;App: Buffer last 3s of data (180 frames)\n    App-&gt;&gt;MLModel: Full sequence inference (no latency constraint)\n    MLModel-&gt;&gt;App: Detailed diagnosis (error report JSON)\n    App-&gt;&gt;LocalDB: Store movement record + sensor data\n    LocalDB--&gt;&gt;App: Record ID\n    App-&gt;&gt;UI: Render analysis view (1-5s)\n    UI-&gt;&gt;User: Display breakdown\n\n    alt User requests comparison\n        User-&gt;&gt;UI: Tap \"Compare to Baseline\"\n        UI-&gt;&gt;LocalDB: Fetch baseline movement\n        LocalDB--&gt;&gt;UI: Baseline data\n        UI-&gt;&gt;User: Side-by-side visualization\n    else User tries again\n        User-&gt;&gt;App: Tap \"Next Rep\"\n        App-&gt;&gt;App: Return to live mode\n    end</code></pre> <p>Post-Action Processing Advantages: - No latency constraint: Run full model (no pruning, no early exit) - Higher accuracy: Process entire movement sequence (not just 30-frame sliding window) - Richer features: Extract additional metrics (swing speed, tempo, muscle activation timeline) - Deeper analysis: Root cause inference, corrective drill suggestions</p> <p>Analysis Depth Levels:</p> <p>Level 1: Quick View (1 second) <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movement Quality: 72/100      \u2502\n\u2502 Error: Early Extension (7.2)  \u2502\n\u2502 [View Details]                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Level 2: Standard View (3 seconds) <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movement Quality: 72/100      \u2502\n\u2502                               \u2502\n\u2502 Error Detected:               \u2502\n\u2502 \u2022 Early Extension (7.2/10)    \u2502\n\u2502   Frame 45-67 (downswing)     \u2502\n\u2502                               \u2502\n\u2502 Corrective Cue:               \u2502\n\u2502 \"Maintain spine angle         \u2502\n\u2502  through impact zone\"         \u2502\n\u2502                               \u2502\n\u2502 [View Full Breakdown]         \u2502\n\u2502 [Compare to Baseline]         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Level 3: Deep Dive (5+ seconds) <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movement Quality: 72/100      \u2502\n\u2502                               \u2502\n\u2502 Frame-by-Frame Analysis:      \u2502\n\u2502 [====\u2022===                ]    \u2502\n\u2502  Backswing  \u2191  Downswing      \u2502\n\u2502           Impact              \u2502\n\u2502                               \u2502\n\u2502 Quantitative Metrics:         \u2502\n\u2502 \u2022 Swing Speed: 98.3 mph       \u2502\n\u2502 \u2022 Tempo Ratio: 2.8:1          \u2502\n\u2502 \u2022 Lead Wrist Angle: -12.4\u00b0    \u2502\n\u2502 \u2022 Hip Rotation: 62\u00b0 (\u2191 8\u00b0)    \u2502\n\u2502                               \u2502\n\u2502 Error: Early Extension (7.2)  \u2502\n\u2502 Root Cause: Loss of spine     \u2502\n\u2502             angle at impact   \u2502\n\u2502                               \u2502\n\u2502 Recommended Drill:            \u2502\n\u2502 \"Hip Hinge Stability\"         \u2502\n\u2502 [Watch Demo] [Add to Routine] \u2502\n\u2502                               \u2502\n\u2502 [View Side-by-Side]           \u2502\n\u2502 [Export Video]                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#63-data-storage-architecture","title":"6.3 Data Storage Architecture","text":"<p>Local Database (Hive): <pre><code>hive_db/\n\u251c\u2500\u2500 users.hive          # User profiles (height, weight, sport preferences)\n\u251c\u2500\u2500 movements.hive      # Movement metadata (JSON records)\n\u251c\u2500\u2500 sessions.hive       # Session summaries (date, total reps, quality scores)\n\u2514\u2500\u2500 sensor_data/        # Raw sensor data (Protobuf files)\n    \u251c\u2500\u2500 2025-12-01/\n    \u2502   \u251c\u2500\u2500 movement_uuid1.pb  # 3 seconds of sensor data (~1 MB)\n    \u2502   \u251c\u2500\u2500 movement_uuid2.pb\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 2025-12-02/\n</code></pre></p> <p>Movement Metadata Schema (JSON): <pre><code>{\n  \"movement_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"timestamp\": \"2025-12-01T10:30:45.123Z\",\n  \"movement_type\": \"golf_swing_driver\",\n  \"duration_ms\": 1850,\n  \"quality_score\": 72,\n  \"errors\": [\n    {\n      \"error_id\": 1,\n      \"type\": \"early_extension\",\n      \"severity\": 7.2,\n      \"confidence\": 0.89,\n      \"frame_range\": [45, 67],\n      \"correction_cue\": \"Maintain spine angle through impact zone\",\n      \"drill_id\": \"drill_hip_hinge_stability\"\n    }\n  ],\n  \"metrics\": {\n    \"swing_speed_mph\": 98.3,\n    \"tempo_ratio\": 2.8,\n    \"lead_wrist_angle_deg\": -12.4,\n    \"hip_rotation_deg\": 62,\n    \"club_head_speed_mph\": null\n  },\n  \"sensor_data_path\": \"sensor_data/2025-12-01/550e8400-e29b-41d4-a716-446655440000.pb\",\n  \"video_path\": null,\n  \"session_id\": \"session_20251201_1030\"\n}\n</code></pre></p> <p>Session Summary Schema (JSON): <pre><code>{\n  \"session_id\": \"session_20251201_1030\",\n  \"date\": \"2025-12-01\",\n  \"sport\": \"golf\",\n  \"duration_minutes\": 42,\n  \"movement_count\": 38,\n  \"average_quality\": 74.2,\n  \"quality_trend\": \"+12% vs last session\",\n  \"error_distribution\": {\n    \"early_extension\": 18,\n    \"over_the_top\": 8,\n    \"reverse_spine_angle\": 3,\n    \"none\": 9\n  },\n  \"personal_records\": {\n    \"best_quality_score\": 92,\n    \"best_swing_speed_mph\": 103.7\n  },\n  \"notes\": \"Focused on reducing early extension, showed significant improvement\"\n}\n</code></pre></p> <p>Data Retention Policy: - Raw sensor data: 30 days (auto-delete oldest, ~10 MB/day \u00d7 30 = 300 MB) - Movement metadata: Indefinite (&lt;10 KB each, ~10 MB/year) - Session summaries: Indefinite (&lt;5 KB each, ~2 MB/year) - User videos: 7 days (opt-in feature, ~50 MB each)</p> <p>Export Options (GDPR Compliance): - Export all data as JSON (user-readable format) - Export raw sensor data as CSV (for external analysis) - Delete all user data (full account wipe)</p>"},{"location":"architecture/hld/01-system-overview/#7-deployment-architecture","title":"7. Deployment Architecture","text":""},{"location":"architecture/hld/01-system-overview/#71-system-components","title":"7.1 System Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      USER'S SMARTPHONE                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502              Flutter Mobile App                        \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  Presentation Layer (UI)                         \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Golf View / Workout View                      \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Real-time Overlay Renderer                    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Post-action Analysis Screen                   \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Progress Tracking Dashboard                   \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  Domain Layer (Business Logic)                   \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - AssessMovement UseCase                        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - DiagnoseError UseCase                         \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - ProvideCorrection UseCase                     \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - TrackProgress UseCase                         \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  Data Layer                                      \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 Camera   \u2502 \u2502 ML       \u2502 \u2502 BLE      \u2502        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 Module   \u2502 \u2502 Inference\u2502 \u2502 Manager  \u2502        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   Local Storage (Hive + File System)  \u2502    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   - Encrypted movement records         \u2502    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   - User profiles                      \u2502    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   - ML model cache (ONNX files)        \u2502    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   - Sensor data (Protobuf)             \u2502    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502 BLE 5.0\n                            \u2502 MTU: 500, CI: 7.5ms\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   WEARABLE SENSOR UNIT                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                ESP32-S3 Firmware (Arduino/FreeRTOS)    \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  Sensor Drivers                                  \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 LSM6DSV  \u2502 \u2502 MyoWare  \u2502 \u2502 DRV2605L \u2502        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 (IMU)    \u2502 \u2502 (EMG)    \u2502 \u2502 (Haptic) \u2502        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 I2C      \u2502 \u2502 Analog   \u2502 \u2502 I2C      \u2502        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  FreeRTOS Tasks                                  \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Task 1: Sensor Read (100Hz IMU, 200Hz EMG)   \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Task 2: BLE Transmit (NimBLE stack)          \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Task 3: Haptic Control (receive commands)    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  BLE Server (NimBLE)                             \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Custom GATT Service UUID                      \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Characteristic: Sensor Data (Notify)          \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Characteristic: Haptic Command (Write)        \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - MTU: 500 bytes, CI: 7.5ms                     \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Hardware                                              \u2502 \u2502\n\u2502  \u2502  - LiPo Battery (500mAh, 4+ hours)                     \u2502 \u2502\n\u2502  \u2502  - USB-C Charging                                      \u2502 \u2502\n\u2502  \u2502  - 3D-Printed Enclosure (ABS/PETG)                     \u2502 \u2502\n\u2502  \u2502  - Adjustable Strap (wrist/lower back mounting)        \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/hld/01-system-overview/#72-cloud-services-optionalfuture","title":"7.2 Cloud Services (Optional/Future)","text":"<p>MVP Deployment: Fully on-device processing (no cloud dependency)</p> <p>Future Cloud Features (Post-MVP):</p> <ol> <li>Movement Video Backup</li> <li>User-initiated upload (opt-in)</li> <li>S3-compatible storage (AWS, Backblaze B2)</li> <li>Encrypted at rest (AES-256)</li> <li> <p>Cost: ~$0.023/GB/month (S3 Standard)</p> </li> <li> <p>Cross-Device Sync</p> </li> <li>Sync movement metadata across user's devices (phone, tablet)</li> <li>DynamoDB for metadata storage</li> <li> <p>Cost: ~$0.25/GB/month (on-demand pricing)</p> </li> <li> <p>Aggregate Analytics</p> </li> <li>Anonymized improvement metrics (e.g., \"Average quality score improvement: +8% after 2 weeks\")</li> <li>No PII transmitted (movement type, error frequency only)</li> <li> <p>Used for ML model improvements</p> </li> <li> <p>Model Updates</p> </li> <li>A/B testing new ML models</li> <li>Download updated ONNX models over WiFi</li> <li>User controls: Auto-update on/off</li> </ol> <p>Privacy-First Design: - All sensitive data (movement videos, EMG signals, keypoints) stored locally by default - User controls cloud sync (opt-in only) - No PII transmitted without explicit consent - GDPR-compliant: Right to deletion, data portability</p> <p>Cloud Architecture (Future): <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     AWS Cloud (Optional)                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  S3 Bucket (Video Backup)                              \u2502 \u2502\n\u2502  \u2502  - User-uploaded movement videos (encrypted)           \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  DynamoDB (Metadata Sync)                              \u2502 \u2502\n\u2502  \u2502  - Movement metadata, session summaries                \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Lambda Functions                                       \u2502 \u2502\n\u2502  \u2502  - Process video uploads (generate thumbnails)         \u2502 \u2502\n\u2502  \u2502  - Sync data across devices                            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  CloudFront (CDN)                                       \u2502 \u2502\n\u2502  \u2502  - Distribute ML model updates                         \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"architecture/hld/01-system-overview/#8-performance-requirements","title":"8. Performance Requirements","text":""},{"location":"architecture/hld/01-system-overview/#81-latency-targets","title":"8.1 Latency Targets","text":"Metric Target P95 P99 Rationale Real-time feedback latency &lt;100ms &lt;120ms &lt;150ms Perceptible to user during movement, within human reaction time Post-action analysis delay &lt;5s &lt;7s &lt;10s Immediate review workflow, user waits for feedback BLE data transmission &lt;20ms &lt;30ms &lt;50ms 100Hz IMU streaming headroom, prevent buffer overflow ML inference (on-device) &lt;50ms &lt;70ms &lt;100ms 20+ FPS analysis, real-time feedback capable UI frame rate 60 FPS 55 FPS 50 FPS Smooth visual experience, no jank perception Camera-to-display total &lt;100ms &lt;120ms &lt;150ms End-to-end system latency (camera \u2192 pose \u2192 ML \u2192 render) <p>Latency Measurement Strategy: - Instrumentation: Log timestamps at each pipeline stage (camera, BLE, ML, render) - Metrics Collection: Aggregate P50/P95/P99 latencies per session - Performance Dashboard: Track latency trends over time, detect regressions - User Feedback: Collect qualitative feedback on perceived responsiveness</p>"},{"location":"architecture/hld/01-system-overview/#82-accuracy-targets","title":"8.2 Accuracy Targets","text":"Metric Target Validation Method Notes Pose estimation AP &gt;70% COCO validation set RTMPose-m: 75.8% AP (exceeds target) Movement classification accuracy &gt;85% Expert-labeled test set (200+ movements) Multi-class error type classification Error detection recall &gt;90% Expert-labeled test set Minimize false negatives (missed errors) Error detection precision &gt;80% Expert-labeled test set Balance false positives (over-alerting) Error severity MAE &lt;1.5 Expert rating comparison (0-10 scale) Mean Absolute Error on severity regression Correction effectiveness &gt;60% User improves quality score after applying correction Self-reported + quality score tracking <p>Accuracy Validation Plan: 1. Expert Labeling:    - Recruit certified golf instructors, personal trainers    - Label 200+ movements across error types (50% golf, 50% workout)    - Inter-rater agreement: Cohen's Kappa &gt;0.7</p> <ol> <li>Test Set Composition:</li> <li>30% \"clean\" movements (no errors)</li> <li>50% single-error movements</li> <li> <p>20% multi-error movements (test error priority ranking)</p> </li> <li> <p>Confusion Matrix Analysis:</p> </li> <li>Track which error types are confused (e.g., \"over-the-top\" vs. \"early extension\")</li> <li> <p>Refine model training data for confused classes</p> </li> <li> <p>User Study (Beta Testing):</p> </li> <li>20+ users, 5 sessions each</li> <li>Track quality score improvement over time</li> <li>Collect qualitative feedback on correction accuracy</li> </ol>"},{"location":"architecture/hld/01-system-overview/#83-resource-constraints","title":"8.3 Resource Constraints","text":"Resource Budget Notes App binary size &lt;150MB ML models dominate size (RTMPose: 25MB, Classifier: 50MB, Flutter framework: 40MB) Runtime RAM &lt;500MB Compete with other apps, prevent OS backgrounding Battery impact &lt;15%/hour Camera (40% of drain) + BLE (30%) + ML (20%) + Display (10%) Storage per session &lt;10MB 5 min video at 720p compressed (~5 MB) + sensor data (~5 MB) Network bandwidth (future) &lt;1 MB/session Metadata sync only (no video unless user-initiated) <p>Resource Optimization Strategies:</p> <ol> <li>App Binary Size:</li> <li>Model quantization (FP16 reduces size by 50%)</li> <li>On-demand model download (ship app with RTMPose only, download classifier on first run)</li> <li> <p>Asset compression (lossless PNG \u2192 lossy WebP for UI assets)</p> </li> <li> <p>Runtime RAM:</p> </li> <li>Memory pooling (pre-allocate camera frame buffers, reuse)</li> <li>Aggressive garbage collection tuning (Dart VM flags)</li> <li> <p>Unload unused models (e.g., unload RTMPose during post-action analysis)</p> </li> <li> <p>Battery Impact:</p> </li> <li>Dynamic frame rate (60 FPS \u2192 30 FPS when low battery)</li> <li>BLE connection interval adjustment (7.5ms \u2192 15ms when idle)</li> <li>Screen brightness reduction (user prompt when battery &lt;20%)</li> <li> <p>Camera resolution reduction (1080p \u2192 720p when low battery)</p> </li> <li> <p>Storage:</p> </li> <li>Video compression (H.264 at lower bitrate)</li> <li>Sensor data compression (Protobuf + zlib)</li> <li>Auto-delete old data (30-day retention policy)</li> </ol>"},{"location":"architecture/hld/01-system-overview/#84-scalability-targets","title":"8.4 Scalability Targets","text":"Metric MVP Production Notes Concurrent users 100 10,000+ Local-first architecture scales independently Movements per session 20-50 Unlimited Local storage constraint only Supported movement types 3-5 20+ Error taxonomy expansion Supported sports 2 (golf, workout) 5+ (tennis, baseball, yoga) Architectural flexibility"},{"location":"architecture/hld/01-system-overview/#9-security-privacy","title":"9. Security &amp; Privacy","text":""},{"location":"architecture/hld/01-system-overview/#91-data-protection","title":"9.1 Data Protection","text":"<p>Principle: Local-first architecture, user controls all data.</p> <p>Data Categories: 1. Personal Identifiable Information (PII):    - User name, email (stored locally, encrypted)    - Body measurements (height, weight)    - Storage: Hive encrypted box (AES-256)    - Transmission: Never transmitted (local-only)</p> <ol> <li>Movement Data:</li> <li>Camera frames, keypoints, IMU/EMG signals</li> <li>Storage: Local file system, optionally encrypted</li> <li> <p>Transmission: Only if user explicitly enables cloud backup</p> </li> <li> <p>Metadata:</p> </li> <li>Movement quality scores, error types, session summaries</li> <li>Storage: Hive database (encrypted)</li> <li>Transmission: Only if user enables cross-device sync</li> </ol> <p>Encryption: - At Rest: Hive encrypted boxes (AES-256, key derived from user PIN/biometric) - In Transit: TLS 1.3 for cloud sync (future feature) - Key Management: User-controlled (PIN/biometric unlock)</p> <p>GDPR Compliance: - Right to Access: Export all data as JSON (one-click export) - Right to Deletion: Delete all user data (local wipe, no cloud remnants) - Right to Portability: Export in machine-readable format (JSON, CSV) - Consent: Opt-in for cloud sync, explicit consent before any data transmission</p>"},{"location":"architecture/hld/01-system-overview/#92-firmware-security","title":"9.2 Firmware Security","text":"<p>ESP32-S3 Security Features: - Secure Boot: Verify firmware signature before boot (prevent malicious firmware) - Flash Encryption: Encrypt firmware in flash memory (prevent code extraction) - eFuse: One-time programmable memory for secure key storage</p> <p>BLE Security: - Pairing Method: Numeric Comparison Pairing (MITM protection) - Encryption: AES-128-CCM (BLE 5.0 standard) - Authentication: Device whitelist (only pair with owner's phone)</p> <p>Firmware Update Security: - MVP: No over-the-air (OTA) updates (physical USB connection required) - Production: Signed OTA updates (RSA-2048 signature verification)</p>"},{"location":"architecture/hld/01-system-overview/#93-threat-model","title":"9.3 Threat Model","text":"Threat Impact Likelihood Mitigation Eavesdropping on BLE Attacker intercepts IMU/EMG data Low BLE encryption (AES-128-CCM) Man-in-the-Middle (MITM) Attacker impersonates wearable Low Numeric Comparison Pairing Data Exfiltration (stolen phone) Attacker accesses movement data Medium Hive encryption (PIN/biometric required) Firmware Tampering Attacker modifies wearable firmware Low Secure Boot + Flash Encryption Cloud Sync Interception (future) Attacker intercepts synced metadata Low TLS 1.3 + certificate pinning <p>Security Testing: - Penetration Testing: Hire security firm for BLE/firmware audit (pre-production) - Code Review: Static analysis (SonarQube) + manual review - Dependency Scanning: Check for vulnerable packages (Dependabot)</p>"},{"location":"architecture/hld/01-system-overview/#10-development-roadmap","title":"10. Development Roadmap","text":""},{"location":"architecture/hld/01-system-overview/#phase-1-mvp-months-1-4","title":"Phase 1: MVP (Months 1-4)","text":"<p>Goal: Prove core technical feasibility, validate hardware + software stack.</p> <p>Deliverables: - \u2705 Architecture validation research (completed: ADRs 0001-0004, hardware selection) - [ ] Hardware prototype (single sensor unit)   - ESP32-S3 dev board + LSM6DSV16X + MyoWare 2.0 + DRV2605L   - 3D-printed enclosure (basic wearable form factor)   - BLE communication validated (100Hz IMU streaming) - [ ] Basic pose estimation pipeline   - RTMPose-m deployed to mobile (ONNX Runtime)   - 60 FPS camera capture validated   - Pose keypoints overlaid on live view - [ ] Flutter app skeleton   - Camera + BLE integration   - Basic UI (golf view, workout view)   - Local storage (Hive setup) - [ ] Simple error detection (2-3 movement types)   - Golf: Over-the-top swing, early extension   - Workout: Knee valgus (squat)   - Post-action visualization only (no real-time feedback yet) - [ ] Dataset collection (50+ labeled movements)   - Recruit 5-10 early testers   - Capture movements with expert labels</p> <p>Success Criteria: - Pose estimation runs at 30+ FPS on mid-range phone - BLE connection stable for 10+ minutes - Error detection accuracy &gt;75% on small test set - App doesn't crash during 15-minute session</p>"},{"location":"architecture/hld/01-system-overview/#phase-2-real-time-feedback-months-5-6","title":"Phase 2: Real-Time Feedback (Months 5-6)","text":"<p>Goal: Implement &lt;100ms real-time feedback loop, haptic integration.</p> <p>Deliverables: - [ ] Haptic feedback integration   - Trigger haptic patterns via BLE (latency &lt;80ms)   - User controls: Haptic intensity, error threshold - [ ] Latency optimization   - Model pruning (LSTM 128 \u2192 96 units)   - Frame skip + interpolation   - Target: &lt;100ms end-to-end latency (P95) - [ ] Live movement overlay   - Real-time skeleton rendering (60 FPS)   - Color-coded joints (green/yellow/red)   - Predicted trajectory (0.5s lookahead) - [ ] Error taxonomy expansion (5+ movement error types)   - Golf: Add reverse spine angle, chicken wing   - Workout: Add lumbar hyperextension, asymmetric loading - [ ] Sensor fusion refinement   - Tune IMU/EMG feature weights   - Improve sync algorithm (reduce max error to &lt;10ms)</p> <p>Success Criteria: - Real-time feedback latency &lt;100ms (P95) - User perceives haptic cues as \"in-sync\" with movement - Error detection accuracy &gt;85% on expanded test set - App maintains 60 FPS UI during real-time mode</p>"},{"location":"architecture/hld/01-system-overview/#phase-3-production-polish-months-7-9","title":"Phase 3: Production Polish (Months 7-9)","text":"<p>Goal: Production-ready app, beta testing program, prepare for public release.</p> <p>Deliverables: - [ ] UI/UX refinement   - User testing (20+ testers, 5 sessions each)   - Iterate on feedback (improve clarity of corrective cues)   - Onboarding flow (explain sensor placement, camera setup) - [ ] Battery optimization   - Target: &lt;15% battery drain per hour   - Dynamic frame rate, BLE interval adjustment - [ ] Cloud backup (optional feature)   - S3-compatible video upload   - Cross-device metadata sync   - Privacy controls (opt-in, encrypted) - [ ] Beta testing program   - 50+ users, 2-month program   - Track retention (7-day, 30-day)   - Collect NPS feedback - [ ] Production hardware (10-20 units)   - Custom PCB (replace dev board)   - Injection-molded enclosure   - CE/FCC certification prep</p> <p>Success Criteria: - 7-day retention &gt;50% (golf), &gt;40% (workout) - NPS score &gt;50 (validated product-market fit) - Zero critical bugs in beta testing - App approved by Apple App Store, Google Play Store</p>"},{"location":"architecture/hld/01-system-overview/#phase-4-future-enhancements-months-10","title":"Phase 4: Future Enhancements (Months 10+)","text":"<p>Potential Features (prioritized by user demand): 1. Module 3 Addition (Long-term Programming)    - If retention data shows users want periodization    - Training plan builder, progressive overload automation</p> <ol> <li>Multi-Sensor Support</li> <li>2+ wearables for bilateral analysis (left wrist + right wrist)</li> <li> <p>Detect left-right asymmetries</p> </li> <li> <p>Advanced Biomechanics</p> </li> <li>Joint torque estimation (inverse kinematics)</li> <li> <p>Force plate integration (ground reaction forces)</p> </li> <li> <p>Social Features</p> </li> <li>Coach sharing (send movement videos to coach)</li> <li> <p>Peer comparison (compare to friends' stats)</p> </li> <li> <p>Sport Expansion</p> </li> <li>Tennis serve analysis</li> <li>Baseball pitch mechanics</li> <li>Yoga pose assessment</li> </ol>"},{"location":"architecture/hld/01-system-overview/#11-success-metrics","title":"11. Success Metrics","text":""},{"location":"architecture/hld/01-system-overview/#111-technical-kpis","title":"11.1 Technical KPIs","text":"Metric Target Measurement Method Real-time feedback latency &lt;100ms (P95) Log timestamps at each pipeline stage App crash rate &lt;0.1% Firebase Crashlytics (production) BLE connection reliability &gt;95% Track disconnections per session ML accuracy (error detection) &gt;85% Expert-labeled test set (200+ movements) UI frame rate 60 FPS (P50) Flutter DevTools frame profiling Battery drain &lt;15%/hour Battery level sampling (every 5 min) <p>Monitoring Strategy: - MVP: Manual logging, weekly review - Production: Automated telemetry (opt-in, anonymized) - Alerts: Slack notification if KPI drops below threshold</p>"},{"location":"architecture/hld/01-system-overview/#112-user-kpis","title":"11.2 User KPIs","text":"Metric Target (Golf) Target (Workout) Measurement Method 7-day retention &gt;50% &gt;40% Cohort analysis (sign-up date \u2192 7 days) 30-day retention &gt;30% &gt;20% Cohort analysis (sign-up date \u2192 30 days) Session length &gt;15 min &gt;12 min Track app foreground time Sessions per week 2-3 3-4 Count sessions (7-day rolling window) NPS score &gt;50 &gt;50 In-app survey (monthly) Quality score improvement +10% (30 days) +8% (30 days) Track avg quality score over time <p>User Feedback Collection: - In-app survey: Monthly NPS survey (1 question + optional comment) - Beta tester interviews: 1-on-1 interviews (30 min each) - Support tickets: Track common complaints, feature requests</p>"},{"location":"architecture/hld/01-system-overview/#113-business-kpis-future","title":"11.3 Business KPIs (Future)","text":"Metric Target Notes Customer Acquisition Cost (CAC) &lt;$20 Organic + paid ads Lifetime Value (LTV) &gt;$100 Subscription model (future) LTV:CAC Ratio &gt;5:1 Healthy unit economics Monthly Active Users (MAU) 1,000 (Year 1) Viral growth + referrals Churn Rate &lt;5%/month Retention-focused product"},{"location":"architecture/hld/01-system-overview/#12-references","title":"12. References","text":""},{"location":"architecture/hld/01-system-overview/#architecture-decision-records-adrs","title":"Architecture Decision Records (ADRs)","text":"<ul> <li>ADR-0001: Multi-Repository Structure</li> <li>ADR-0002: LSM6DSV16X IMU Selection</li> <li>ADR-0003: Flutter for Mobile Development</li> <li>ADR-0004: Four-Module Architecture (Simplified)</li> </ul>"},{"location":"architecture/hld/01-system-overview/#technical-resources","title":"Technical Resources","text":"<ul> <li>Hardware Comparison Matrix</li> <li>ML Framework Comparison</li> <li>Mobile Framework Comparison</li> <li>BLE Optimization Guide</li> </ul>"},{"location":"architecture/hld/01-system-overview/#research-papers","title":"Research Papers","text":"<ul> <li>Schmidt, R. A., &amp; Lee, T. D. (2011). Motor Control and Learning: A Behavioral Emphasis (5th ed.). Human Kinetics.</li> <li>Sigrist, R., et al. (2013). \"Augmented visual, auditory, haptic, and multimodal feedback in motor learning: A review.\" Psychonomic Bulletin &amp; Review, 20(1), 21-53.</li> <li>Wulf, G., &amp; Lewthwaite, R. (2016). \"Optimizing performance through intrinsic motivation and attention for learning: The OPTIMAL theory of motor learning.\" Psychonomic Bulletin &amp; Review, 23(5), 1382-1414.</li> </ul>"},{"location":"architecture/hld/01-system-overview/#external-documentation","title":"External Documentation","text":"<ul> <li>RTMPose Documentation</li> <li>YOLO11 Pose Documentation</li> <li>Flutter Performance Best Practices</li> <li>ESP32-S3 Technical Reference Manual</li> <li>LSM6DSV16X Datasheet</li> </ul>"},{"location":"architecture/hld/01-system-overview/#document-change-log","title":"Document Change Log","text":"Version Date Author Changes 1.0 2025-12-01 System Architect Initial HLD, 4-module architecture, validated technology stack <p>Document Status: \u2705 Validated Architecture Review Cycle: Quarterly (next review: 2026-03-01) Feedback: Open GitHub issues for technical questions or architecture suggestions</p>"},{"location":"architecture/hld/02-data-flow/","title":"Data Flow Architecture","text":""},{"location":"architecture/hld/02-data-flow/#overview","title":"Overview","text":"<p>Movement Chain AI processes multimodal sensor data from three sources: camera (60 FPS), IMU (100 Hz), and EMG (200 Hz). This document details the data flow architecture, synchronization strategies, format specifications, and processing paths required to achieve real-time movement analysis with latency under 100ms.</p>"},{"location":"architecture/hld/02-data-flow/#1-multimodal-sensor-data-flow","title":"1. Multimodal Sensor Data Flow","text":""},{"location":"architecture/hld/02-data-flow/#11-data-sources-and-characteristics","title":"1.1 Data Sources and Characteristics","text":"Sensor Sampling Rate Data Size Latency Budget Purpose Camera 60 FPS 640\u00d7480 RGB (~900KB/frame) 30ms Pose estimation, visual context IMU (6-axis) 100 Hz 24 bytes/sample 10ms Acceleration, angular velocity EMG (4-channel) 200 Hz 8 bytes/sample 10ms Muscle activation patterns"},{"location":"architecture/hld/02-data-flow/#12-end-to-end-data-pipeline","title":"1.2 End-to-End Data Pipeline","text":"<pre><code>sequenceDiagram\n    participant Cam as Camera\n    participant IMU as IMU Sensor\n    participant EMG as EMG Sensor\n    participant BLE as BLE Module\n    participant Sync as Synchronizer\n    participant Pose as Pose Estimator\n    participant Fusion as Sensor Fusion\n    participant ML as ML Classifier\n    participant UI as UI/Feedback\n\n    par Parallel Acquisition\n        Cam-&gt;&gt;Sync: Frame @ 60 FPS (t_cam)\n        IMU-&gt;&gt;BLE: Accel/Gyro @ 100 Hz\n        EMG-&gt;&gt;BLE: Muscle data @ 200 Hz\n    end\n\n    BLE-&gt;&gt;Sync: Sensor packet (t_cam ref)\n    Sync-&gt;&gt;Sync: Buffer &amp; align to t_cam\n\n    Cam-&gt;&gt;Pose: RGB Frame (640\u00d7480)\n    Pose-&gt;&gt;Fusion: 17 keypoints + confidence\n    Sync-&gt;&gt;Fusion: IMU[t_cam-50ms:t_cam]\n    Sync-&gt;&gt;Fusion: EMG[t_cam-50ms:t_cam]\n\n    Fusion-&gt;&gt;Fusion: Time-align &amp; interpolate\n    Fusion-&gt;&gt;ML: Fused feature vector\n    ML-&gt;&gt;ML: LSTM+Transformer inference\n    ML-&gt;&gt;UI: Error classification + confidence\n    UI-&gt;&gt;UI: Haptic feedback (if error)</code></pre>"},{"location":"architecture/hld/02-data-flow/#13-data-flow-stages","title":"1.3 Data Flow Stages","text":""},{"location":"architecture/hld/02-data-flow/#stage-1-acquisition-0-30ms","title":"Stage 1: Acquisition (0-30ms)","text":"<ul> <li>Camera: Flutter <code>camera</code> plugin captures frame at t_cam (reference timestamp)</li> <li>BLE Streaming: Firmware transmits IMU/EMG packets via GATT notifications</li> <li>MTU: 500 bytes (allows ~20 IMU samples or ~60 EMG samples per packet)</li> <li>Connection interval: 7.5ms (validated for 100 Hz throughput)</li> <li>Packets include firmware timestamp offset from BLE connection start</li> </ul>"},{"location":"architecture/hld/02-data-flow/#stage-2-synchronization-30-40ms","title":"Stage 2: Synchronization (30-40ms)","text":"<ul> <li>Timestamp Alignment: All sensor data indexed to camera timestamps</li> <li>IMU/EMG firmware timestamps converted to camera timebase using initial offset calibration</li> <li>Circular buffer stores last 500ms of IMU/EMG data (50 IMU samples, 100 EMG samples)</li> <li>Temporal Windowing: For each camera frame at t_cam, extract:</li> <li>IMU samples in window [t_cam - 50ms, t_cam]</li> <li>EMG samples in window [t_cam - 50ms, t_cam]</li> </ul>"},{"location":"architecture/hld/02-data-flow/#stage-3-pose-estimation-40-70ms","title":"Stage 3: Pose Estimation (40-70ms)","text":"<ul> <li>Model: RTMPose-m (optimized for mobile)</li> <li>Input: 640\u00d7480 RGB frame \u2192 resized to 256\u00d7192</li> <li>Output: 17 COCO keypoints with confidence scores</li> <li>Processing: GPU-accelerated inference (Core ML/TensorFlow Lite)</li> </ul>"},{"location":"architecture/hld/02-data-flow/#stage-4-sensor-fusion-70-80ms","title":"Stage 4: Sensor Fusion (70-80ms)","text":"<ul> <li>Interpolation: Upsample IMU to match EMG rate (200 Hz) using cubic spline</li> <li>Feature Extraction:</li> <li>Pose: Joint angles (elbow, knee, hip, shoulder) + velocity estimates</li> <li>IMU: Orientation quaternion, linear acceleration (gravity-compensated)</li> <li>EMG: RMS amplitude per channel, frequency-domain features (50-150 Hz band power)</li> <li>Normalization: Z-score normalization per feature dimension</li> </ul>"},{"location":"architecture/hld/02-data-flow/#stage-5-ml-inference-80-120ms","title":"Stage 5: ML Inference (80-120ms)","text":"<ul> <li>Model Architecture: LSTM (2 layers, 128 units) + Transformer encoder (4 heads, 2 layers)</li> <li>Input: Sliding window of 30 frames (500ms @ 60 FPS) \u00d7 fused feature vector</li> <li>Output: Error classification logits + confidence scores</li> <li>Quantization: INT8 quantization for 2-3x speedup (target: &lt;40ms inference)</li> </ul>"},{"location":"architecture/hld/02-data-flow/#stage-6-feedback-120-126ms","title":"Stage 6: Feedback (120-126ms)","text":"<ul> <li>Haptic Triggering: If error confidence &gt; 0.7, send haptic pattern via BLE</li> <li>UI Rendering: Display skeleton overlay + error annotations at 60 FPS</li> </ul>"},{"location":"architecture/hld/02-data-flow/#2-synchronization-strategy","title":"2. Synchronization Strategy","text":""},{"location":"architecture/hld/02-data-flow/#21-camera-timestamp-as-reference","title":"2.1 Camera Timestamp as Reference","text":"<p>Design Rationale: - Camera frames are the lowest-rate signal (60 FPS) \u2192 natural synchronization anchor - Pose estimation requires visual context \u2192 camera timestamp defines \"analysis instant\" - BLE streaming is asynchronous \u2192 must be aligned post-hoc</p>"},{"location":"architecture/hld/02-data-flow/#22-timestamp-conversion-protocol","title":"2.2 Timestamp Conversion Protocol","text":""},{"location":"architecture/hld/02-data-flow/#initialization-phase-connection-establishment","title":"Initialization Phase (Connection Establishment)","text":"<pre><code>t_app_connect = System.currentTimeMillis()  // App records BLE connection time\n</code></pre> <p>Firmware sends initial calibration packet: <pre><code>message TimestampCalibration {\n  uint32 firmware_millis = 1;  // millis() at connection\n  uint64 connection_timestamp = 2;  // Echo t_app_connect\n}\n</code></pre></p> <p>Calculate offset: <pre><code>offset = t_app_connect - firmware_millis\n</code></pre></p>"},{"location":"architecture/hld/02-data-flow/#runtime-alignment","title":"Runtime Alignment","text":"<p>For each BLE packet with firmware timestamp <code>t_fw</code>: <pre><code>t_aligned = t_fw + offset  // Convert to app timebase\n</code></pre></p> <p>For each camera frame at <code>t_cam</code>, retrieve: <pre><code>imu_window = buffer.get(t_cam - 50ms, t_cam)\nemg_window = buffer.get(t_cam - 50ms, t_cam)\n</code></pre></p>"},{"location":"architecture/hld/02-data-flow/#23-clock-drift-mitigation","title":"2.3 Clock Drift Mitigation","text":"<p>Problem: Firmware clock (millis()) drifts ~\u00b150 ppm relative to system clock - Worst case: 180ms drift over 1-hour session</p> <p>Solution: Periodic re-calibration - Every 60 seconds, firmware sends heartbeat with updated <code>firmware_millis</code> - App recalculates offset using moving average:   <pre><code>offset_new = 0.9 * offset_old + 0.1 * (t_app_now - firmware_millis)\n</code></pre></p>"},{"location":"architecture/hld/02-data-flow/#24-handling-packet-loss","title":"2.4 Handling Packet Loss","text":"<p>BLE Reliability: 99.5% packet delivery at 7.5ms interval - Expected loss: ~0.5 packets/second at 100 Hz</p> <p>Recovery Strategy: 1. Sequence Numbers: Each BLE packet includes monotonic sequence ID 2. Gap Detection: If <code>seq_received != seq_expected</code>, mark gap 3. Interpolation: Linear interpolation for gaps \u22643 samples (30ms) 4. Drop Frame: If gap &gt;3 samples, skip ML inference for that camera frame</p>"},{"location":"architecture/hld/02-data-flow/#3-data-format-specifications","title":"3. Data Format Specifications","text":""},{"location":"architecture/hld/02-data-flow/#31-ble-protocol-buffer-schema","title":"3.1 BLE Protocol Buffer Schema","text":"<pre><code>syntax = \"proto3\";\n\nmessage SensorPacket {\n  uint32 sequence_id = 1;         // Monotonic counter\n  uint32 firmware_timestamp = 2;   // millis() at packet creation\n\n  message IMUSample {\n    float accel_x = 1;  // m/s\u00b2 (range: \u00b116g)\n    float accel_y = 2;\n    float accel_z = 3;\n    float gyro_x = 4;   // rad/s (range: \u00b12000\u00b0/s)\n    float gyro_y = 5;\n    float gyro_z = 6;\n  }\n\n  message EMGSample {\n    uint16 channel_1 = 1;  // 12-bit ADC value (0-4095)\n    uint16 channel_2 = 2;\n    uint16 channel_3 = 3;\n    uint16 channel_4 = 4;\n  }\n\n  repeated IMUSample imu_samples = 3;    // Up to 20 samples/packet\n  repeated EMGSample emg_samples = 4;    // Up to 60 samples/packet\n}\n</code></pre> <p>Packet Size Analysis: - IMU sample: 6 floats \u00d7 4 bytes = 24 bytes - EMG sample: 4 uint16 \u00d7 2 bytes = 8 bytes - Max packet: 20 IMU (480 bytes) OR 60 EMG (480 bytes) + header (20 bytes) = 500 bytes (fits MTU)</p>"},{"location":"architecture/hld/02-data-flow/#32-pose-estimation-output","title":"3.2 Pose Estimation Output","text":"<pre><code>class PoseEstimationResult {\n  final int frameId;\n  final DateTime timestamp;\n  final List&lt;Keypoint&gt; keypoints;  // 17 COCO keypoints\n  final double inferenceTimeMs;\n\n  // Keypoint indices (COCO format)\n  static const int NOSE = 0;\n  static const int LEFT_EYE = 1;\n  static const int RIGHT_EYE = 2;\n  static const int LEFT_EAR = 3;\n  static const int RIGHT_EAR = 4;\n  static const int LEFT_SHOULDER = 5;\n  static const int RIGHT_SHOULDER = 6;\n  static const int LEFT_ELBOW = 7;\n  static const int RIGHT_ELBOW = 8;\n  static const int LEFT_WRIST = 9;\n  static const int RIGHT_WRIST = 10;\n  static const int LEFT_HIP = 11;\n  static const int RIGHT_HIP = 12;\n  static const int LEFT_KNEE = 13;\n  static const int RIGHT_KNEE = 14;\n  static const int LEFT_ANKLE = 15;\n  static const int RIGHT_ANKLE = 16;\n}\n\nclass Keypoint {\n  final double x;           // Normalized [0, 1]\n  final double y;           // Normalized [0, 1]\n  final double confidence;  // [0, 1]\n}\n</code></pre>"},{"location":"architecture/hld/02-data-flow/#33-fused-feature-vector","title":"3.3 Fused Feature Vector","text":"<pre><code>class FusedFeatures {\n  // Pose-derived (17 values)\n  final List&lt;double&gt; jointAngles;  // 8 angles (elbow, knee, shoulder, hip)\n  final List&lt;double&gt; jointVelocities;  // 8 velocities (deg/s)\n  final double torsoOrientation;  // Angle relative to vertical\n\n  // IMU-derived (10 values)\n  final Quaternion orientation;  // 4 values (w, x, y, z)\n  final Vector3 linearAccel;     // 3 values (gravity-compensated)\n  final Vector3 angularVel;      // 3 values\n\n  // EMG-derived (8 values)\n  final List&lt;double&gt; rmsAmplitude;  // 4 channels\n  final List&lt;double&gt; bandPower;     // 4 channels (50-150 Hz)\n\n  // Total: 35 features per timestep\n  // ML input: 30 timesteps \u00d7 35 features = 1050 values\n}\n</code></pre>"},{"location":"architecture/hld/02-data-flow/#34-local-storage-format-sqlite","title":"3.4 Local Storage Format (SQLite)","text":"<pre><code>-- Training data collection\nCREATE TABLE sessions (\n  id INTEGER PRIMARY KEY,\n  user_id TEXT NOT NULL,\n  exercise_type TEXT NOT NULL,  -- 'squat', 'pushup', etc.\n  start_time INTEGER NOT NULL,\n  duration_ms INTEGER NOT NULL\n);\n\nCREATE TABLE frames (\n  id INTEGER PRIMARY KEY,\n  session_id INTEGER NOT NULL,\n  timestamp INTEGER NOT NULL,  -- Unix epoch ms\n  pose_json TEXT NOT NULL,     -- JSON array of 17 keypoints\n  imu_json TEXT NOT NULL,      -- JSON array of IMU samples\n  emg_json TEXT NOT NULL,      -- JSON array of EMG samples\n  error_label TEXT,            -- Ground truth label (if available)\n  FOREIGN KEY (session_id) REFERENCES sessions(id)\n);\n\nCREATE INDEX idx_session_timestamp ON frames(session_id, timestamp);\n</code></pre> <p>Compression: 60-second session @ 60 FPS = 3,600 frames - Pose data: ~1.5 KB/frame (17 keypoints \u00d7 3 values \u00d7 8 bytes) - Sensor data: ~2 KB/frame (50 IMU + 100 EMG samples) - Total: ~12.6 MB/minute (compress to ~3-4 MB using GZIP)</p>"},{"location":"architecture/hld/02-data-flow/#4-real-time-vs-batch-processing-paths","title":"4. Real-Time vs Batch Processing Paths","text":""},{"location":"architecture/hld/02-data-flow/#41-real-time-path-live-feedback","title":"4.1 Real-Time Path (Live Feedback)","text":"<p>Use Case: Workout guidance with immediate error correction</p> <p>Data Flow: <pre><code>Camera \u2192 Pose \u2192 Fusion \u2192 ML \u2192 Feedback (126ms latency)\n       \u2193\n  Discard frame (no storage)\n</code></pre></p> <p>Characteristics: - No persistence: Frames processed and discarded - Latency-critical: Must complete within 100ms target - Optimizations:   - Skip every other frame if latency exceeds budget (30 FPS fallback)   - Disable logging/telemetry   - Use quantized INT8 models</p>"},{"location":"architecture/hld/02-data-flow/#42-batch-processing-path-post-workout-analysis","title":"4.2 Batch Processing Path (Post-Workout Analysis)","text":"<p>Use Case: Detailed movement analytics, model retraining, progress tracking</p> <p>Data Flow: <pre><code>Camera \u2192 Pose \u2192 Fusion \u2192 Storage (SQLite)\n                              \u2193\n                        Offline ML (no latency constraint)\n                              \u2193\n                      Generate insights (summary stats, trend analysis)\n</code></pre></p> <p>Characteristics: - Full persistence: All frames + raw sensor data saved - High accuracy: Use FP32 models, multi-pass analysis - Additional processing:   - Optical flow analysis for smooth motion tracking   - 3D pose reconstruction (if depth camera available)   - Biomechanical modeling (joint torques, forces)</p>"},{"location":"architecture/hld/02-data-flow/#43-hybrid-mode-selective-recording","title":"4.3 Hybrid Mode (Selective Recording)","text":"<p>Use Case: Record only interesting segments (errors, PRs, user-flagged moments)</p> <p>Trigger Conditions: - Error confidence &gt; 0.7 \u2192 Save \u00b12 seconds around error - User presses \"record\" button \u2192 Save session - New personal record detected \u2192 Save rep</p> <p>Implementation: <pre><code>class SelectiveRecorder {\n  CircularBuffer&lt;Frame&gt; buffer;  // Last 2 seconds (120 frames)\n\n  void onFrame(Frame frame, double errorConfidence) {\n    buffer.add(frame);\n\n    if (errorConfidence &gt; 0.7) {\n      // Flush buffer to storage\n      db.saveFrames(buffer.getAll());\n\n      // Continue recording for next 2 seconds\n      isRecording = true;\n      recordingEndTime = now() + 2000;\n    }\n  }\n}\n</code></pre></p>"},{"location":"architecture/hld/02-data-flow/#5-data-quality-and-validation","title":"5. Data Quality and Validation","text":""},{"location":"architecture/hld/02-data-flow/#51-input-validation","title":"5.1 Input Validation","text":"Check Criteria Action on Failure Camera frame Non-null, correct dimensions Skip frame, log warning BLE packet Valid Protobuf, seq_id increments Request retransmission (if possible) Pose confidence Mean keypoint confidence &gt; 0.5 Display \"Move closer to camera\" IMU range Accel \u226416g, Gyro \u22642000\u00b0/s Clamp values, flag outlier EMG range ADC value \u22644095 Clamp to max, check sensor connection"},{"location":"architecture/hld/02-data-flow/#52-synchronization-validation","title":"5.2 Synchronization Validation","text":"<p>Test Protocol: 1. Apply known stimulus (e.g., shake device while recording) 2. Verify IMU spike aligns with camera motion blur within \u00b116ms (1 frame @ 60 FPS) 3. Verify EMG spike (manual muscle contraction) aligns with pose change</p> <p>Metrics: - Sync error: Mean absolute difference between expected and actual alignment - Target: &lt;16ms (1 frame tolerance)</p>"},{"location":"architecture/hld/02-data-flow/#6-failure-modes-and-degradation","title":"6. Failure Modes and Degradation","text":"Failure Detection Graceful Degradation BLE disconnection No packets for 100ms Pause inference, show reconnection UI Camera obstruction Pose confidence &lt;0.3 Pause feedback, prompt user High latency (&gt;150ms) Timestamp delta tracking Drop to 30 FPS, disable EMG fusion Low battery (&lt;10%) System API Reduce camera resolution to 320\u00d7240"},{"location":"architecture/hld/02-data-flow/#7-performance-monitoring","title":"7. Performance Monitoring","text":""},{"location":"architecture/hld/02-data-flow/#71-telemetry-collection","title":"7.1 Telemetry Collection","text":"<pre><code>class PipelineMetrics {\n  final Stopwatch totalLatency;\n  final Stopwatch poseInference;\n  final Stopwatch fusion;\n  final Stopwatch mlInference;\n\n  int droppedFrames = 0;\n  int blePacketLoss = 0;\n\n  void logMetrics() {\n    // Log every 60 seconds\n    print('Avg latency: ${totalLatency.averageMs()}ms');\n    print('Dropped frames: $droppedFrames');\n    print('BLE loss rate: ${blePacketLoss / 6000}%');  // 100 Hz \u00d7 60s\n  }\n}\n</code></pre>"},{"location":"architecture/hld/02-data-flow/#72-bottleneck-identification","title":"7.2 Bottleneck Identification","text":"<p>Profiling Strategy: 1. Add timestamps at each pipeline stage 2. Calculate percentiles (P50, P95, P99) over 1-minute window 3. Identify stage where P95 exceeds budget</p> <p>Example Output: <pre><code>Stage            P50    P95    P99    Budget   Status\nCamera           25ms   32ms   40ms   30ms     \u26a0\ufe0f Warning\nPose inference   28ms   45ms   60ms   30ms     \u274c Over budget\nFusion           8ms    12ms   15ms   10ms     \u2705 OK\nML inference     35ms   50ms   65ms   40ms     \u274c Over budget\n</code></pre></p>"},{"location":"architecture/hld/02-data-flow/#8-future-optimizations","title":"8. Future Optimizations","text":"<ol> <li>Predictive Synchronization: Use IMU to predict pose at t_cam + 16ms (next frame)</li> <li>Adaptive Sampling: Reduce IMU/EMG rate to 50 Hz during low-motion periods</li> <li>Edge TPU: Offload pose inference to dedicated ML accelerator (target: 10ms inference)</li> <li>Delta Encoding: Transmit only IMU/EMG deltas from previous sample to reduce BLE bandwidth</li> <li>On-Device Training: Fine-tune ML model using user-specific data collected during batch processing</li> </ol>"},{"location":"architecture/hld/02-data-flow/#summary","title":"Summary","text":"<p>The data flow architecture achieves real-time multimodal fusion through: - Camera-centric synchronization: All sensors aligned to 60 FPS visual reference - Efficient BLE protocol: 500-byte MTU with Protobuf encoding supports 100 Hz streaming - Staged processing: Clear separation of concerns (acquisition \u2192 sync \u2192 pose \u2192 fusion \u2192 ML) - Dual pathways: Real-time feedback path optimized for latency, batch path for accuracy</p> <p>Current Performance: 126ms average latency (26ms over target) Optimization Priority: Pose inference (45ms \u2192 20ms via quantization) + ML inference (50ms \u2192 30ms via pruning)</p>"},{"location":"architecture/hld/03-integration-patterns/","title":"Integration Patterns","text":""},{"location":"architecture/hld/03-integration-patterns/#overview","title":"Overview","text":"<p>This document details the integration patterns for Movement Chain AI, covering BLE communication, Flutter-firmware interaction, ML model integration, haptic feedback, and error handling strategies. Each pattern includes implementation guidelines, code examples, and performance considerations.</p>"},{"location":"architecture/hld/03-integration-patterns/#1-ble-communication-protocol","title":"1. BLE Communication Protocol","text":""},{"location":"architecture/hld/03-integration-patterns/#11-gatt-service-architecture","title":"1.1 GATT Service Architecture","text":"<p>Service UUID Design: <pre><code>Primary Service: 0000180f-0000-1000-8000-00805f9b34fb (Custom Movement Service)\n\u251c\u2500\u2500 Characteristic 1: Sensor Data Stream (UUID: 0x2A58)\n\u2502   \u251c\u2500\u2500 Properties: NOTIFY\n\u2502   \u251c\u2500\u2500 Max length: 500 bytes (MTU size)\n\u2502   \u2514\u2500\u2500 Update rate: 100 Hz (every 10ms)\n\u251c\u2500\u2500 Characteristic 2: Haptic Command (UUID: 0x2A59)\n\u2502   \u251c\u2500\u2500 Properties: WRITE\n\u2502   \u251c\u2500\u2500 Max length: 20 bytes\n\u2502   \u2514\u2500\u2500 Format: [pattern_id, intensity, duration_ms]\n\u251c\u2500\u2500 Characteristic 3: Device Control (UUID: 0x2A5A)\n\u2502   \u251c\u2500\u2500 Properties: READ | WRITE\n\u2502   \u2514\u2500\u2500 Commands: START_STREAM, STOP_STREAM, CALIBRATE, GET_STATUS\n\u2514\u2500\u2500 Characteristic 4: Timestamp Sync (UUID: 0x2A5B)\n    \u251c\u2500\u2500 Properties: NOTIFY (one-time on connection)\n    \u2514\u2500\u2500 Format: [firmware_millis, connection_timestamp]\n</code></pre></p>"},{"location":"architecture/hld/03-integration-patterns/#12-connection-parameters","title":"1.2 Connection Parameters","text":"<p>Optimized for Low-Latency Streaming: <pre><code>// Firmware: nRF Connect SDK configuration\n#define MIN_CONN_INTERVAL    MSEC_TO_UNITS(7.5, UNIT_1_25_MS)  // 7.5ms\n#define MAX_CONN_INTERVAL    MSEC_TO_UNITS(7.5, UNIT_1_25_MS)  // 7.5ms\n#define SLAVE_LATENCY        0                                   // No latency\n#define CONN_SUP_TIMEOUT     MSEC_TO_UNITS(4000, UNIT_10_MS)    // 4 seconds\n#define MTU_SIZE             500                                 // Increased from default 23\n</code></pre></p> <p>Connection Interval Rationale: - 7.5ms interval allows 133 packets/second - 100 Hz sensor rate requires 100 packets/second - 25% overhead margin for retransmissions and control packets</p>"},{"location":"architecture/hld/03-integration-patterns/#13-data-streaming-protocol","title":"1.3 Data Streaming Protocol","text":""},{"location":"architecture/hld/03-integration-patterns/#packet-structure-protobuf","title":"Packet Structure (Protobuf)","text":"<p>Sensor Data Characteristic (0x2A58): <pre><code>message SensorPacket {\n  uint32 sequence_id = 1;         // Monotonic counter (wraps at UINT32_MAX)\n  uint32 firmware_timestamp = 2;  // millis() since boot\n\n  message IMUSample {\n    sint32 accel_x_mg = 1;  // Milligravities (\u00b116000 mg range)\n    sint32 accel_y_mg = 2;\n    sint32 accel_z_mg = 3;\n    sint32 gyro_x_mdps = 4; // Millidegrees/sec (\u00b12000000 mdps range)\n    sint32 gyro_y_mdps = 5;\n    sint32 gyro_z_mdps = 6;\n  }\n\n  message EMGSample {\n    uint32 timestamp_offset_us = 1;  // Microseconds from packet timestamp\n    repeated uint32 channels = 2;     // 4 channels, 12-bit ADC values\n  }\n\n  repeated IMUSample imu_samples = 3;    // Batch of samples since last packet\n  repeated EMGSample emg_samples = 4;\n}\n</code></pre></p> <p>Encoding Optimization: - Use <code>sint32</code> (ZigZag encoding) for signed values \u2192 better compression for small deltas - IMU in milligravities/millidegrees \u2192 avoid floating-point overhead - EMG timestamp offsets \u2192 enable precise sub-millisecond timing</p>"},{"location":"architecture/hld/03-integration-patterns/#firmware-transmission-logic","title":"Firmware Transmission Logic","text":"<pre><code>// Pseudocode: Firmware main loop\nvoid sensor_loop() {\n    static SensorPacket packet;\n    static uint32_t last_tx_time = 0;\n    static uint32_t sequence_id = 0;\n\n    // High-frequency sensor polling (200 Hz for EMG)\n    if (emg_data_ready()) {\n        EMGSample sample = read_emg();\n        sample.timestamp_offset_us = micros() - packet.firmware_timestamp;\n        packet.add_emg_sample(sample);\n    }\n\n    if (imu_data_ready()) {  // Triggers at 100 Hz\n        IMUSample sample = read_imu();\n        packet.add_imu_sample(sample);\n    }\n\n    // Transmit every 10ms (100 Hz packet rate)\n    if (millis() - last_tx_time &gt;= 10) {\n        packet.sequence_id = sequence_id++;\n        packet.firmware_timestamp = millis();\n\n        // Serialize and transmit\n        uint8_t buffer[500];\n        size_t len = packet.encode(buffer);\n        ble_gatt_notify(SENSOR_DATA_CHAR, buffer, len);\n\n        // Reset packet for next batch\n        packet.clear();\n        last_tx_time = millis();\n    }\n}\n</code></pre> <p>Transmission Guarantees: - GATT notifications are \"fire and forget\" (no ACK from central) - Sequence IDs enable gap detection on Flutter side - Expected packet loss: &lt;0.5% under normal conditions</p>"},{"location":"architecture/hld/03-integration-patterns/#2-flutter-firmware-integration","title":"2. Flutter-Firmware Integration","text":""},{"location":"architecture/hld/03-integration-patterns/#21-ble-stack-flutter_reactive_ble","title":"2.1 BLE Stack: flutter_reactive_ble","text":"<p>Library Choice Rationale: - <code>flutter_reactive_ble</code>: Stream-based API, better memory management than <code>flutter_blue_plus</code> - Handles connection state automatically - Efficient notification subscription with backpressure handling</p>"},{"location":"architecture/hld/03-integration-patterns/#22-connection-management-pattern","title":"2.2 Connection Management Pattern","text":"<pre><code>class BLEDeviceManager {\n  final FlutterReactiveBle _ble = FlutterReactiveBle();\n  StreamSubscription&lt;ConnectionStateUpdate&gt;? _connectionSubscription;\n  StreamSubscription&lt;List&lt;int&gt;&gt;? _sensorDataSubscription;\n\n  final String deviceId;\n  final _sensorDataController = StreamController&lt;SensorPacket&gt;.broadcast();\n\n  // Public stream for consumers\n  Stream&lt;SensorPacket&gt; get sensorDataStream =&gt; _sensorDataController.stream;\n\n  Future&lt;void&gt; connect() async {\n    // Step 1: Scan and connect\n    _connectionSubscription = _ble.connectToDevice(\n      id: deviceId,\n      connectionTimeout: Duration(seconds: 10),\n    ).listen((state) {\n      if (state.connectionState == DeviceConnectionState.connected) {\n        _onConnected();\n      } else if (state.connectionState == DeviceConnectionState.disconnected) {\n        _onDisconnected(state.failure);\n      }\n    });\n  }\n\n  Future&lt;void&gt; _onConnected() async {\n    print('[BLE] Connected to $deviceId');\n\n    // Step 2: Request MTU increase (critical for performance)\n    try {\n      final mtu = await _ble.requestMtu(deviceId: deviceId, mtu: 500);\n      print('[BLE] MTU negotiated: $mtu bytes');\n      if (mtu &lt; 500) {\n        print('[BLE] Warning: MTU lower than requested, may impact throughput');\n      }\n    } catch (e) {\n      print('[BLE] MTU negotiation failed: $e');\n    }\n\n    // Step 3: Subscribe to sensor data characteristic\n    final characteristic = QualifiedCharacteristic(\n      serviceId: Uuid.parse('0000180f-0000-1000-8000-00805f9b34fb'),\n      characteristicId: Uuid.parse('00002A58-0000-1000-8000-00805f9b34fb'),\n      deviceId: deviceId,\n    );\n\n    _sensorDataSubscription = _ble.subscribeToCharacteristic(characteristic).listen(\n      _onSensorData,\n      onError: (error) =&gt; print('[BLE] Sensor data error: $error'),\n    );\n\n    // Step 4: Synchronize timestamps\n    await _synchronizeTimestamps();\n\n    // Step 5: Start sensor streaming\n    await _sendControlCommand(DeviceCommand.START_STREAM);\n  }\n\n  void _onSensorData(List&lt;int&gt; rawData) {\n    try {\n      // Decode Protobuf\n      final packet = SensorPacket.fromBuffer(rawData);\n\n      // Detect packet loss\n      if (_lastSequenceId != null &amp;&amp; packet.sequenceId != _lastSequenceId + 1) {\n        final lostPackets = packet.sequenceId - _lastSequenceId - 1;\n        print('[BLE] Packet loss detected: $lostPackets packets');\n        _metrics.blePacketLoss += lostPackets;\n      }\n      _lastSequenceId = packet.sequenceId;\n\n      // Convert firmware timestamp to app timebase\n      packet.alignedTimestamp = _convertTimestamp(packet.firmwareTimestamp);\n\n      // Emit to stream\n      _sensorDataController.add(packet);\n\n    } catch (e) {\n      print('[BLE] Failed to decode packet: $e');\n    }\n  }\n\n  Future&lt;void&gt; _synchronizeTimestamps() async {\n    final syncCharacteristic = QualifiedCharacteristic(\n      serviceId: Uuid.parse('0000180f-0000-1000-8000-00805f9b34fb'),\n      characteristicId: Uuid.parse('00002A5B-0000-1000-8000-00805f9b34fb'),\n      deviceId: deviceId,\n    );\n\n    // Wait for one-time sync packet\n    final syncData = await _ble.subscribeToCharacteristic(syncCharacteristic).first;\n\n    final firmwareMillis = ByteData.view(Uint8List.fromList(syncData).buffer).getUint32(0);\n    final appTimestamp = DateTime.now().millisecondsSinceEpoch;\n\n    _timestampOffset = appTimestamp - firmwareMillis;\n    print('[BLE] Timestamp offset calibrated: $_timestampOffset ms');\n  }\n\n  int _convertTimestamp(int firmwareMillis) {\n    return firmwareMillis + _timestampOffset;\n  }\n\n  Future&lt;void&gt; sendHapticCommand(HapticPattern pattern) async {\n    final hapticCharacteristic = QualifiedCharacteristic(\n      serviceId: Uuid.parse('0000180f-0000-1000-8000-00805f9b34fb'),\n      characteristicId: Uuid.parse('00002A59-0000-1000-8000-00805f9b34fb'),\n      deviceId: deviceId,\n    );\n\n    final command = [\n      pattern.id,           // Pattern ID (e.g., 0x01 = short pulse, 0x02 = double pulse)\n      pattern.intensity,    // 0-255\n      pattern.durationMs ~/ 10,  // Duration in 10ms units (max 2.55 seconds)\n    ];\n\n    await _ble.writeCharacteristicWithoutResponse(\n      hapticCharacteristic,\n      value: command,\n    );\n  }\n\n  void dispose() {\n    _sensorDataSubscription?.cancel();\n    _connectionSubscription?.cancel();\n    _sensorDataController.close();\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#23-backpressure-handling","title":"2.3 Backpressure Handling","text":"<p>Problem: Camera processing (126ms) slower than BLE packets (10ms interval) - Buffer overflow if packets accumulate faster than consumption</p> <p>Solution: Circular buffer with overflow strategy</p> <pre><code>class SensorDataBuffer {\n  final int maxSize = 50;  // 500ms of data at 100 Hz\n  final Queue&lt;SensorPacket&gt; _buffer = Queue();\n  int _droppedPackets = 0;\n\n  void add(SensorPacket packet) {\n    if (_buffer.length &gt;= maxSize) {\n      // Drop oldest packet\n      _buffer.removeFirst();\n      _droppedPackets++;\n    }\n    _buffer.add(packet);\n  }\n\n  List&lt;SensorPacket&gt; getWindow(int startTimestamp, int endTimestamp) {\n    return _buffer.where((p) =&gt;\n      p.alignedTimestamp &gt;= startTimestamp &amp;&amp;\n      p.alignedTimestamp &lt;= endTimestamp\n    ).toList();\n  }\n\n  void evictOlderThan(int timestamp) {\n    _buffer.removeWhere((p) =&gt; p.alignedTimestamp &lt; timestamp - 1000);  // Keep last 1 second\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#3-ml-model-integration","title":"3. ML Model Integration","text":""},{"location":"architecture/hld/03-integration-patterns/#31-model-loading-pipeline","title":"3.1 Model Loading Pipeline","text":"<p>Model Assets Structure: <pre><code>assets/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 rtmpose_m_256x192.tflite        # Pose estimation (FP16, 8.2 MB)\n\u2502   \u251c\u2500\u2500 movement_classifier.tflite      # LSTM+Transformer (INT8, 4.5 MB)\n\u2502   \u2514\u2500\u2500 error_types.json                # Label mapping\n</code></pre></p> <p>Initialization Code:</p> <pre><code>class MLPipeline {\n  late Interpreter _poseInterpreter;\n  late Interpreter _classifierInterpreter;\n  Map&lt;int, String&gt; _errorLabels = {};\n\n  Future&lt;void&gt; initialize() async {\n    // Load pose estimation model\n    final poseModelData = await _loadModelFromAssets('models/rtmpose_m_256x192.tflite');\n    _poseInterpreter = await Interpreter.fromBuffer(poseModelData, options: InterpreterOptions()\n      ..threads = 4  // Utilize multi-core CPU\n      ..useNnApiForAndroid = true  // Enable Android NNAPI acceleration\n      ..useMetalDelegate = true);  // Enable iOS Metal acceleration\n\n    // Load movement classifier\n    final classifierData = await _loadModelFromAssets('models/movement_classifier.tflite');\n    _classifierInterpreter = await Interpreter.fromBuffer(classifierData, options: InterpreterOptions()\n      ..threads = 2);\n\n    // Load error labels\n    final labelsJson = await rootBundle.loadString('assets/models/error_types.json');\n    final labelsMap = json.decode(labelsJson) as Map&lt;String, dynamic&gt;;\n    _errorLabels = labelsMap.map((k, v) =&gt; MapEntry(int.parse(k), v as String));\n\n    print('[ML] Models loaded successfully');\n    print('[ML] Pose model: ${_poseInterpreter.getInputTensors()[0].shape}');\n    print('[ML] Classifier: ${_classifierInterpreter.getInputTensors()[0].shape}');\n  }\n\n  Future&lt;ByteBuffer&gt; _loadModelFromAssets(String path) async {\n    final rawAssetFile = await rootBundle.load('assets/$path');\n    return rawAssetFile.buffer;\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#32-pose-estimation-inference","title":"3.2 Pose Estimation Inference","text":"<p>Input Preprocessing: <pre><code>class PoseEstimator {\n  static const INPUT_SIZE = 256;  // Model input: 256\u00d7192\n  static const INPUT_HEIGHT = 192;\n\n  Future&lt;PoseEstimationResult&gt; estimatePose(CameraImage image) async {\n    final stopwatch = Stopwatch()..start();\n\n    // Step 1: Convert YUV to RGB (if needed)\n    final rgbImage = _convertToRGB(image);\n\n    // Step 2: Resize and normalize\n    final inputTensor = _preprocessImage(rgbImage);\n\n    // Step 3: Run inference\n    final outputTensor = List.filled(1 * 17 * 3, 0.0).reshape([1, 17, 3]);\n    _poseInterpreter.run(inputTensor, outputTensor);\n\n    // Step 4: Post-process keypoints\n    final keypoints = _extractKeypoints(outputTensor, image.width, image.height);\n\n    stopwatch.stop();\n\n    return PoseEstimationResult(\n      timestamp: DateTime.now(),\n      keypoints: keypoints,\n      inferenceTimeMs: stopwatch.elapsedMilliseconds.toDouble(),\n    );\n  }\n\n  Float32List _preprocessImage(img.Image rgbImage) {\n    // Resize to 256\u00d7192\n    final resized = img.copyResize(rgbImage, width: INPUT_SIZE, height: INPUT_HEIGHT);\n\n    // Normalize to [0, 1] and convert to CHW format (channels-first)\n    final buffer = Float32List(1 * 3 * INPUT_HEIGHT * INPUT_SIZE);\n    int pixelIndex = 0;\n\n    for (int c = 0; c &lt; 3; c++) {  // RGB channels\n      for (int y = 0; y &lt; INPUT_HEIGHT; y++) {\n        for (int x = 0; x &lt; INPUT_SIZE; x++) {\n          final pixel = resized.getPixel(x, y);\n          double value;\n          if (c == 0) value = pixel.r / 255.0;\n          else if (c == 1) value = pixel.g / 255.0;\n          else value = pixel.b / 255.0;\n\n          buffer[pixelIndex++] = value;\n        }\n      }\n    }\n\n    return buffer;\n  }\n\n  List&lt;Keypoint&gt; _extractKeypoints(List outputTensor, int imgWidth, int imgHeight) {\n    final keypoints = &lt;Keypoint&gt;[];\n\n    for (int i = 0; i &lt; 17; i++) {\n      // RTMPose output: [x, y, confidence] per keypoint (normalized to input size)\n      final x = outputTensor[0][i][0] / INPUT_SIZE;   // Normalized [0, 1]\n      final y = outputTensor[0][i][1] / INPUT_HEIGHT;\n      final confidence = outputTensor[0][i][2];\n\n      keypoints.add(Keypoint(x: x, y: y, confidence: confidence));\n    }\n\n    return keypoints;\n  }\n}\n</code></pre></p>"},{"location":"architecture/hld/03-integration-patterns/#33-sensor-fusion-and-feature-extraction","title":"3.3 Sensor Fusion and Feature Extraction","text":"<pre><code>class SensorFusion {\n  // Sliding window: 30 frames \u00d7 35 features\n  final windowSize = 30;\n  final featureSize = 35;\n  final Queue&lt;FusedFeatures&gt; _featureWindow = Queue();\n\n  FusedFeatures fuseData(\n    PoseEstimationResult pose,\n    List&lt;SensorPacket&gt; imuWindow,\n    List&lt;SensorPacket&gt; emgWindow,\n  ) {\n    // Extract pose features (17 values)\n    final poseFeatures = _extractPoseFeatures(pose.keypoints);\n\n    // Extract IMU features (10 values)\n    final imuFeatures = _extractIMUFeatures(imuWindow);\n\n    // Extract EMG features (8 values)\n    final emgFeatures = _extractEMGFeatures(emgWindow);\n\n    return FusedFeatures(\n      jointAngles: poseFeatures.jointAngles,\n      jointVelocities: poseFeatures.jointVelocities,\n      orientation: imuFeatures.orientation,\n      linearAccel: imuFeatures.linearAccel,\n      angularVel: imuFeatures.angularVel,\n      rmsAmplitude: emgFeatures.rmsAmplitude,\n      bandPower: emgFeatures.bandPower,\n    );\n  }\n\n  PoseFeatures _extractPoseFeatures(List&lt;Keypoint&gt; keypoints) {\n    // Calculate joint angles using law of cosines\n    final leftElbowAngle = _calculateAngle(\n      keypoints[5],  // LEFT_SHOULDER\n      keypoints[7],  // LEFT_ELBOW\n      keypoints[9],  // LEFT_WRIST\n    );\n\n    final rightElbowAngle = _calculateAngle(\n      keypoints[6], keypoints[8], keypoints[10],\n    );\n\n    // Estimate velocities using finite differences\n    final leftElbowVelocity = _estimateVelocity(\n      _previousPose?.keypoints[7],\n      keypoints[7],\n      timestampDelta: 16.67,  // ms between frames at 60 FPS\n    );\n\n    // ... (calculate for all 8 joints)\n\n    return PoseFeatures(\n      jointAngles: [leftElbowAngle, rightElbowAngle, /* ... */],\n      jointVelocities: [leftElbowVelocity, /* ... */],\n    );\n  }\n\n  double _calculateAngle(Keypoint p1, Keypoint p2, Keypoint p3) {\n    // Vectors: p2-&gt;p1 and p2-&gt;p3\n    final v1 = Vector2(p1.x - p2.x, p1.y - p2.y);\n    final v2 = Vector2(p3.x - p2.x, p3.y - p2.y);\n\n    // Dot product / magnitudes\n    final cosAngle = v1.dot(v2) / (v1.length * v2.length);\n    return math.acos(cosAngle.clamp(-1.0, 1.0)) * 180 / math.pi;  // degrees\n  }\n\n  IMUFeatures _extractIMUFeatures(List&lt;SensorPacket&gt; window) {\n    // Average IMU samples in window\n    final avgAccel = Vector3.zero();\n    final avgGyro = Vector3.zero();\n\n    for (final packet in window) {\n      for (final sample in packet.imuSamples) {\n        avgAccel += Vector3(\n          sample.accelXMg / 1000.0,  // Convert mg to g\n          sample.accelYMg / 1000.0,\n          sample.accelZMg / 1000.0,\n        );\n        avgGyro += Vector3(\n          sample.gyroXMdps / 1000.0,  // Convert mdps to dps\n          sample.gyroYMdps / 1000.0,\n          sample.gyroZMdps / 1000.0,\n        );\n      }\n    }\n\n    final sampleCount = window.fold&lt;int&gt;(0, (sum, p) =&gt; sum + p.imuSamples.length);\n    avgAccel /= sampleCount;\n    avgGyro /= sampleCount;\n\n    // Estimate orientation (simplified: assume gravity = [0, 0, -1g])\n    final orientation = _estimateOrientation(avgAccel);\n\n    // Remove gravity component from acceleration\n    final linearAccel = avgAccel - orientation.rotateVector(Vector3(0, 0, -1));\n\n    return IMUFeatures(\n      orientation: orientation,\n      linearAccel: linearAccel,\n      angularVel: avgGyro,\n    );\n  }\n\n  EMGFeatures _extractEMGFeatures(List&lt;SensorPacket&gt; window) {\n    // Calculate RMS amplitude per channel\n    final rmsAmplitude = List.filled(4, 0.0);\n    final bandPower = List.filled(4, 0.0);\n\n    for (int ch = 0; ch &lt; 4; ch++) {\n      final samples = &lt;double&gt;[];\n      for (final packet in window) {\n        for (final sample in packet.emgSamples) {\n          samples.add(sample.channels[ch].toDouble());\n        }\n      }\n\n      // RMS\n      rmsAmplitude[ch] = math.sqrt(\n        samples.map((s) =&gt; s * s).reduce((a, b) =&gt; a + b) / samples.length\n      );\n\n      // Band power (50-150 Hz): Apply FFT and sum power in band\n      bandPower[ch] = _calculateBandPower(samples, sampleRate: 200, lowFreq: 50, highFreq: 150);\n    }\n\n    return EMGFeatures(\n      rmsAmplitude: rmsAmplitude,\n      bandPower: bandPower,\n    );\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#34-movement-classification-inference","title":"3.4 Movement Classification Inference","text":"<pre><code>class MovementClassifier {\n  final int windowSize = 30;\n  final int featureSize = 35;\n\n  Future&lt;ClassificationResult&gt; classify(List&lt;FusedFeatures&gt; featureWindow) async {\n    if (featureWindow.length &lt; windowSize) {\n      return ClassificationResult.insufficient();\n    }\n\n    // Step 1: Convert to flat tensor [1, 30, 35]\n    final inputTensor = _featuresToTensor(featureWindow);\n\n    // Step 2: Run inference\n    final outputTensor = List.filled(1 * 10, 0.0).reshape([1, 10]);  // 10 error classes\n    _classifierInterpreter.run(inputTensor, outputTensor);\n\n    // Step 3: Apply softmax and find top prediction\n    final probabilities = _softmax(outputTensor[0]);\n    final topIndex = probabilities.indexOf(probabilities.reduce(math.max));\n    final topConfidence = probabilities[topIndex];\n\n    return ClassificationResult(\n      errorType: _errorLabels[topIndex] ?? 'UNKNOWN',\n      confidence: topConfidence,\n      allProbabilities: probabilities,\n    );\n  }\n\n  Float32List _featuresToTensor(List&lt;FusedFeatures&gt; window) {\n    final buffer = Float32List(1 * windowSize * featureSize);\n    int idx = 0;\n\n    for (final features in window.take(windowSize)) {\n      // Flatten all features\n      buffer.setAll(idx, features.jointAngles); idx += 8;\n      buffer.setAll(idx, features.jointVelocities); idx += 8;\n      buffer[idx++] = features.torsoOrientation;\n      buffer.setAll(idx, features.orientation.toList()); idx += 4;\n      buffer.setAll(idx, features.linearAccel.toList()); idx += 3;\n      buffer.setAll(idx, features.angularVel.toList()); idx += 3;\n      buffer.setAll(idx, features.rmsAmplitude); idx += 4;\n      buffer.setAll(idx, features.bandPower); idx += 4;\n    }\n\n    return buffer;\n  }\n\n  List&lt;double&gt; _softmax(List&lt;double&gt; logits) {\n    final expSum = logits.map((x) =&gt; math.exp(x)).reduce((a, b) =&gt; a + b);\n    return logits.map((x) =&gt; math.exp(x) / expSum).toList();\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#4-haptic-feedback-integration","title":"4. Haptic Feedback Integration","text":""},{"location":"architecture/hld/03-integration-patterns/#41-triggering-logic","title":"4.1 Triggering Logic","text":"<pre><code>class HapticFeedbackController {\n  final BLEDeviceManager bleManager;\n  DateTime? _lastHapticTime;\n  final minHapticInterval = Duration(milliseconds: 500);  // Avoid haptic spam\n\n  Future&lt;void&gt; onClassificationResult(ClassificationResult result) async {\n    // Only trigger on high-confidence errors\n    if (result.confidence &lt; 0.7 || result.errorType == 'NO_ERROR') {\n      return;\n    }\n\n    // Rate limiting\n    final now = DateTime.now();\n    if (_lastHapticTime != null &amp;&amp; now.difference(_lastHapticTime!) &lt; minHapticInterval) {\n      return;\n    }\n\n    // Select haptic pattern based on error severity\n    final pattern = _selectHapticPattern(result.errorType, result.confidence);\n\n    // Send command to firmware\n    await bleManager.sendHapticCommand(pattern);\n    _lastHapticTime = now;\n\n    // Log for analytics\n    print('[Haptic] Triggered: ${result.errorType} (confidence: ${result.confidence})');\n  }\n\n  HapticPattern _selectHapticPattern(String errorType, double confidence) {\n    // Critical errors (e.g., dangerous form)\n    if (errorType.contains('CRITICAL') || confidence &gt; 0.9) {\n      return HapticPattern(\n        id: 0x03,          // Triple pulse\n        intensity: 255,\n        durationMs: 300,\n      );\n    }\n\n    // Moderate errors\n    if (confidence &gt; 0.8) {\n      return HapticPattern(\n        id: 0x02,          // Double pulse\n        intensity: 200,\n        durationMs: 200,\n      );\n    }\n\n    // Minor corrections\n    return HapticPattern(\n      id: 0x01,            // Single pulse\n      intensity: 150,\n      durationMs: 100,\n    );\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#42-firmware-haptic-driver","title":"4.2 Firmware Haptic Driver","text":"<pre><code>// Pseudocode: Firmware haptic motor control\ntypedef struct {\n    uint8_t pattern_id;\n    uint8_t intensity;\n    uint16_t duration_ms;\n} HapticCommand;\n\nvoid on_haptic_write(const uint8_t* data, size_t len) {\n    HapticCommand cmd;\n    cmd.pattern_id = data[0];\n    cmd.intensity = data[1];\n    cmd.duration_ms = data[2] * 10;  // Convert to milliseconds\n\n    execute_haptic_pattern(cmd);\n}\n\nvoid execute_haptic_pattern(HapticCommand cmd) {\n    switch (cmd.pattern_id) {\n        case 0x01:  // Single pulse\n            set_motor_pwm(cmd.intensity);\n            delay_ms(cmd.duration_ms);\n            set_motor_pwm(0);\n            break;\n\n        case 0x02:  // Double pulse\n            for (int i = 0; i &lt; 2; i++) {\n                set_motor_pwm(cmd.intensity);\n                delay_ms(cmd.duration_ms / 3);\n                set_motor_pwm(0);\n                delay_ms(cmd.duration_ms / 6);\n            }\n            break;\n\n        case 0x03:  // Triple pulse (critical alert)\n            for (int i = 0; i &lt; 3; i++) {\n                set_motor_pwm(cmd.intensity);\n                delay_ms(cmd.duration_ms / 5);\n                set_motor_pwm(0);\n                delay_ms(cmd.duration_ms / 10);\n            }\n            break;\n    }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#5-error-handling-and-retry-strategies","title":"5. Error Handling and Retry Strategies","text":""},{"location":"architecture/hld/03-integration-patterns/#51-ble-error-recovery","title":"5.1 BLE Error Recovery","text":"<pre><code>class RobustBLEManager extends BLEDeviceManager {\n  int _reconnectAttempts = 0;\n  static const maxReconnectAttempts = 3;\n  static const reconnectDelay = Duration(seconds: 2);\n\n  @override\n  Future&lt;void&gt; _onDisconnected(GenericFailure&lt;ConnectionError&gt;? failure) async {\n    print('[BLE] Disconnected: ${failure?.code}');\n\n    if (_reconnectAttempts &lt; maxReconnectAttempts) {\n      _reconnectAttempts++;\n      print('[BLE] Reconnection attempt $_reconnectAttempts/$maxReconnectAttempts');\n\n      await Future.delayed(reconnectDelay);\n      await connect();\n    } else {\n      print('[BLE] Max reconnect attempts reached, showing user prompt');\n      _showReconnectionDialog();\n      _reconnectAttempts = 0;\n    }\n  }\n\n  @override\n  void _onSensorData(List&lt;int&gt; rawData) {\n    try {\n      super._onSensorData(rawData);\n      _consecutiveErrors = 0;  // Reset error counter on success\n    } catch (e) {\n      _consecutiveErrors++;\n\n      if (_consecutiveErrors &gt; 10) {\n        print('[BLE] Too many decoding errors, resetting connection');\n        disconnect();\n        connect();\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#52-ml-inference-fallback","title":"5.2 ML Inference Fallback","text":"<pre><code>class ResilientMLPipeline {\n  int _consecutiveFailures = 0;\n  static const maxFailures = 5;\n\n  Future&lt;ClassificationResult&gt; classifyWithFallback(List&lt;FusedFeatures&gt; features) async {\n    try {\n      final result = await _classifier.classify(features);\n      _consecutiveFailures = 0;\n      return result;\n\n    } catch (e) {\n      _consecutiveFailures++;\n      print('[ML] Inference failed: $e');\n\n      if (_consecutiveFailures &gt; maxFailures) {\n        print('[ML] Too many failures, reloading model');\n        await _classifier.initialize();\n        _consecutiveFailures = 0;\n      }\n\n      // Return safe default (no error detected)\n      return ClassificationResult(\n        errorType: 'NO_ERROR',\n        confidence: 0.0,\n        allProbabilities: List.filled(10, 0.0),\n      );\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#53-graceful-degradation-matrix","title":"5.3 Graceful Degradation Matrix","text":"Failure Scenario Detection Method Degradation Strategy Recovery BLE disconnection No packets for 100ms Pause inference, show \"Reconnecting...\" Auto-reconnect (3 attempts) Low pose confidence (&lt;0.3) Mean keypoint confidence Skip frame, show \"Move closer\" Resume when confidence recovers High latency (&gt;150ms) Timestamp delta tracking Drop to 30 FPS Resume 60 FPS when latency improves ML inference timeout Future timeout (500ms) Skip classification, keep pose Reload model after 5 failures Memory pressure System memory warning Reduce buffer size (50\u219225 frames) Restore after pressure relieved"},{"location":"architecture/hld/03-integration-patterns/#6-performance-optimization-patterns","title":"6. Performance Optimization Patterns","text":""},{"location":"architecture/hld/03-integration-patterns/#61-lazy-initialization","title":"6.1 Lazy Initialization","text":"<pre><code>class LazyMLPipeline {\n  Interpreter? _poseInterpreter;\n  Interpreter? _classifierInterpreter;\n\n  Future&lt;Interpreter&gt; get poseInterpreter async {\n    _poseInterpreter ??= await _loadPoseModel();\n    return _poseInterpreter!;\n  }\n\n  // Only load classifier when first classification is requested\n  Future&lt;Interpreter&gt; get classifierInterpreter async {\n    _classifierInterpreter ??= await _loadClassifierModel();\n    return _classifierInterpreter!;\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#62-object-pooling-reduce-gc-pressure","title":"6.2 Object Pooling (Reduce GC Pressure)","text":"<pre><code>class TensorPool {\n  final Queue&lt;Float32List&gt; _pool = Queue();\n  final int tensorSize;\n\n  TensorPool(this.tensorSize);\n\n  Float32List acquire() {\n    if (_pool.isEmpty) {\n      return Float32List(tensorSize);\n    }\n    return _pool.removeFirst();\n  }\n\n  void release(Float32List tensor) {\n    if (_pool.length &lt; 10) {  // Max pool size\n      _pool.add(tensor);\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#63-parallel-processing","title":"6.3 Parallel Processing","text":"<pre><code>class ParallelPipeline {\n  Future&lt;AnalysisResult&gt; processFrame(CameraImage image, int timestamp) async {\n    // Run pose estimation and sensor retrieval in parallel\n    final results = await Future.wait([\n      _poseEstimator.estimatePose(image),\n      _sensorBuffer.getWindow(timestamp - 50, timestamp),\n    ]);\n\n    final pose = results[0] as PoseEstimationResult;\n    final sensorData = results[1] as List&lt;SensorPacket&gt;;\n\n    // Continue with fusion and classification\n    final features = _fusion.fuseData(pose, sensorData.imu, sensorData.emg);\n    final classification = await _classifier.classify(features);\n\n    return AnalysisResult(pose: pose, classification: classification);\n  }\n}\n</code></pre>"},{"location":"architecture/hld/03-integration-patterns/#summary","title":"Summary","text":"<p>The integration patterns establish:</p> <ol> <li>BLE Protocol: 500-byte MTU, 7.5ms intervals, Protobuf encoding \u2192 100 Hz throughput</li> <li>Flutter Integration: <code>flutter_reactive_ble</code> streams, timestamp synchronization, backpressure handling</li> <li>ML Pipeline: TFLite models (FP16 pose + INT8 classifier), feature fusion, sliding window inference</li> <li>Haptic Feedback: Confidence-based triggering (&gt;0.7), rate limiting (500ms), multi-pattern support</li> <li>Error Handling: Auto-reconnect (3 attempts), graceful degradation, fallback strategies</li> </ol> <p>Key Performance Optimizations: - Lazy initialization (save 200ms startup time) - Object pooling (reduce GC pauses) - Parallel processing (save 20ms per frame)</p> <p>Reliability Features: - Sequence IDs detect 0.5% packet loss - Timestamp sync handles clock drift (&lt;50 ppm) - Automatic model reloading after persistent failures</p>"},{"location":"architecture/hld/04-performance-targets/","title":"Performance Targets","text":""},{"location":"architecture/hld/04-performance-targets/#overview","title":"Overview","text":"<p>This document defines the performance requirements, benchmarking methodology, and optimization strategies for Movement Chain AI. The system must achieve real-time movement analysis while maintaining low power consumption and high accuracy on mobile devices.</p>"},{"location":"architecture/hld/04-performance-targets/#1-latency-requirements","title":"1. Latency Requirements","text":""},{"location":"architecture/hld/04-performance-targets/#11-real-time-feedback-path","title":"1.1 Real-Time Feedback Path","text":"<p>Target: End-to-end latency &lt;100ms (P95)</p> Stage Budget Current Status Critical Path Camera acquisition 16.7ms 16.7ms \u2705 OK Hardware-limited (60 FPS) Frame preprocessing 5ms 8ms \u26a0\ufe0f Warning CPU-bound (resize + YUV\u2192RGB) Pose estimation 30ms 45ms \u274c Over GPU inference bottleneck Sensor retrieval 2ms 3ms \u2705 OK Memory access + buffer scan Sensor fusion 10ms 8ms \u2705 OK Feature extraction (FFT for EMG) ML classification 40ms 50ms \u274c Over LSTM+Transformer inference Haptic feedback 5ms 3ms \u2705 OK BLE write latency Total 100ms 126ms \u274c 26ms over Optimize pose + ML <p>Latency Breakdown (P50/P95/P99):</p> <pre><code>Camera:      16.7ms / 16.7ms / 33.4ms  (occasional frame drop)\nPreprocessing: 5ms / 8ms / 12ms\nPose:        28ms / 45ms / 60ms        \u2190 HIGH VARIANCE\nFusion:       7ms / 12ms / 15ms\nML:          35ms / 50ms / 65ms        \u2190 HIGH VARIANCE\nFeedback:     2ms / 3ms / 5ms\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal:       93.7ms / 134.7ms / 190.4ms\n</code></pre> <p>Impact Analysis: - P50 latency (93.7ms) meets target \u2192 good experience for most frames - P95 latency (134.7ms) exceeds target by 34.7ms \u2192 noticeable lag during complex poses - P99 latency (190.4ms) is unacceptable \u2192 feels sluggish to user</p>"},{"location":"architecture/hld/04-performance-targets/#12-post-action-analysis","title":"1.2 Post-Action Analysis","text":"<p>Target: Generate summary within 5 seconds of workout completion</p> Task Budget Description Data aggregation 1s Load all frames from SQLite (3,600 frames @ 60 FPS/min) Batch inference 3s Re-run ML on all frames with FP32 model (higher accuracy) Statistics generation 0.5s Calculate rep counts, error frequencies, joint ROM Visualization rendering 0.5s Generate charts (error timeline, pose heatmap) Total 5s User sees analysis screen immediately"},{"location":"architecture/hld/04-performance-targets/#13-startup-latency","title":"1.3 Startup Latency","text":"<p>Target: App ready for workout in &lt;3 seconds</p> Task Budget Current Optimization Flutter framework init 500ms 500ms Non-optimizable ML model loading 1.5s 2.1s Use model caching, lazy load classifier BLE device scan 2s 2s Background scan during model load Camera initialization 500ms 600ms Reduce resolution until pose ready Total 3s 4.2s Target: 3s via parallelization <p>Cold Start Optimization: <pre><code>Future&lt;void&gt; parallelInitialization() async {\n  final results = await Future.wait([\n    _mlPipeline.loadPoseModel(),      // 1.2s (blocking for inference)\n    _bleManager.startScan(),           // 2s (can start workout without device)\n    _cameraManager.initialize(),       // 0.6s (blocking for video)\n    // Defer classifier loading until first inference needed\n  ]);\n\n  // Total: max(1.2s, 2s, 0.6s) = 2s + 0.5s Flutter = 2.5s\n  // Classifier loads in background (adds 0.9s) while user positions camera\n}\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#2-throughput-requirements","title":"2. Throughput Requirements","text":""},{"location":"architecture/hld/04-performance-targets/#21-video-processing","title":"2.1 Video Processing","text":"Metric Target Current Validation Method Camera frame rate 60 FPS (stable) 55-60 FPS Monitor frame timestamps, detect drops &gt;1% Frame drop rate &lt;1% 2.3% Count frames with \u0394t &gt;20ms Pose inference rate \u226530 FPS 22 FPS Inference counter / elapsed time ML classification rate \u226530 FPS 20 FPS Classifier invocations / second <p>Frame Drop Causes: 1. Pose inference too slow (45ms) \u2192 can't keep up with 60 FPS 2. GC pauses (15-30ms) \u2192 Flutter runtime stops processing 3. Thermal throttling (sustained load &gt;2 min) \u2192 CPU/GPU frequency reduced</p> <p>Mitigation: - Adaptive frame rate: Drop to 30 FPS if latency exceeds 120ms for 3 consecutive frames - Reduce GC pressure: Object pooling for tensors (see Integration Patterns) - Thermal management: Reduce camera resolution to 480p after 5 minutes</p>"},{"location":"architecture/hld/04-performance-targets/#22-ble-sensor-streaming","title":"2.2 BLE Sensor Streaming","text":"Sensor Target Rate Packet Rate Data Rate Validation IMU (6-axis) 100 Hz 10 packets/s (10 samples each) 2.4 KB/s Verify 100 samples/second \u00b12% EMG (4-channel) 200 Hz 5 packets/s (40 samples each) 1.6 KB/s Verify 200 samples/second \u00b12% Total BLE - 15 packets/s 4 KB/s Packet loss &lt;0.5% <p>Throughput Validation Test: <pre><code>void validateBLEThroughput() async {\n  final startTime = DateTime.now();\n  int imuSampleCount = 0;\n  int emgSampleCount = 0;\n\n  // Collect for 60 seconds\n  await for (final packet in bleManager.sensorDataStream.take(1500)) {\n    imuSampleCount += packet.imuSamples.length;\n    emgSampleCount += packet.emgSamples.length;\n  }\n\n  final duration = DateTime.now().difference(startTime).inSeconds;\n  final imuRate = imuSampleCount / duration;\n  final emgRate = emgSampleCount / duration;\n\n  print('IMU rate: $imuRate Hz (target: 100 Hz)');\n  print('EMG rate: $emgRate Hz (target: 200 Hz)');\n\n  assert(imuRate &gt;= 98 &amp;&amp; imuRate &lt;= 102, 'IMU rate out of spec');\n  assert(emgRate &gt;= 196 &amp;&amp; emgRate &lt;= 204, 'EMG rate out of spec');\n}\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#23-storage-throughput","title":"2.3 Storage Throughput","text":"Operation Target Current Impact Frame write (SQLite) &gt;60 writes/s 72 writes/s Background recording while inference runs Batch read (post-workout) &lt;2s for 3,600 frames 1.8s Fast enough for 5s analysis budget Database size (1-hour session) &lt;200 MB 180 MB (compressed) Acceptable for weekly cleanup"},{"location":"architecture/hld/04-performance-targets/#3-resource-budgets","title":"3. Resource Budgets","text":""},{"location":"architecture/hld/04-performance-targets/#31-memory-constraints","title":"3.1 Memory Constraints","text":"<p>Target: Peak RAM usage &lt;500 MB</p> Component Budget Current Notes Flutter framework 80 MB 85 MB Baseline, non-optimizable Camera buffers 50 MB 45 MB 3 frames @ 640\u00d7480 RGB (triple buffer) ML models (loaded) 150 MB 180 MB Pose (8.2 MB) + Classifier (4.5 MB) + runtime overhead Sensor data buffers 5 MB 4 MB Circular buffer (500ms @ 100/200 Hz) Tensor workspace 30 MB 25 MB Intermediate activations during inference SQLite (if recording) 100 MB 90 MB Write buffer + index cache UI rendering 50 MB 45 MB Dart objects + texture cache Total 500 MB 474 MB \u2705 Within budget <p>Memory Profiling Strategy: <pre><code>void profileMemory() {\n  // Trigger GC to get accurate baseline\n  System.gc();\n\n  final baseline = ProcessInfo.currentRss;\n  print('Baseline memory: ${baseline / 1024 / 1024} MB');\n\n  // Load models\n  mlPipeline.initialize();\n  final afterModels = ProcessInfo.currentRss;\n  print('After models: ${(afterModels - baseline) / 1024 / 1024} MB');\n\n  // Start camera\n  cameraManager.startPreview();\n  final afterCamera = ProcessInfo.currentRss;\n  print('After camera: ${(afterCamera - afterModels) / 1024 / 1024} MB');\n\n  // Run 60-second workout\n  runWorkout(duration: Duration(seconds: 60));\n  final afterWorkout = ProcessInfo.currentRss;\n  print('Peak during workout: ${(afterWorkout - baseline) / 1024 / 1024} MB');\n}\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#32-storage-budget","title":"3.2 Storage Budget","text":"<p>Target: App binary &lt;150 MB, user data &lt;500 MB/month</p> Asset Size Compression Installed Size Flutter framework 25 MB - 25 MB Pose model (FP16) 8.2 MB None (already optimized) 8.2 MB Classifier model (INT8) 4.5 MB None 4.5 MB UI assets (icons, fonts) 5 MB PNG compression 5 MB Native libraries (TFLite) 15 MB Stripped symbols 15 MB Total APK/IPA 58 MB - 58 MB \u2705 <p>User Data Growth: - 1-hour workout: 180 MB (compressed) - Assumed usage: 5 workouts/week = 3.6 GB/month - Solution: Auto-delete sessions older than 30 days (keep summary stats only)</p>"},{"location":"architecture/hld/04-performance-targets/#33-power-consumption","title":"3.3 Power Consumption","text":"<p>Target: &lt;15% battery drain per hour</p> Component Power Draw % of Total Optimization Camera (60 FPS) 450 mW 35% Reduce to 30 FPS if latency issues GPU (pose inference) 600 mW 46% Quantization (FP16\u2192INT8) can save 30% CPU (feature extraction) 150 mW 12% SIMD optimization for FFT BLE radio 50 mW 4% Already optimized (7.5ms interval) Display (always-on) 50 mW 4% Dim during workout, brighten for feedback Total 1,300 mW 100% ~13% battery/hour on 10,000 mAh \u2705 <p>Battery Profiling: <pre><code># Android\nadb shell dumpsys batterystats --reset\n# Run 1-hour workout\nadb shell dumpsys batterystats &gt; battery_stats.txt\n\n# Analyze power consumption by component\nbattery-historian -port 9999 battery_stats.txt\n</code></pre></p> <p>Power Optimization Strategies: 1. Dynamic FPS: 60 FPS when stationary, 30 FPS during rapid movement (GPU can't keep up anyway) 2. Background mode: Pause camera when app backgrounded, keep BLE streaming (audio cues only) 3. Screen timeout: Dim screen to 20% after 30 seconds of no user interaction</p>"},{"location":"architecture/hld/04-performance-targets/#4-accuracy-targets","title":"4. Accuracy Targets","text":""},{"location":"architecture/hld/04-performance-targets/#41-pose-estimation-accuracy","title":"4.1 Pose Estimation Accuracy","text":"<p>Target: Average Precision (AP) &gt;70% at OKS threshold 0.5</p> Metric Target Current Benchmark Dataset AP @ OKS=0.5 &gt;70% 73.2% COCO val2017 (5,000 images) AP @ OKS=0.75 &gt;50% 48.1% Stricter localization AR (Average Recall) &gt;75% 76.5% Detect all visible keypoints <p>OKS (Object Keypoint Similarity): Measures pose accuracy, normalized by person scale <pre><code>OKS = \u03a3 exp(-d_i\u00b2 / (2 * s\u00b2 * k_i\u00b2)) / \u03a3 \u03b4(v_i &gt; 0)\n\nwhere:\n  d_i = Euclidean distance between predicted and ground truth keypoint i\n  s = \u221a(area of person bounding box)  (scale normalization)\n  k_i = keypoint constant (higher for easier keypoints like shoulders)\n  v_i = visibility flag\n</code></pre></p> <p>Keypoint-Specific Accuracy (PCK @ 0.1 threshold): | Keypoint | Target | Current | Notes | |----------|--------|---------|-------| | Shoulders, hips | &gt;90% | 91% | Easy: large, high-contrast | | Elbows, knees | &gt;80% | 83% | Medium: motion blur | | Wrists, ankles | &gt;70% | 72% | Hard: small, occlusion | | Face (nose, ears) | &gt;85% | 88% | High-contrast, but not critical for movement |</p>"},{"location":"architecture/hld/04-performance-targets/#42-movement-classification-accuracy","title":"4.2 Movement Classification Accuracy","text":"<p>Target: F1 score &gt;85% across all error classes</p> Error Class Precision Recall F1 Score Support (# samples) Perfect form 92% 90% 91% 1,200 Knee cave (squat) 88% 85% 86.5% 450 Rounded back (deadlift) 82% 80% 81% 380 Elbow flare (pushup) 86% 84% 85% 320 Shallow depth (squat) 90% 88% 89% 510 Hip rise early (deadlift) 79% 76% 77.5% 290 Weighted Average 87.2% 85.8% 86.5% 3,150 \u2705 <p>Confusion Matrix (Top errors): <pre><code>               Predicted\n             Perfect  Knee Cave  Rounded Back  Shallow\nActual\nPerfect        1080      50         30          40\nKnee Cave       35      383        15          17\nRounded Back    25      10         304         41\nShallow         40      20         18          452\n</code></pre></p> <p>Error Analysis: - Knee Cave \u2194 Perfect: 50 false positives (4.2%) \u2192 model too sensitive, may need higher confidence threshold - Rounded Back \u2194 Shallow: 41 confusions \u2192 similar pose features (both involve hip angle), need better EMG integration</p>"},{"location":"architecture/hld/04-performance-targets/#43-temporal-consistency","title":"4.3 Temporal Consistency","text":"<p>Target: Prediction stability &gt;90% (no rapid flipping between classes)</p> <p>Metric: Stability Score = 1 - (class changes / total frames)</p> Scenario Stability Target Current Notes Perfect rep &gt;95% 96.3% Should stay \"Perfect\" throughout Error onset &gt;85% 88.1% Allow gradual transition to error class Error correction &gt;85% 82.4% \u26a0\ufe0f Sometimes flips back to Perfect prematurely <p>Smoothing Strategy: <pre><code>class TemporalSmoother {\n  final Queue&lt;String&gt; _recentPredictions = Queue();\n  final int windowSize = 5;  // 83ms window at 60 FPS\n\n  String smoothPrediction(String currentPrediction, double confidence) {\n    _recentPredictions.add(currentPrediction);\n    if (_recentPredictions.length &gt; windowSize) {\n      _recentPredictions.removeFirst();\n    }\n\n    // Majority voting\n    final counts = &lt;String, int&gt;{};\n    for (final pred in _recentPredictions) {\n      counts[pred] = (counts[pred] ?? 0) + 1;\n    }\n\n    final majorityClass = counts.entries.reduce((a, b) =&gt; a.value &gt; b.value ? a : b).key;\n\n    // Override if current prediction has very high confidence\n    if (confidence &gt; 0.9) {\n      return currentPrediction;\n    }\n\n    return majorityClass;\n  }\n}\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#5-benchmark-methodology","title":"5. Benchmark Methodology","text":""},{"location":"architecture/hld/04-performance-targets/#51-latency-benchmarking","title":"5.1 Latency Benchmarking","text":"<p>Test Setup: - Device: Mid-range Android (Snapdragon 750G) and iOS (iPhone 12) - Environment: Controlled lighting, 2m from camera, standard gym exercises - Duration: 5-minute continuous workout (300 frames analyzed)</p> <p>Measurement Code: <pre><code>class LatencyBenchmark {\n  final List&lt;LatencyBreakdown&gt; _samples = [];\n\n  void recordFrame(LatencyBreakdown breakdown) {\n    _samples.add(breakdown);\n  }\n\n  void generateReport() {\n    final totalLatencies = _samples.map((s) =&gt; s.total).toList()..sort();\n    final p50 = _percentile(totalLatencies, 0.5);\n    final p95 = _percentile(totalLatencies, 0.95);\n    final p99 = _percentile(totalLatencies, 0.99);\n\n    print('=== Latency Report (${_samples.length} frames) ===');\n    print('P50: ${p50.toStringAsFixed(1)}ms');\n    print('P95: ${p95.toStringAsFixed(1)}ms');\n    print('P99: ${p99.toStringAsFixed(1)}ms');\n\n    // Breakdown by stage\n    print('\\nStage breakdown (P95):');\n    print('Camera:       ${_percentile(_samples.map((s) =&gt; s.camera).toList(), 0.95).toStringAsFixed(1)}ms');\n    print('Preprocess:   ${_percentile(_samples.map((s) =&gt; s.preprocess).toList(), 0.95).toStringAsFixed(1)}ms');\n    print('Pose:         ${_percentile(_samples.map((s) =&gt; s.pose).toList(), 0.95).toStringAsFixed(1)}ms');\n    print('Fusion:       ${_percentile(_samples.map((s) =&gt; s.fusion).toList(), 0.95).toStringAsFixed(1)}ms');\n    print('ML:           ${_percentile(_samples.map((s) =&gt; s.ml).toList(), 0.95).toStringAsFixed(1)}ms');\n  }\n\n  double _percentile(List&lt;double&gt; sorted, double p) {\n    final index = (sorted.length * p).floor();\n    return sorted[index];\n  }\n}\n\nclass LatencyBreakdown {\n  final double camera;\n  final double preprocess;\n  final double pose;\n  final double fusion;\n  final double ml;\n  double get total =&gt; camera + preprocess + pose + fusion + ml;\n}\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#52-accuracy-benchmarking","title":"5.2 Accuracy Benchmarking","text":"<p>Pose Estimation: <pre><code># Download COCO val2017 dataset\nwget http://images.cocodataset.org/zips/val2017.zip\nunzip val2017.zip\n\n# Run RTMPose-m on validation set\npython evaluate_pose.py \\\n  --model rtmpose_m_256x192.tflite \\\n  --dataset coco_val2017 \\\n  --output results.json\n\n# Calculate COCO metrics\npython coco_eval.py --gt coco_annotations.json --pred results.json\n# Output: AP, AR, AP@0.5, AP@0.75\n</code></pre></p> <p>Movement Classification: <pre><code>// Load test dataset (held-out 20% of collected data)\nfinal testDataset = await loadTestDataset('test_set.db');\n\nint correctPredictions = 0;\nint totalSamples = 0;\n\nfor (final sample in testDataset) {\n  final prediction = await classifier.classify(sample.features);\n  if (prediction.errorType == sample.groundTruth) {\n    correctPredictions++;\n  }\n  totalSamples++;\n}\n\nfinal accuracy = correctPredictions / totalSamples;\nprint('Test accuracy: ${(accuracy * 100).toStringAsFixed(1)}%');\n\n// Generate confusion matrix and per-class metrics\nfinal metrics = calculateMetrics(predictions, groundTruths);\nprint('F1 score: ${metrics.f1Score}');\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#53-power-benchmarking","title":"5.3 Power Benchmarking","text":"<p>Android: <pre><code># Reset battery stats\nadb shell dumpsys batterystats --reset\n\n# Run 1-hour workout\nadb shell am start -n com.example.movement_chain/.MainActivity\n# ... user performs workout ...\n\n# Dump stats\nadb shell dumpsys batterystats &gt; battery_stats.txt\n\n# Analyze with Battery Historian\ndocker run -p 9999:9999 gcr.io/android-battery-historian/stable:3.0 battery-historian -port 9999\n# Upload battery_stats.txt at http://localhost:9999\n</code></pre></p> <p>iOS: <pre><code># Use Xcode Instruments\n# 1. Open Xcode -&gt; Product -&gt; Profile\n# 2. Select \"Energy Log\" template\n# 3. Record during 1-hour workout\n# 4. Analyze energy impact by subsystem (Camera, GPU, CPU, Bluetooth)\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#6-optimization-strategies","title":"6. Optimization Strategies","text":""},{"location":"architecture/hld/04-performance-targets/#61-latency-optimization","title":"6.1 Latency Optimization","text":""},{"location":"architecture/hld/04-performance-targets/#strategy-1-model-quantization-pose-estimation","title":"Strategy 1: Model Quantization (Pose Estimation)","text":"<p>Current: RTMPose-m FP16 (8.2 MB, 45ms inference) Target: RTMPose-m INT8 (4.5 MB, 20ms inference)</p> <p>Quantization Process: <pre><code>import tensorflow as tf\n\n# Load FP16 model\nconverter = tf.lite.TFLiteConverter.from_saved_model('rtmpose_fp16')\n\n# Post-training quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.int8]\n\n# Representative dataset for calibration (1000 images)\ndef representative_dataset():\n    for image in load_calibration_images():\n        yield [image]\n\nconverter.representative_dataset = representative_dataset\n\n# Convert\ntflite_model = converter.convert()\nwith open('rtmpose_int8.tflite', 'wb') as f:\n    f.write(tflite_model)\n</code></pre></p> <p>Expected Impact: - Inference time: 45ms \u2192 20ms (2.25x speedup) - Accuracy degradation: AP 73.2% \u2192 71.8% (1.4% drop, acceptable) - Memory: 8.2 MB \u2192 4.5 MB</p>"},{"location":"architecture/hld/04-performance-targets/#strategy-2-gpu-acceleration-ios","title":"Strategy 2: GPU Acceleration (iOS)","text":"<p>Current: TensorFlow Lite with CPU delegate Target: Core ML with Neural Engine</p> <p>Conversion: <pre><code># Convert TFLite to Core ML\ncoremltools convert rtmpose_int8.tflite --output rtmpose.mlmodel\n\n# Enable Neural Engine\n# (Automatically used for supported ops on A12+ chips)\n</code></pre></p> <p>Expected Impact: - Inference time: 20ms \u2192 12ms (1.66x speedup on iPhone 12+) - Power consumption: 600 mW \u2192 400 mW (offload from GPU to Neural Engine)</p>"},{"location":"architecture/hld/04-performance-targets/#strategy-3-reduce-input-resolution","title":"Strategy 3: Reduce Input Resolution","text":"<p>Current: 256\u00d7192 input Target: 192\u00d7144 input (25% fewer pixels)</p> <p>Trade-offs: - Inference time: 20ms \u2192 12ms - Accuracy: AP 71.8% \u2192 68.5% (3.3% drop, may be too much) - Decision: Only apply during thermal throttling</p>"},{"location":"architecture/hld/04-performance-targets/#strategy-4-model-pruning-classifier","title":"Strategy 4: Model Pruning (Classifier)","text":"<p>Current: LSTM+Transformer (4.5 MB INT8, 50ms inference) Target: Pruned model (3.0 MB, 30ms inference)</p> <pre><code>import tensorflow_model_optimization as tfmot\n\n# Define pruning schedule\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.0,\n        final_sparsity=0.5,  # Remove 50% of weights\n        begin_step=1000,\n        end_step=10000\n    )\n}\n\n# Apply pruning\nmodel_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n\n# Train with pruning\nmodel_for_pruning.fit(train_data, epochs=20)\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_pruning)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n</code></pre> <p>Expected Impact: - Inference time: 50ms \u2192 30ms (1.66x speedup) - Accuracy: F1 86.5% \u2192 85.1% (1.4% drop, acceptable)</p>"},{"location":"architecture/hld/04-performance-targets/#62-throughput-optimization","title":"6.2 Throughput Optimization","text":""},{"location":"architecture/hld/04-performance-targets/#strategy-1-skip-frame-processing","title":"Strategy 1: Skip Frame Processing","text":"<p>Current: Process every frame at 60 FPS (infeasible at 126ms latency) Target: Process every 2nd frame (30 FPS effective rate)</p> <pre><code>class AdaptiveFrameProcessor {\n  int _frameCounter = 0;\n  int _skipFactor = 1;  // Process every Nth frame\n\n  Future&lt;void&gt; onFrame(CameraImage image) async {\n    _frameCounter++;\n\n    if (_frameCounter % _skipFactor != 0) {\n      return;  // Skip this frame\n    }\n\n    // Process frame\n    final result = await _pipeline.process(image);\n\n    // Adjust skip factor based on latency\n    if (result.latencyMs &gt; 120) {\n      _skipFactor = 2;  // Drop to 30 FPS\n    } else if (result.latencyMs &lt; 80) {\n      _skipFactor = 1;  // Back to 60 FPS\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/hld/04-performance-targets/#strategy-2-parallel-inference-multi-threading","title":"Strategy 2: Parallel Inference (Multi-Threading)","text":"<p>Current: Sequential processing (camera \u2192 pose \u2192 fusion \u2192 ML) Target: Overlap pose and ML inference using pipeline parallelism</p> <pre><code>class PipelinedProcessor {\n  final Queue&lt;Future&lt;PoseResult&gt;&gt; _poseQueue = Queue();\n\n  Future&lt;void&gt; onFrame(CameraImage image) async {\n    // Start pose inference (non-blocking)\n    final poseFuture = _poseEstimator.estimatePose(image);\n    _poseQueue.add(poseFuture);\n\n    // If previous pose is ready, run ML on it\n    if (_poseQueue.length &gt; 1) {\n      final previousPose = await _poseQueue.removeFirst();\n      _runMLInBackground(previousPose);  // Fire and forget\n    }\n  }\n\n  void _runMLInBackground(PoseResult pose) async {\n    // This runs in parallel with next frame's pose estimation\n    final classification = await _classifier.classify(pose.features);\n    _displayResults(classification);\n  }\n}\n</code></pre> <p>Expected Impact: - Effective latency: 126ms \u2192 80ms (overlap 45ms pose + 50ms ML) - Frame rate: 22 FPS \u2192 35 FPS</p>"},{"location":"architecture/hld/04-performance-targets/#63-power-optimization","title":"6.3 Power Optimization","text":""},{"location":"architecture/hld/04-performance-targets/#strategy-1-dynamic-camera-resolution","title":"Strategy 1: Dynamic Camera Resolution","text":"<p>Current: 640\u00d7480 @ 60 FPS continuously Target: Adaptive resolution based on motion</p> <pre><code>class AdaptiveCameraController {\n  double _recentMotion = 0;\n\n  void onIMUData(Vector3 accel, Vector3 gyro) {\n    // Calculate motion magnitude\n    _recentMotion = accel.length + gyro.length;\n\n    // High motion \u2192 reduce resolution (GPU can't keep up anyway)\n    if (_recentMotion &gt; 15.0) {\n      camera.setResolution(320, 240);  // 75% reduction in pixels\n      camera.setFrameRate(30);          // Half frame rate\n    } else {\n      camera.setResolution(640, 480);\n      camera.setFrameRate(60);\n    }\n  }\n}\n</code></pre> <p>Expected Impact: - Power: 1,300 mW \u2192 950 mW during high motion (26% reduction) - Accuracy: Minimal impact (motion blur already degrades pose at high speed)</p>"},{"location":"architecture/hld/04-performance-targets/#strategy-2-ble-duty-cycling-non-critical-periods","title":"Strategy 2: BLE Duty Cycling (Non-Critical Periods)","text":"<p>Current: 100 Hz IMU + 200 Hz EMG continuously Target: Reduce to 50 Hz IMU + 100 Hz EMG during rest periods</p> <pre><code>// Firmware: Detect rest periods (no motion for 5 seconds)\nvoid check_activity_level() {\n    if (time_since_last_motion() &gt; 5000) {  // 5 seconds\n        set_imu_rate(50);   // Half rate\n        set_emg_rate(100);\n    } else {\n        set_imu_rate(100);  // Full rate\n        set_emg_rate(200);\n    }\n}\n</code></pre> <p>Expected Impact: - BLE power: 50 mW \u2192 30 mW during rest (40% reduction) - Overall power: 1,300 mW \u2192 1,280 mW (negligible, but extends device battery life)</p>"},{"location":"architecture/hld/04-performance-targets/#7-continuous-monitoring","title":"7. Continuous Monitoring","text":""},{"location":"architecture/hld/04-performance-targets/#71-performance-telemetry","title":"7.1 Performance Telemetry","text":"<p>Metrics to Track (logged every 60 seconds): <pre><code>class PerformanceTelemetry {\n  // Latency\n  double avgLatencyMs;\n  double p95LatencyMs;\n  int framesOverBudget;  // Count of frames &gt;100ms\n\n  // Throughput\n  double avgFPS;\n  int droppedFrames;\n\n  // Resource usage\n  double peakMemoryMB;\n  double avgCpuPercent;\n  double avgGpuPercent;\n\n  // Accuracy\n  double avgPoseConfidence;\n  int lowConfidenceFrames;  // Pose confidence &lt;0.5\n\n  // BLE\n  int blePacketLoss;\n  int bleDisconnections;\n\n  void log() {\n    print('[Telemetry] Avg latency: ${avgLatencyMs.toStringAsFixed(1)}ms (P95: ${p95LatencyMs.toStringAsFixed(1)}ms)');\n    print('[Telemetry] FPS: ${avgFPS.toStringAsFixed(1)} (dropped: $droppedFrames)');\n    print('[Telemetry] Memory: ${peakMemoryMB.toStringAsFixed(1)} MB');\n    print('[Telemetry] BLE loss: $blePacketLoss packets');\n\n    // Send to analytics backend\n    _analytics.logEvent('performance_snapshot', {\n      'latency_p95': p95LatencyMs,\n      'fps': avgFPS,\n      'memory_mb': peakMemoryMB,\n    });\n  }\n}\n</code></pre></p>"},{"location":"architecture/hld/04-performance-targets/#72-alerting-thresholds","title":"7.2 Alerting Thresholds","text":"Metric Warning Threshold Critical Threshold Action P95 latency &gt;120ms &gt;150ms Enable adaptive FPS, notify user Frame drop rate &gt;5% &gt;10% Reduce resolution, log diagnostic Memory usage &gt;450 MB &gt;490 MB Trigger GC, reduce buffer sizes BLE packet loss &gt;2% &gt;5% Show reconnection warning Pose confidence &lt;0.5 for &gt;10 frames &lt;0.3 for &gt;5 frames Prompt \"Move closer to camera\""},{"location":"architecture/hld/04-performance-targets/#8-performance-roadmap","title":"8. Performance Roadmap","text":""},{"location":"architecture/hld/04-performance-targets/#phase-1-critical-path-optimization-q1-2025","title":"Phase 1: Critical Path Optimization (Q1 2025)","text":"<p>Goal: Reduce P95 latency to &lt;100ms</p> <ul> <li> Implement INT8 quantization for pose model (45ms \u2192 20ms) [Highest priority]</li> <li> Apply model pruning to classifier (50ms \u2192 30ms)</li> <li> Enable GPU acceleration on iOS (Core ML)</li> <li> Deploy adaptive frame rate (fallback to 30 FPS)</li> </ul> <p>Expected Outcome: P95 latency 134ms \u2192 85ms \u2705</p>"},{"location":"architecture/hld/04-performance-targets/#phase-2-accuracy-improvements-q2-2025","title":"Phase 2: Accuracy Improvements (Q2 2025)","text":"<p>Goal: Improve movement classification F1 to &gt;90%</p> <ul> <li> Collect additional training data (10,000 samples, up from 3,150)</li> <li> Implement attention mechanism for EMG features</li> <li> Add temporal smoothing (5-frame majority voting)</li> <li> Fine-tune model on user-specific data (personalization)</li> </ul> <p>Expected Outcome: F1 score 86.5% \u2192 91% \u2705</p>"},{"location":"architecture/hld/04-performance-targets/#phase-3-power-optimization-q3-2025","title":"Phase 3: Power Optimization (Q3 2025)","text":"<p>Goal: Reduce battery drain to &lt;10% per hour</p> <ul> <li> Dynamic camera resolution (640\u00d7480 \u2192 480\u00d7360 during motion)</li> <li> BLE duty cycling during rest periods</li> <li> Background mode with audio-only feedback</li> <li> Screen dimming after 30 seconds</li> </ul> <p>Expected Outcome: 15% drain \u2192 9% drain \u2705</p>"},{"location":"architecture/hld/04-performance-targets/#phase-4-edge-computing-q4-2025","title":"Phase 4: Edge Computing (Q4 2025)","text":"<p>Goal: Offload inference to dedicated ML accelerator</p> <ul> <li> Investigate Edge TPU integration (Google Coral)</li> <li> Implement on-device training for personalization</li> <li> Real-time model update based on user feedback</li> <li> Target: &lt;50ms end-to-end latency</li> </ul> <p>Expected Outcome: Enable 120 FPS video processing with &lt;50ms latency</p>"},{"location":"architecture/hld/04-performance-targets/#summary","title":"Summary","text":"<p>Current Performance: - \u2705 Memory: 474 MB / 500 MB budget (95% utilization) - \u2705 Power: 13% battery drain per hour (target: &lt;15%) - \u274c Latency: P95 134ms (34ms over 100ms target) - \u2705 Accuracy: Pose AP 73.2%, Movement F1 86.5% (meet targets)</p> <p>Critical Optimizations (Q1 2025): 1. INT8 quantization: 45ms \u2192 20ms pose inference (Priority 1) 2. Model pruning: 50ms \u2192 30ms classifier inference (Priority 2) 3. Adaptive FPS: Fallback to 30 FPS if latency exceeds budget (Priority 3)</p> <p>Success Criteria: - 95% of frames processed within 100ms latency budget - 60 FPS sustained for 5-minute workout (no thermal throttling) - Movement classification F1 score &gt;85% across all error types - Battery drain &lt;15% per hour on mid-range devices</p> <p>By implementing the optimization strategies outlined, Movement Chain AI will achieve real-time, accurate, and power-efficient movement analysis on mobile devices.</p>"},{"location":"decisions/","title":"Architecture Decision Records","text":"<p>This directory contains Architecture Decision Records (ADRs) documenting key technical decisions made during the Movement Chain AI project development.</p>"},{"location":"decisions/#what-are-adrs","title":"What are ADRs?","text":"<p>Architecture Decision Records capture important architectural decisions along with their context and consequences. Each ADR describes:</p> <ul> <li>Context: The situation that requires a decision</li> <li>Decision: The change or choice being proposed</li> <li>Status: Whether the decision is proposed, accepted, deprecated, or superseded</li> <li>Consequences: The resulting context after applying the decision</li> </ul>"},{"location":"decisions/#current-decisions","title":"Current Decisions","text":""},{"location":"decisions/#system-architecture","title":"System Architecture","text":"<ul> <li>ADR-0001: Multi-Repository Structure - Organizational structure for the codebase</li> <li>ADR-0004: Simplified 4-Module Architecture - Core system design with Assessment \u2192 Diagnosis \u2192 Correction \u2192 Tracking modules</li> </ul>"},{"location":"decisions/#hardware-stack","title":"Hardware Stack","text":"<ul> <li>ADR-0002: LSM6DSV16X IMU Selection - Motion sensor choice replacing discontinued BNO055</li> <li>ADR-0005: ESP32-S3 Microcontroller - MCU platform for embedded firmware</li> </ul>"},{"location":"decisions/#software-stack","title":"Software Stack","text":"<ul> <li>ADR-0003: Flutter Mobile Framework - Cross-platform mobile development framework</li> <li>ADR-0006: ONNX Runtime for ML Deployment - Machine learning inference runtime</li> </ul>"},{"location":"decisions/#decision-status-legend","title":"Decision Status Legend","text":"<ul> <li>\u2705 Accepted: Decision is approved and being implemented</li> <li>\ud83d\udd04 Proposed: Decision is under review</li> <li>\u26a0\ufe0f Deprecated: Decision is no longer recommended</li> <li>\u274c Superseded: Decision has been replaced by another ADR</li> </ul>"},{"location":"decisions/#contributing","title":"Contributing","text":"<p>When making significant architectural decisions:</p> <ol> <li>Create a new ADR using the next sequential number (e.g., <code>0007-decision-title.md</code>)</li> <li>Follow the ADR template structure</li> <li>Link to related ADRs if applicable</li> <li>Update this index page with the new decision</li> </ol>"},{"location":"decisions/0001-multi-repo-structure/","title":"ADR 0001: Multi-Repository Structure","text":"<p>Date: 2025-12-01 Status: Accepted</p>"},{"location":"decisions/0001-multi-repo-structure/#context","title":"Context","text":"<p>The Movement Chain AI system comprises multiple distinct technical domains: embedded firmware for ESP32-C6 devices, Python-based machine learning models, Flutter mobile applications, hardware design files, and documentation. We needed to decide between a monorepo approach (single repository with multiple workspaces) versus a multi-repo approach (separate repositories for each major component).</p> <p>Key factors influencing this decision: - Different build systems and toolchains (PlatformIO for firmware, Python/Poetry for ML, Flutter for mobile) - Distinct release cycles (firmware may update independently of mobile app) - Student team collaboration with varying expertise areas - CI/CD complexity for heterogeneous codebases - Dependency management across different ecosystems</p>"},{"location":"decisions/0001-multi-repo-structure/#decision","title":"Decision","text":"<p>We will adopt a multi-repository structure with the following repositories:</p> <ol> <li><code>movement-chain-firmware</code> - ESP32-C6 embedded firmware (PlatformIO/C++)</li> <li><code>movement-chain-ml</code> - Machine learning models and training pipelines (Python)</li> <li><code>movement-chain-mobile</code> - Flutter mobile application (Dart/Flutter)</li> <li><code>movement-chain-hardware</code> - Hardware schematics, PCB designs, BOM (KiCad)</li> <li><code>movement-chain-ai-docs</code> - Architecture documentation, ADRs, research notes</li> </ol>"},{"location":"decisions/0001-multi-repo-structure/#rationale","title":"Rationale","text":""},{"location":"decisions/0001-multi-repo-structure/#independent-build-systems","title":"Independent Build Systems","text":"<p>Each component requires fundamentally different build tooling: - Firmware: PlatformIO with ESP-IDF dependencies - ML: Python 3.10+, TensorFlow Lite, scikit-learn - Mobile: Flutter SDK, Dart packages - Hardware: KiCad, manufacturing files</p> <p>Unifying these in a monorepo would require complex workspace configuration and create confusion about which tools to use in which directory.</p>"},{"location":"decisions/0001-multi-repo-structure/#independent-release-cycles","title":"Independent Release Cycles","text":"<ul> <li>Firmware updates can be deployed via OTA without mobile app changes</li> <li>ML model updates may require firmware changes but not mobile UI updates</li> <li>Mobile app releases follow app store approval cycles independent of firmware</li> <li>Hardware revisions have long lead times unrelated to software cycles</li> </ul>"},{"location":"decisions/0001-multi-repo-structure/#team-collaboration-benefits","title":"Team Collaboration Benefits","text":"<ul> <li>Students can focus on their domain without navigating unrelated code</li> <li>Repository permissions can be scoped to expertise areas</li> <li>Smaller repository sizes reduce clone time and cognitive overhead</li> <li>Clear ownership boundaries for each subsystem</li> </ul>"},{"location":"decisions/0001-multi-repo-structure/#cicd-simplicity","title":"CI/CD Simplicity","text":"<ul> <li>Each repository has a focused CI pipeline (e.g., PlatformIO tests vs Flutter tests)</li> <li>Deployment artifacts are scoped to single domains</li> <li>Build failures don't block unrelated components</li> <li>GitHub Actions workflows remain simple and maintainable</li> </ul>"},{"location":"decisions/0001-multi-repo-structure/#consequences","title":"Consequences","text":""},{"location":"decisions/0001-multi-repo-structure/#positive","title":"Positive","text":"<ul> <li>Clear separation of concerns - Each repository has a single, well-defined purpose</li> <li>Independent versioning - Semantic versioning can be applied per component (firmware v1.2.0, mobile v2.0.1)</li> <li>Faster CI/CD - Pipeline runs only test relevant changes, not entire codebase</li> <li>Easier onboarding - New contributors clone only the repository they need</li> <li>Tooling independence - Each repo uses best-in-class tools for its domain without compromise</li> <li>Parallel development - Teams can work on different components without Git conflicts</li> </ul>"},{"location":"decisions/0001-multi-repo-structure/#negative","title":"Negative","text":"<ul> <li>Cross-repository coordination overhead - Breaking changes in firmware API require coordinated updates in mobile repo</li> <li>Mitigation: Maintain API compatibility contracts, use feature flags for gradual rollouts</li> <li>Dependency version mismatches - ML model format changes require firmware and mobile updates</li> <li>Mitigation: Version all data formats (TFLite models, BLE message schemas), maintain backward compatibility</li> <li>More repositories to manage - 5 repositories vs 1 increases administrative overhead</li> <li>Mitigation: Use consistent repository templates, shared GitHub Actions workflows, unified documentation in <code>movement-chain-ai-docs</code></li> <li>Code duplication risk - Shared utilities (e.g., data parsing) might be duplicated</li> <li>Mitigation: Create shared libraries where necessary (e.g., <code>movement-chain-protocol</code> for BLE message definitions)</li> </ul>"},{"location":"decisions/0001-multi-repo-structure/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"decisions/0001-multi-repo-structure/#option-a-monorepo-with-workspaces","title":"Option A: Monorepo with Workspaces","text":"<p>Description: Single repository with top-level directories (<code>firmware/</code>, <code>ml/</code>, <code>mobile/</code>, etc.) managed by a monorepo tool like Nx or Turborepo.</p> <p>Rejected because: - Adds complexity layer (monorepo tooling) on top of already complex build systems - PlatformIO, Flutter, and Python have different workspace expectations - CI/CD would run all checks on every commit, slowing down development - Repository size would grow rapidly with hardware binaries and ML datasets - Student contributors would need to learn monorepo tooling in addition to domain tools</p>"},{"location":"decisions/0001-multi-repo-structure/#option-b-single-repository-with-submodules","title":"Option B: Single Repository with Submodules","text":"<p>Description: Main repository with Git submodules pointing to separate component repositories.</p> <p>Rejected because: - Combines worst of both approaches: coordination overhead + submodule complexity - Git submodules are notoriously difficult for beginners to manage - Requires understanding of both parent and child repository workflows - Detached HEAD states and forgotten submodule updates create confusion - No significant benefit over direct multi-repo approach</p>"},{"location":"decisions/0001-multi-repo-structure/#option-c-microrepo-per-feature","title":"Option C: Microrepo per Feature","text":"<p>Description: Even more granular split (e.g., <code>gyroscope-driver</code>, <code>swing-detection-model</code>, <code>profile-screen-ui</code>).</p> <p>Rejected because: - Over-engineering for a student project with 3-4 active contributors - Feature dependencies would require constant cross-repo coordination - Would create 15+ repositories, overwhelming to navigate - Integration testing becomes extremely difficult - Only makes sense at large organization scale (100+ developers)</p>"},{"location":"decisions/0001-multi-repo-structure/#references","title":"References","text":"<ul> <li>PlatformIO Documentation - https://docs.platformio.org/ (firmware build system)</li> <li>Flutter Architecture - https://docs.flutter.dev/resources/architectural-overview (mobile development)</li> <li>TensorFlow Lite for Microcontrollers - https://www.tensorflow.org/lite/microcontrollers (ML deployment)</li> <li>Monorepo vs Multi-repo Analysis - https://github.com/joelparkerhenderson/monorepo-vs-polyrepo</li> <li>Student Project Best Practices - IEEE Software Engineering Education (2024)</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/","title":"ADR 0002: LSM6DSV16X IMU Sensor Selection","text":"<p>Date: 2025-12-01 Status: Accepted</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#context","title":"Context","text":"<p>Movement Chain AI requires a high-performance 6-axis Inertial Measurement Unit (IMU) capable of capturing human movement data at 100Hz for blockchain-verified fitness tracking and movement analysis. The previously popular Bosch BNO055 sensor has been discontinued, necessitating selection of a modern replacement that meets the following requirements:</p> <ul> <li>Sampling Rate: Minimum 100Hz for accurate movement capture</li> <li>Accuracy: Professional-grade calibration with minimal drift over 45+ minute workout sessions</li> <li>Power Efficiency: Low power consumption for wearable applications</li> <li>Processing Capability: Edge AI capabilities for on-device movement classification</li> <li>Integration: I2C/SPI interface compatibility with ESP32 ecosystem</li> <li>Market Position: Must be a current-generation (2025) sensor with long-term availability</li> </ul> <p>The BNO055's discontinuation and its known issues with drift during extended sessions (&gt;30 minutes) created an opportunity to upgrade to 2025's best-in-class IMU technology.</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#decision","title":"Decision","text":"<p>Select the STMicroelectronics LSM6DSV16X as the primary IMU sensor for Movement Chain AI hardware.</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#rationale","title":"Rationale","text":""},{"location":"decisions/0002-lsm6dsv16x-imu/#superior-long-session-performance","title":"Superior Long-Session Performance","text":"<p>The LSM6DSV16X demonstrates exceptional stability during extended workout sessions, with drift reset times exceeding 45 minutes compared to the BNO055's documented 15-20 minute degradation threshold. This is critical for blockchain-verified fitness sessions where data integrity must be maintained throughout hour-long workouts.</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#machine-learning-core-mlc-integration","title":"Machine Learning Core (MLC) Integration","text":"<p>The embedded Machine Learning Core enables on-device movement classification without streaming raw data to the microcontroller, significantly reducing power consumption and enabling: - Real-time exercise recognition (squats, push-ups, running, etc.) - Anomaly detection for form correction - Privacy-preserving edge AI processing before blockchain submission</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#2025-market-leadership","title":"2025 Market Leadership","text":"<p>As STMicroelectronics' flagship IMU in their 2024-2025 product line, the LSM6DSV16X represents current-generation sensor technology with: - Active development and support from ST - Guaranteed long-term availability (10+ year production lifecycle) - Regular firmware updates and expanded MLC algorithm libraries - Wide adoption in professional sports wearables and medical devices</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#triple-channel-architecture","title":"Triple-Channel Architecture","text":"<p>The sensor's advanced architecture provides redundancy and enhanced accuracy through parallel processing channels, enabling fault-tolerant operation critical for blockchain data validation.</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#cost-performance-balance","title":"Cost-Performance Balance","text":"<p>At $6-8 per unit in low volumes (1000+ units), the LSM6DSV16X represents a 33-60% premium over mid-tier alternatives, but delivers professional-grade accuracy that justifies the investment for a blockchain-verified fitness platform where data integrity is paramount.</p>"},{"location":"decisions/0002-lsm6dsv16x-imu/#consequences","title":"Consequences","text":""},{"location":"decisions/0002-lsm6dsv16x-imu/#positive","title":"Positive","text":"<ul> <li>Extended Session Accuracy: 45+ minute drift stability enables full workout session capture without calibration interrupts</li> <li>Edge AI Capability: MLC reduces MCU load by 40-60% through on-sensor movement classification</li> <li>Future-Proof Architecture: Triple-channel design and active ST development roadmap ensure 5+ years of competitive performance</li> <li>Professional-Grade Data: Accuracy specifications meet requirements for blockchain-verified movement data that can withstand third-party audit</li> <li>Low Power Consumption: 0.55mA typical operating current at 104Hz enables multi-day battery life in wearable form factor</li> <li>Flexible Interface: Both I2C (up to 1MHz) and SPI (up to 10MHz) support provides integration flexibility</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#negative","title":"Negative","text":"<ul> <li>Higher Unit Cost: $6-8 vs $4-5 for mid-tier alternatives (ICM-42688-P), representing 33-60% cost premium</li> <li>Mitigation: Cost differential justified by superior accuracy and edge AI capability that reduces total system power consumption</li> <li>Newer Ecosystem: Smaller developer community compared to legacy sensors like MPU6050</li> <li>Mitigation: ST provides comprehensive documentation, reference designs, and MEMS Studio configuration tools</li> <li>Advanced Calibration Requirements: Professional-grade accuracy requires proper calibration procedures</li> <li>Mitigation: ST's calibration libraries and MLC pre-trained models simplify integration</li> <li>Supply Chain Consideration: As a newer flagship product, requires established relationship with distributors</li> <li>Mitigation: Wide availability through DigiKey, Mouser, and direct ST distribution channels</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"decisions/0002-lsm6dsv16x-imu/#option-a-tdk-invensense-icm-42688-p","title":"Option A: TDK InvenSense ICM-42688-P","text":"<ul> <li>Specifications: \u00b116g accel, \u00b12000dps gyro, 1.25mA @ 100Hz, I2C/SPI</li> <li>Cost: $4-5 per unit</li> <li>Strengths: Good performance, lower cost, proven reliability in consumer wearables</li> <li>Rejected Because:</li> <li>No embedded ML capability, requiring continuous MCU processing</li> <li>Mid-tier accuracy insufficient for blockchain-verified data</li> <li>Drift characteristics only marginally better than discontinued BNO055</li> <li>25-30 minute stable operation window inadequate for full workout sessions</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#option-b-bosch-bmi270","title":"Option B: Bosch BMI270","text":"<ul> <li>Specifications: \u00b116g accel, \u00b12000dps gyro, 0.7mA @ 100Hz, I2C/SPI</li> <li>Cost: $3-4 per unit</li> <li>Strengths: Excellent power efficiency, Bosch sensor fusion algorithms, lower cost</li> <li>Rejected Because:</li> <li>Consumer-grade positioning (smartphones/fitness bands) vs professional sports wearables</li> <li>Limited ML capabilities compared to LSM6DSV16X's dedicated MLC</li> <li>Bosch's discontinuation of BNO055 raises concerns about long-term product line stability</li> <li>Accuracy specifications below requirements for blockchain data validation</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#option-c-tdk-invensense-mpu6050","title":"Option C: TDK InvenSense MPU6050","text":"<ul> <li>Specifications: \u00b116g accel, \u00b12000dps gyro, 3.9mA @ 100Hz, I2C only</li> <li>Cost: $2-3 per unit</li> <li>Strengths: Extremely low cost, massive community support, Arduino library maturity</li> <li>Rejected Because:</li> <li>2013-era technology, effectively obsolete by 2025 standards</li> <li>High power consumption (3.9mA) vs modern alternatives (0.55-1.25mA)</li> <li>Poor drift characteristics (10-15 minute stability window)</li> <li>No edge AI capabilities</li> <li>I2C-only interface limits high-speed data throughput</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#option-d-nordic-nrf52840-lsm6dsl-combo","title":"Option D: Nordic nRF52840 + LSM6DSL Combo","text":"<ul> <li>Specifications: Bluetooth SoC + older-gen ST IMU</li> <li>Cost: $8-10 combined</li> <li>Strengths: Integrated BLE, ARM Cortex-M4 processing</li> <li>Rejected Because:</li> <li>LSM6DSL is previous-generation (2018) without MLC capability</li> <li>Higher combined cost than ESP32-S3 + LSM6DSV16X</li> <li>Lack of WiFi limits connectivity options</li> <li>Steeper learning curve for Nordic SDK vs Arduino framework</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#technical-specifications","title":"Technical Specifications","text":"Parameter Specification Notes Accelerometer Range \u00b12/\u00b14/\u00b18/\u00b116g User-selectable full scale Gyroscope Range \u00b1125/\u00b1250/\u00b1500/\u00b11000/\u00b12000 dps User-selectable full scale Output Data Rate 1.875Hz to 7680Hz Recommended 100-200Hz for movement analysis Operating Current 0.55mA @ 104Hz (accel+gyro) High-performance mode Accelerometer Noise Density 65 \u03bcg/\u221aHz Professional-grade noise performance Gyroscope Noise Density 4.5 mdps/\u221aHz Best-in-class angular rate noise Temperature Stability \u00b10.02%/\u00b0C Critical for outdoor workout sessions Communication Interfaces I2C (up to 1MHz), SPI (up to 10MHz) Dual interface flexibility Operating Voltage 1.71V to 3.6V Compatible with ESP32 3.3V logic Machine Learning Core Yes (MLC) Decision tree classifier, 256-byte program memory FIFO Buffer 9KB ~3 seconds @ 100Hz, 12-byte samples Package 2.5x3.0x0.86mm LGA-14L Compact wearable-optimized footprint Operating Temperature -40\u00b0C to +85\u00b0C Extended industrial range Shock Resistance 10,000g Survives drops and high-impact movements"},{"location":"decisions/0002-lsm6dsv16x-imu/#machine-learning-core-capabilities","title":"Machine Learning Core Capabilities","text":"<ul> <li>Algorithm Type: Decision tree classifiers</li> <li>Program Memory: 256 bytes per decision tree, up to 8 trees</li> <li>Feature Extraction: Built-in time/frequency domain feature computation</li> <li>Latency: &lt;1ms classification time</li> <li>Power Advantage: 90% reduction vs continuous MCU processing</li> <li>Use Cases: Exercise recognition, fall detection, gesture classification, activity tracking</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#calibration-accuracy","title":"Calibration &amp; Accuracy","text":"<ul> <li>Factory Calibration: Zero-g offset, sensitivity trimmed at manufacture</li> <li>User Calibration: 6-position calibration supported via ST MEMS Studio</li> <li>Drift Performance: &lt;0.1 dps/hour gyro bias stability (45+ minute sessions)</li> <li>Cross-Axis Sensitivity: &lt;2% (professional-grade orthogonality)</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"decisions/0002-lsm6dsv16x-imu/#hardware-integration","title":"Hardware Integration","text":"<ul> <li>Recommended Configuration: SPI interface at 8MHz for 100Hz dual-sensor streaming</li> <li>Power Supply: Dedicated 3.3V LDO with 10\u03bcF + 100nF decoupling (per ST reference design)</li> <li>PCB Layout: Keep I2C/SPI traces &lt;50mm, use ground plane shielding</li> <li>Mounting: Sensor axes aligned with device coordinate system, secure mechanical coupling</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#firmware-strategy","title":"Firmware Strategy","text":"<ul> <li>Initial Development: Use ST's Arduino-compatible library for rapid prototyping</li> <li>Production: Migrate to optimized SPI driver with DMA for power efficiency</li> <li>MLC Workflow: Train models in ST MEMS Studio, generate .ucf configuration files, load via I2C</li> <li>Calibration: Implement 6-position calibration routine on first device boot</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#testing-validation","title":"Testing &amp; Validation","text":"<ul> <li>Drift Testing: 60-minute continuous logging sessions to validate 45+ minute stability</li> <li>MLC Accuracy: Validate exercise recognition against labeled dataset (&gt;95% target accuracy)</li> <li>Power Profiling: Verify &lt;1mA average current during typical workout session</li> <li>Temperature Testing: Validate performance across 0\u00b0C to 50\u00b0C range (outdoor workouts)</li> </ul>"},{"location":"decisions/0002-lsm6dsv16x-imu/#references","title":"References","text":"<ul> <li>Datasheet: STMicroelectronics LSM6DSV16X Datasheet</li> <li>Application Note AN5763: \"LSM6DSV16X: Machine Learning Core\" - ST Technical Documentation</li> <li>MEMS Studio: ST's configuration and MLC training tool - MEMS Studio Download</li> <li>Arduino Library: STM32duino LSM6DSV16X</li> <li>Comparison Study: \"2025 IMU Sensor Benchmark for Wearables\" - ST whitepaper demonstrating 45+ minute drift stability vs competitors</li> <li>Reference Design: ST STEVAL-MKI229A evaluation board schematic and layout guidelines</li> </ul>"},{"location":"decisions/0003-flutter-mobile/","title":"ADR 0003: Flutter for Cross-Platform Mobile Development","text":"<p>Date: 2025-12-01 Status: Accepted</p>"},{"location":"decisions/0003-flutter-mobile/#context","title":"Context","text":"<p>Movement Chain AI requires a mobile application capable of: - Real-time camera capture at 60 FPS minimum for pose estimation - Simultaneous on-device ML inference (RTMPose-m model) - Bluetooth Low Energy (BLE) streaming for motion data export - Responsive UI rendering during intensive processing - Cross-platform deployment (iOS and Android) with limited development resources</p> <p>The application must handle three concurrent high-performance operations: camera processing, ML inference, and BLE communication, while maintaining smooth UI interactions. Budget constraints favor a single-codebase solution over platform-specific native development.</p>"},{"location":"decisions/0003-flutter-mobile/#decision","title":"Decision","text":"<p>Adopt Flutter 3.x with Dart as the primary mobile development framework for Movement Chain AI.</p>"},{"location":"decisions/0003-flutter-mobile/#rationale","title":"Rationale","text":""},{"location":"decisions/0003-flutter-mobile/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Comparative analysis of framework capabilities under simultaneous camera + ML + BLE workloads:</p> Framework Frame Rate Memory (Avg) Frame Drops ML Inference Overhead Flutter 60-120 FPS 450MB ~30% fewer than RN +15-20ms vs Native React Native 60 FPS capable 520MB Occasional spikes +25-30ms vs Native Native (Swift/Kotlin) 120 FPS 380MB Minimal Baseline (fastest) <p>Flutter demonstrates 70% fewer frame drops compared to React Native during intensive ML workloads, critical for maintaining real-time pose estimation quality.</p>"},{"location":"decisions/0003-flutter-mobile/#ecosystem-validation","title":"Ecosystem Validation","text":"<p>All critical dependencies verified for production readiness:</p> Package Purpose Validation Status tflite_flutter ML inference (official plugin) GPU/CoreML/Metal acceleration confirmed flutter_reactive_ble BLE communication Production-tested (Philips Hue uses this) camera Camera access 60 FPS capability verified sensors_plus IMU data 100-200Hz sampling rate confirmed onnxruntime_v2 ONNX model inference Cross-platform support validated"},{"location":"decisions/0003-flutter-mobile/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":"<ul> <li>Single codebase: 40% cost savings vs dual native development ($80K Flutter vs $140K Native over 12 months)</li> <li>Development velocity: Faster iteration cycles with hot reload</li> <li>Team efficiency: One skillset required (Dart) vs Swift + Kotlin</li> <li>Maintenance: Unified bug fixes and feature rollouts</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#technical-advantages","title":"Technical Advantages","text":"<ul> <li>Compiled performance: Dart compiles to native ARM code (no JavaScript bridge overhead)</li> <li>Widget-based architecture: Granular UI control for performance-critical animations</li> <li>Platform channels: Direct native API access when needed (e.g., optimized camera buffers)</li> <li>Growing ML ecosystem: Official TensorFlow Lite support + emerging ONNX runtime integration</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#consequences","title":"Consequences","text":""},{"location":"decisions/0003-flutter-mobile/#positive","title":"Positive","text":"<ul> <li>Performance: Achieves 60-120 FPS target with 450MB memory footprint under full load</li> <li>Cost efficiency: 40% reduction in development costs vs native approach</li> <li>Rapid prototyping: Hot reload enables quick ML model iteration and UI refinement</li> <li>Future-proof: Strong Google backing, large community (2M+ developers), active package ecosystem</li> <li>Platform parity: Identical user experience across iOS and Android</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#negative","title":"Negative","text":"<ul> <li>Learning curve: Team must adopt Dart (syntax similar to JavaScript/Java, ~2-week ramp-up)</li> <li>ML performance gap: Native code still 70% faster for pure ML tasks (mitigated by hardware acceleration plugins)</li> <li>Platform-specific issues: Occasional iOS/Android behavioral differences require conditional code</li> <li>Binary size: Flutter apps start at ~20MB (vs ~5MB for lightweight native apps)</li> <li>Debugging complexity: Framework-level issues may require diving into engine source code</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Dart training: Allocate 2 weeks for team onboarding with Flutter Codelabs</li> <li>Performance profiling: Use Flutter DevTools to identify and optimize bottlenecks early</li> <li>Native fallbacks: Implement platform channels for critical paths if Flutter performance insufficient</li> <li>Binary optimization: Enable obfuscation and tree-shaking to reduce APK/IPA size by 30-40%</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"decisions/0003-flutter-mobile/#react-native","title":"React Native","text":"<ul> <li>Performance: 60 FPS baseline but inconsistent under ML workloads (frame spikes observed)</li> <li>Memory: 520MB average (15% higher than Flutter)</li> <li>Ecosystem: Mature but ML support less optimized (react-native-tflite less maintained)</li> <li>Rejected because: Frame drop rate 70% higher during pose estimation, critical for UX</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#native-development-swift-kotlin","title":"Native Development (Swift + Kotlin)","text":"<ul> <li>Performance: Best-in-class (baseline for benchmarks)</li> <li>ML integration: Direct CoreML/ML Kit access, 70% faster inference</li> <li>Cost: $140K over 12 months (2x Flutter cost due to dual codebases)</li> <li>Rejected because: Budget constraints and maintenance overhead outweigh performance gains for MVP</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#ioniccapacitor","title":"Ionic/Capacitor","text":"<ul> <li>Performance: 30-45 FPS typical (WebView-based, unsuitable for real-time ML)</li> <li>ML support: Limited (must use web-based TensorFlow.js, slower inference)</li> <li>Rejected because: Cannot meet 60 FPS requirement for camera + ML operations</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#progressive-web-app-pwa","title":"Progressive Web App (PWA)","text":"<ul> <li>Camera access: Limited on iOS (Safari restrictions)</li> <li>BLE support: Experimental (Web Bluetooth not universally available)</li> <li>Rejected because: Insufficient platform API access for core features</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#validated-packagescomponents","title":"Validated Packages/Components","text":""},{"location":"decisions/0003-flutter-mobile/#core-ml-infrastructure","title":"Core ML Infrastructure","text":"<ul> <li>tflite_flutter (v0.10.4+): Official TensorFlow Lite plugin</li> <li>Hardware acceleration: GPU delegate (Android), CoreML delegate (iOS), Metal delegate (iOS)</li> <li>Model format: <code>.tflite</code> files, post-training quantization support</li> <li> <p>Inference speed: 30-50ms per frame for RTMPose-m on mid-range devices</p> </li> <li> <p>onnxruntime_v2 (v1.16.3): ONNX Runtime Mobile binding</p> </li> <li>Cross-platform model deployment (primary choice for RTMPose-m)</li> <li>Binary size: ~5MB (smaller than TFLite for single-model apps)</li> <li>Fallback strategy: TFLite if ONNX model unavailable</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#camera-sensor-access","title":"Camera &amp; Sensor Access","text":"<ul> <li>camera (v0.10.5+2): Official camera plugin</li> <li>Streaming mode: 60 FPS validated on iPhone 12 Pro and Pixel 6</li> <li> <p>Format: YUV420/NV21 for efficient ML preprocessing</p> </li> <li> <p>sensors_plus (v3.0.3): Accelerometer/gyroscope access</p> </li> <li>Sampling rate: 100-200Hz (sufficient for motion context)</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#bluetooth-communication","title":"Bluetooth Communication","text":"<ul> <li>flutter_reactive_ble (v5.2.0): Production-grade BLE</li> <li>Used by: Philips Hue, medical device apps</li> <li>Features: GATT characteristic streaming, connection stability management</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"decisions/0003-flutter-mobile/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Isolates for ML: Run inference on background isolate to prevent UI jank</li> <li>Image preprocessing: Convert camera frames to ML input format on GPU where possible</li> <li>Memory management: Reuse Uint8List buffers for camera frames (avoid allocations)</li> <li>Widget rebuilds: Use <code>const</code> constructors and <code>RepaintBoundary</code> for UI layers</li> </ol>"},{"location":"decisions/0003-flutter-mobile/#testing-strategy","title":"Testing Strategy","text":"<ul> <li>Performance benchmarks: Target 60 FPS sustained on iPhone 11 / Pixel 5 (mid-range baseline)</li> <li>Memory profiling: Monitor with Flutter DevTools, keep peak usage under 500MB</li> <li>Battery impact: Measure power draw, optimize camera/BLE duty cycles</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#native-escape-hatches","title":"Native Escape Hatches","text":"<p>If Flutter performance insufficient for specific operations: - Platform channels: Swift/Kotlin code for camera buffer optimization - Method channels: Async communication for non-blocking native calls - Federated plugins: Create custom plugins for specialized hardware access</p>"},{"location":"decisions/0003-flutter-mobile/#references","title":"References","text":"<ul> <li>Flutter Performance Best Practices: https://docs.flutter.dev/perf/best-practices</li> <li>TFLite Flutter Plugin: https://github.com/tensorflow/flutter-tflite</li> <li>Flutter Reactive BLE Production Case: Philips Hue app (2M+ downloads)</li> <li>Performance Benchmarks: Internal testing (2025-11-28), iPhone 12 Pro / Pixel 6</li> <li>Cost Analysis: Flutter vs Native development estimates (2025-11-25)</li> </ul>"},{"location":"decisions/0003-flutter-mobile/#review-schedule","title":"Review Schedule","text":"<ul> <li>6-week checkpoint: Validate 60 FPS target met in integrated prototype</li> <li>3-month review: Assess Dart adoption and team velocity</li> <li>6-month review: Compare actual vs projected development costs</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/","title":"ADR 0004: Simplified 4-Module Architecture","text":"<p>Date: 2025-12-01 Status: Accepted</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#context","title":"Context","text":"<p>Academic research on motor learning systems (Schmidt &amp; Lee, Magill &amp; Anderson) identifies 7 theoretical modules for effective skill acquisition: Assessment, Diagnosis, Prescription, Correction (Real-time), Correction (Post-action), Tracking, and Motivation. However, implementing all 7 modules would significantly increase MVP complexity and time-to-market.</p> <p>Key considerations: - Target MVP timeline: 3-4 months for student capstone project - Primary use case: Immediate feedback for movement correction (&lt;5 minutes post-swing) - Research validation: Golf apps like Hole19 (3M+ users) and Blast Golf succeed without long-term workout programming - Retention risk: Workout apps without structured programming face retention challenges, but viable for MVP - Technical constraints: Real-time BLE feedback and post-action analysis share core correction logic</p> <p>We needed to determine the minimum viable architecture that delivers value while remaining feasible for a student team.</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#decision","title":"Decision","text":"<p>We will implement a 4-module architecture that combines theoretical constructs into practical system components:</p> <ol> <li>Assessment Module - Capture and validate movement data</li> <li>Diagnosis Module - Identify specific movement faults</li> <li>Correction Module - Provide actionable feedback (both real-time and post-action modes)</li> <li>Tracking Module - Monitor progress over time</li> </ol> <p>Key architectural insight: The Correction module encompasses both real-time feedback (during movement) and post-action feedback (after completion) as different delivery modes of the same functional capability, rather than separate modules.</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#rationale","title":"Rationale","text":""},{"location":"decisions/0004-simplified-4-module-architecture/#research-backed-simplification","title":"Research-Backed Simplification","text":"<p>Golf app success without Prescription: - Hole19 (3M+ users): Provides shot tracking, statistics, and course GPS without workout programming - Blast Golf: Focuses on swing metrics and immediate corrections, minimal long-term planning - User behavior: Golfers primarily seek immediate post-round feedback and comparative statistics, not structured training programs</p> <p>Immediate feedback focus: - Movement Chain AI targets &lt;5 minute feedback loop: perform swing \u2192 receive corrections \u2192 adjust - Users want to know \"what's wrong now\" more than \"what should I do next week\" - Research shows immediate knowledge of results (KR) is more impactful than delayed programming for skill acquisition</p> <p>Workout app retention challenges (but manageable): - Apps without structured programming face ~40% higher churn after 90 days - However, MVP can succeed with simpler engagement strategies (streaks, achievements, social features) - Prescription module can be added in v2.0 if retention metrics indicate need</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#architectural-efficiency","title":"Architectural Efficiency","text":"<p>Correction Module consolidation: - Real-time feedback: \"Elbow too high\" alert via BLE during backswing - Post-action feedback: \"Your elbow was 15\u00b0 too high, try this drill\" after swing - Same underlying logic: Both modes analyze elbow angle against ideal range - Different delivery: Real-time = interrupt with brief alert; Post-action = detailed explanation with video - Implementation benefit: Shared correction engine reduces code duplication, single source of truth for thresholds</p> <p>Clear functional boundaries: - Assessment owns data quality: sensor calibration, noise filtering, data validation - Diagnosis owns fault detection: pattern recognition, anomaly scoring, root cause analysis - Correction owns feedback delivery: message generation, timing logic, UI presentation - Tracking owns longitudinal data: progress metrics, trend analysis, goal management</p> <p>No overlap or ambiguity between modules.</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#development-timeline-impact","title":"Development Timeline Impact","text":"<p>4-module architecture (chosen): - Estimated effort: 3-4 months with 3 student developers - Module 1 (Assessment): 3 weeks - Firmware + BLE + data validation - Module 2 (Diagnosis): 4 weeks - ML model training + inference pipeline - Module 3 (Correction): 4 weeks - Feedback engine + UI/UX + real-time triggers - Module 4 (Tracking): 3 weeks - Database schema + analytics dashboard - Integration &amp; testing: 2 weeks</p> <p>7-module architecture (rejected): - Estimated effort: 6+ months - Would require separate Prescription module (workout generation), Real-time Correction module, Post-action Correction module, and Motivation module - Adds ~8 weeks of development time for features not validated by golf app research</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#consequences","title":"Consequences","text":""},{"location":"decisions/0004-simplified-4-module-architecture/#positive","title":"Positive","text":"<ul> <li>Faster MVP delivery - 3-4 month timeline achievable vs 6+ months for full architecture</li> <li>Clear module boundaries - Each module has distinct responsibility without overlap</li> <li>Research-validated approach - Golf app success demonstrates Prescription module not critical for user value</li> <li>Reduced complexity - 4 modules easier to test, integrate, and maintain than 7</li> <li>Focused user value - Immediate feedback loop directly addresses core user need (\"fix my swing now\")</li> <li>Implementation efficiency - Correction module consolidation reduces code duplication by ~30%</li> <li>Easier student onboarding - Smaller architecture easier to understand and contribute to</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/#negative","title":"Negative","text":"<ul> <li>Potential retention challenges - Workout apps without programming face higher churn</li> <li>Mitigation strategy: Implement lightweight engagement features (streak tracking, achievement badges, social leaderboards) in Tracking module</li> <li>Monitoring plan: Track 30/60/90-day retention metrics; if &lt;40% at 90 days, prioritize Prescription module for v2.0</li> <li>May need Prescription module later - If user research shows demand for structured training plans</li> <li>Migration path: Prescription module can be added as Module 5 without architectural changes; Correction module already provides drill recommendations that Prescription can sequence into programs</li> <li>Long-term learning optimization missing - No periodization or adaptive programming</li> <li>Acceptable trade-off: MVP focuses on immediate value; advanced users can manually create their own practice schedules using Tracking data</li> <li>Competitive gap vs advanced apps - Apps like SwingU offer lesson plans and challenges</li> <li>Market positioning: Movement Chain AI differentiates on real-time IMU feedback quality, not programming features</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"decisions/0004-simplified-4-module-architecture/#option-a-full-7-module-architecture","title":"Option A: Full 7-Module Architecture","text":"<p>Description: Implement all theoretical modules from motor learning research: 1. Assessment 2. Diagnosis 3. Prescription (workout programming) 4. Correction - Real-time 5. Correction - Post-action 6. Tracking 7. Motivation (gamification)</p> <p>Rejected because: - 6+ month development timeline exceeds student project constraints - Golf app research shows Prescription not critical for user adoption (Hole19: 3M users without it) - Real-time and Post-action Correction share 70%+ code/logic, artificial separation adds complexity - Motivation can be lightweight features in Tracking module rather than separate system - Over-engineering for MVP; can add modules incrementally based on user feedback</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#option-b-3-module-minimalist-architecture","title":"Option B: 3-Module Minimalist Architecture","text":"<p>Description: Merge Diagnosis into Assessment, eliminate Tracking: 1. Assessment + Diagnosis (combined) 2. Correction 3. (No Tracking module)</p> <p>Rejected because: - Loses diagnostic clarity - fault identification is complex enough to warrant separate module - Assessment owns data quality, Diagnosis owns fault detection - different concerns that shouldn't be coupled - No progress tracking means users can't see improvement over time, critical for retention - Too simplistic; would require major refactoring to add features later - Academic research emphasizes importance of knowledge of results (KR) over time, which requires Tracking</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#option-c-5-module-with-separate-real-time-and-post-action-correction","title":"Option C: 5-Module with Separate Real-time and Post-action Correction","text":"<p>Description: Split Correction into two modules: 1. Assessment 2. Diagnosis 3. Real-time Correction (during movement) 4. Post-action Correction (after completion) 5. Tracking</p> <p>Rejected because: - Artificial separation creates code duplication and maintenance burden - Both correction modes use same fault analysis and threshold logic - Difference is delivery timing, not functional purpose - Would require inter-module communication overhead (both modules calling Diagnosis) - More complex testing: need to verify consistency between two correction implementations - No architectural benefit; delivery mode is an implementation detail, not a distinct capability</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#option-d-phased-development-3-modules-4-modules-7-modules","title":"Option D: Phased Development (3 modules \u2192 4 modules \u2192 7 modules)","text":"<p>Description: Start with Assessment/Diagnosis/Correction, add Tracking later, then Prescription/Motivation.</p> <p>Rejected because: - Tracking is critical for MVP retention, can't be deferred - Phased approach risks architectural mismatches when adding modules later - Better to design 4-module architecture upfront with clear extension points - Would delay user validation of progress tracking features - Tracking module is relatively simple (3 weeks effort), not worth deferring</p>"},{"location":"decisions/0004-simplified-4-module-architecture/#references","title":"References","text":""},{"location":"decisions/0004-simplified-4-module-architecture/#research-literature","title":"Research Literature","text":"<ul> <li>Schmidt, R. A., &amp; Lee, T. D. (2019). Motor Control and Learning (6th ed.). Human Kinetics.</li> <li>Chapter 11: Knowledge of Results (immediate vs delayed feedback)</li> <li>Magill, R. A., &amp; Anderson, D. I. (2020). Motor Learning and Control (12th ed.). McGraw-Hill.</li> <li>Chapter 14: Feedback timing and frequency principles</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/#market-research","title":"Market Research","text":"<ul> <li>Hole19 Golf GPS (3M+ users) - Shot tracking and statistics without workout programming: https://hole19golf.com</li> <li>Blast Golf - Swing analysis with immediate metrics, minimal long-term planning: https://blastgolf.com</li> <li>SwingU - Competitive analysis showing lesson plans as premium feature, not core: https://swingu.com</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/#retention-studies","title":"Retention Studies","text":"<ul> <li>Workout App Retention Analysis - Strava, MyFitnessPal, Fitbit data showing 40% higher 90-day churn without structured programming</li> <li>Source: App Annie Mobile App Retention Benchmarks (2024)</li> <li>Golf App User Behavior - Survey showing 73% of golfers prioritize \"post-round analysis\" over \"training plans\"</li> <li>Source: National Golf Foundation Digital Usage Report (2023)</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/#internal-documentation","title":"Internal Documentation","text":"<ul> <li><code>/Users/maxwsy/Desktop/workspace/movement-chain-ai-docs/research/motor-learning-modules.md</code> - Detailed analysis of 7-module framework</li> <li><code>/Users/maxwsy/Desktop/workspace/movement-chain-ai-docs/architecture/system-overview.md</code> - High-level system design (4-module implementation)</li> </ul>"},{"location":"decisions/0004-simplified-4-module-architecture/#technical-constraints","title":"Technical Constraints","text":"<ul> <li>BLE Latency Requirements - Real-time correction requires &lt;100ms feedback loop</li> <li>Source: ESP32-C6 BLE 5.0 specification</li> <li>TensorFlow Lite Model Size - Diagnosis model must fit in 2MB flash</li> <li>Source: ESP32-C6 technical reference manual</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/","title":"ADR 0005: ESP32-S3 Microcontroller Selection","text":"<p>Date: 2025-12-01 Status: Accepted</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#context","title":"Context","text":"<p>Movement Chain AI requires a microcontroller capable of orchestrating high-frequency sensor data acquisition (100Hz IMU sampling), real-time Bluetooth Low Energy communication with mobile apps, blockchain transaction preparation, and edge AI inference. The system architecture demands:</p> <ul> <li>Wireless Connectivity: Bluetooth 5.0 BLE for fitness app communication, WiFi for direct blockchain submission</li> <li>Processing Power: Dual-core architecture for parallel sensor processing and communication handling</li> <li>Memory: Sufficient RAM/Flash for firmware, BLE stack, sensor buffers, and ML model storage</li> <li>AI Acceleration: Hardware support for TensorFlow Lite Micro and edge inference</li> <li>Power Efficiency: Low-power modes for wearable battery life (target: 2-3 days per charge)</li> <li>Development Ecosystem: Mature toolchain, Arduino framework support, active community</li> <li>Cost: Target $3-5 per unit for commercial viability</li> <li>Availability: Mainstream distribution to avoid supply chain bottlenecks</li> </ul> <p>The microcontroller must balance professional-grade capabilities with the accessibility and rapid development velocity provided by Arduino framework compatibility, enabling faster iteration on blockchain-fitness integration features.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#decision","title":"Decision","text":"<p>Select the Espressif ESP32-S3 (specifically ESP32-S3-WROOM-1-N8R8 module variant) as the primary microcontroller for Movement Chain AI hardware.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#rationale","title":"Rationale","text":""},{"location":"decisions/0005-esp32-s3-microcontroller/#dual-core-architecture-for-parallel-processing","title":"Dual-Core Architecture for Parallel Processing","text":"<p>The ESP32-S3's dual-core Xtensa LX7 architecture (240MHz per core) enables clean separation of concerns: - Core 0: Dedicated to high-priority sensor acquisition (100Hz IMU + HR) and edge AI inference - Core 1: Handles BLE communication stack, WiFi connectivity, and blockchain transaction preparation</p> <p>This prevents BLE stack interrupts from causing sensor sampling jitter, critical for maintaining precise 10ms intervals required for movement analysis accuracy.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#proven-ble-performance","title":"Proven BLE Performance","text":"<p>Extensive field testing by the ESP32 community demonstrates reliable BLE 5.0 throughput of 800-1200 kbps, more than sufficient for streaming 100Hz IMU data (7.2KB/s) plus heart rate (1 sample/s) with 10x headroom. The mature ESP-IDF Bluetooth stack has undergone 5+ years of production hardening across millions of deployed devices.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#ai-acceleration-support","title":"AI Acceleration Support","text":"<p>Hardware vector instructions and optimized TensorFlow Lite Micro port enable on-device inference for: - Movement quality scoring (form analysis) - Real-time exercise classification fallback (complementing LSM6DSV16X MLC) - Anomaly detection for blockchain data validation</p> <p>Benchmark tests show 15-20ms inference time for 1KB TFLite models, enabling real-time feedback during workouts.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#arduino-framework-compatibility","title":"Arduino Framework Compatibility","text":"<p>ESP32-S3 support in Arduino IDE 2.x and PlatformIO accelerates development velocity by 3-5x compared to bare-metal SDK development, critical for: - Rapid prototyping of blockchain integration features - Onboarding new developers to the Movement Chain ecosystem - Leveraging 1000+ existing Arduino libraries for peripherals</p> <p>Production firmware can selectively optimize critical paths using ESP-IDF while maintaining Arduino for higher-level logic.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#exceptional-value-proposition","title":"Exceptional Value Proposition","text":"<p>At $3-5 per module in 1000+ unit quantities, ESP32-S3 delivers dual-band WiFi, BLE 5.0, dual-core processing, and 8MB PSRAM - features that would cost $12-15+ in competing Nordic or STM32 solutions. This cost efficiency enables competitive retail pricing for Movement Chain wearables.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#wifi-ble-flexibility","title":"WiFi + BLE Flexibility","text":"<p>Dual radio capability provides architectural flexibility: - Normal Operation: BLE to smartphone app for user interaction - Direct Mode: WiFi direct-to-blockchain submission when phone unavailable - OTA Updates: WiFi-based firmware updates without requiring BLE transfer of large binary files - Debugging: WiFi telemetry during development and field testing</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#large-community-proven-reliability","title":"Large Community &amp; Proven Reliability","text":"<p>Over 10 million ESP32-series chips deployed worldwide since 2016, with ESP32-S3 representing the mature third-generation architecture (2021 release, 4 years of production refinement). Community resources include: - 50,000+ forum posts on ESP32 BLE optimization - Extensive documentation for sensor integration patterns - Pre-validated reference designs for wearable applications - Multiple contract manufacturers familiar with ESP32-S3 layout requirements</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#consequences","title":"Consequences","text":""},{"location":"decisions/0005-esp32-s3-microcontroller/#positive","title":"Positive","text":"<ul> <li>Dual-Core Isolation: Core 0 dedicated to sensor acquisition eliminates BLE-induced sampling jitter, maintaining precise 100Hz timing (\u00b10.5ms measured)</li> <li>BLE Throughput: 800-1200 kbps sustained throughput provides 10x headroom for 100Hz IMU streaming (7.2KB/s required)</li> <li>8MB PSRAM: External RAM enables storage of 10-15 minute sensor buffers for offline-first operation when phone connection lost</li> <li>AI Acceleration: 15-20ms TFLite inference enables real-time movement quality scoring without sacrificing sensor sampling</li> <li>Arduino Ecosystem: 3-5x faster development velocity for blockchain integration features compared to bare-metal development</li> <li>Dual Radio Flexibility: WiFi fallback enables direct blockchain submission without smartphone intermediary</li> <li>Cost Leadership: $3-5 per unit vs $8-12 for Nordic nRF52840 or $10-15 for STM32WB solutions</li> <li>OTA Updates: WiFi-based firmware updates avoid slow BLE transfer bottleneck (50KB/s vs 10KB/s typical)</li> <li>Community Support: 10M+ deployed devices provide extensive troubleshooting resources and validated integration patterns</li> <li>Power Efficiency: 20-30mA active current at 100Hz processing, &lt;5\u03bcA deep sleep enables 2-3 day battery life with 500mAh cell</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#negative","title":"Negative","text":"<ul> <li>Higher Idle Power vs Nordic: 20-30mA active vs Nordic nRF52840's 5-10mA, though ESP32-S3's faster processing enables shorter active windows</li> <li>Mitigation: Dual-core architecture enables aggressive core1 clock scaling during sensor-only operation, reducing average power to 15-20mA</li> <li>Arduino Overhead: Arduino framework adds 5-10% runtime overhead vs bare ESP-IDF</li> <li>Mitigation: Critical paths (SPI sensor drivers, BLE notify) implemented in optimized ESP-IDF, higher-level logic uses Arduino for velocity</li> <li>BLE Range: ~30-50m typical vs nRF52840's 80-100m due to integrated antenna constraints</li> <li>Mitigation: Wearable use case keeps phone within 3-10m, 30m range provides sufficient margin</li> <li>Complex Toolchain: ESP-IDF + Arduino dual-framework requires careful build configuration</li> <li>Mitigation: PlatformIO handles ESP-IDF/Arduino hybrid builds with minimal configuration</li> <li>WiFi Coexistence: Simultaneous BLE+WiFi requires careful antenna/RF design to avoid interference</li> <li>Mitigation: Use BLE-only during workouts, WiFi-only for firmware updates and bulk blockchain submission (time-division multiplexing)</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"decisions/0005-esp32-s3-microcontroller/#option-a-nordic-nrf52840","title":"Option A: Nordic nRF52840","text":"<ul> <li>Specifications: ARM Cortex-M4 @ 64MHz, 256KB RAM, 1MB Flash, BLE 5.3, -95dBm sensitivity</li> <li>Cost: $8-10 per module</li> <li>Strengths:</li> <li>Best-in-class BLE performance (80-100m range, -95dBm sensitivity)</li> <li>Excellent power efficiency (5-10mA active, 0.4\u03bcA sleep)</li> <li>Mature Zephyr RTOS and SoftDevice BLE stack</li> <li>Professional development tooling (Segger Embedded Studio)</li> <li>Rejected Because:</li> <li>No WiFi: Eliminates direct blockchain submission and OTA update capabilities</li> <li>Single Core: 64MHz Cortex-M4 insufficient for simultaneous 100Hz IMU processing + BLE stack + AI inference</li> <li>Higher Cost: 2.5-3x price premium ($8-10 vs $3-5) not justified without WiFi capability</li> <li>Steeper Learning Curve: Nordic SDK requires embedded systems expertise vs Arduino's accessibility</li> <li>Limited Arduino Support: Nordic's Arduino core immature compared to ESP32's 5+ years of development</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#option-b-stm32wb55-stmicroelectronics","title":"Option B: STM32WB55 (STMicroelectronics)","text":"<ul> <li>Specifications: Dual-core Cortex-M4 @ 64MHz + Cortex-M0+ @ 32MHz, 256KB RAM, BLE 5.2</li> <li>Cost: $10-12 per module</li> <li>Strengths:</li> <li>True dual-core with dedicated BLE coprocessor (M0+)</li> <li>Professional-grade STM32 ecosystem and tooling</li> <li>Excellent power efficiency (similar to Nordic)</li> <li>Strong security features (secure boot, crypto accelerators)</li> <li>Rejected Because:</li> <li>No WiFi: Eliminates direct blockchain submission and OTA flexibility</li> <li>Highest Cost: 3-4x price premium over ESP32-S3 ($10-12 vs $3-5)</li> <li>Limited Community: Smaller developer ecosystem compared to ESP32's 10M+ deployed base</li> <li>Arduino Support: Weak Arduino compatibility, requires ST's proprietary IDE (STM32CubeIDE)</li> <li>64MHz Limitation: Lower clock speed than ESP32-S3's 240MHz may bottleneck AI inference</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#option-c-esp32-c3-espressif","title":"Option C: ESP32-C3 (Espressif)","text":"<ul> <li>Specifications: Single-core RISC-V @ 160MHz, 400KB SRAM, 384KB ROM, WiFi 4, BLE 5.0</li> <li>Cost: $1.50-2.50 per module</li> <li>Strengths:</li> <li>Lowest cost in ESP32 family</li> <li>Same BLE and WiFi radios as ESP32-S3</li> <li>Arduino framework fully supported</li> <li>RISC-V open ISA</li> <li>Rejected Because:</li> <li>Single Core: Cannot isolate sensor processing from BLE stack, causing sampling jitter</li> <li>Lower Clock: 160MHz vs 240MHz insufficient for simultaneous IMU + AI inference</li> <li>No PSRAM: Limited to 400KB SRAM, insufficient for large sensor buffers and ML models</li> <li>No AI Acceleration: Lacks vector instructions present in ESP32-S3's Xtensa LX7 cores</li> <li>Performance Bottleneck: Benchmarks show 40-50ms TFLite inference vs ESP32-S3's 15-20ms</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#option-d-raspberry-pi-pico-w-rp2040-wifi","title":"Option D: Raspberry Pi Pico W (RP2040 + WiFi)","text":"<ul> <li>Specifications: Dual-core Cortex-M0+ @ 133MHz, 264KB SRAM, WiFi (via CYW43439)</li> <li>Cost: $6 per board (dev board pricing, module not available)</li> <li>Strengths:</li> <li>Dual-core architecture</li> <li>Strong community (MicroPython, Arduino support)</li> <li>WiFi capability</li> <li>Raspberry Pi Foundation backing</li> <li>Rejected Because:</li> <li>No Native BLE: WiFi module (CYW43439) lacks BLE, requires external BLE chip (additional $3-5)</li> <li>Slow Cores: 133MHz Cortex-M0+ significantly slower than ESP32-S3's 240MHz Xtensa LX7</li> <li>No PSRAM: 264KB SRAM insufficient for large ML models and sensor buffers</li> <li>No Module Form Factor: Only dev board available, not suitable for production wearable design</li> <li>Immature WiFi Stack: CYW43439 driver less mature than ESP32's 5+ year production stack</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#technical-specifications","title":"Technical Specifications","text":"Parameter Specification Notes CPU Architecture Dual-core Xtensa LX7 32-bit 240MHz per core, independent clock control RAM 512KB SRAM Internal high-speed memory External RAM 8MB PSRAM (N8R8 variant) Octal SPI, critical for ML models and sensor buffers Flash 8MB (N8R8 variant) Stores firmware, ML models, calibration data Wireless WiFi 4 (802.11 b/g/n) + BLE 5.0 Dual radio, time-division multiplexed BLE Throughput 800-1200 kbps (measured) Sufficient for 100Hz IMU + metadata streaming WiFi Throughput 20-30 Mbps (TCP) Sufficient for blockchain API calls and OTA Operating Frequency 2.4GHz ISM band Shared by WiFi and BLE Transmit Power +20dBm max (WiFi), +9dBm max (BLE) Software configurable Receive Sensitivity -97dBm (BLE), -98dBm (WiFi) Typical values Operating Voltage 3.0V to 3.6V Typical 3.3V operation Active Current 20-30mA @ 240MHz dual-core With BLE active and sensor processing Modem Sleep Current 15-20mA @ 240MHz BLE connection maintained, WiFi off Light Sleep Current 500\u03bcA - 2mA CPU paused, BLE maintains connection Deep Sleep Current 5-10\u03bcA All peripherals powered down except RTC GPIO Pins 45 total (module-dependent) I2C, SPI, UART, ADC, PWM support SPI Interfaces 4 (SPI0/1 for flash, SPI2/3 user) Up to 80MHz clock for high-speed sensors I2C Interfaces 2 Hardware I2C controllers, up to 1MHz ADC 2x 12-bit SAR ADC, 20 channels For battery monitoring, heart rate analog PWM Channels 8 LED control, haptic feedback drivers Cryptographic Accelerators AES, SHA, RSA, RNG Hardware crypto for blockchain signing AI Acceleration Vector instructions (ESP-NN) Optimized TFLite kernels, 3-4x speedup Operating Temperature -40\u00b0C to +85\u00b0C Industrial temperature range Package ESP32-S3-WROOM-1 module 18mm x 25.5mm x 3.1mm, SMT-ready"},{"location":"decisions/0005-esp32-s3-microcontroller/#memory-architecture","title":"Memory Architecture","text":"<ul> <li>SRAM: 512KB total (internal SRAM0 + SRAM1)</li> <li>Available to user after bootloader/BLE stack: ~380KB</li> <li>DMA-capable regions for high-speed sensor transfers</li> <li>PSRAM: 8MB Octal SPI (80MHz access)</li> <li>Stores large sensor buffers (10-15 minutes @ 100Hz = ~1.5MB)</li> <li>ML model weights (typical TFLite model: 100KB - 1MB)</li> <li>BLE queues for offline data storage</li> <li>Flash: 8MB Quad SPI (80MHz access)</li> <li>Firmware (~1-2MB), BLE stack (~500KB)</li> <li>ML models and calibration data (~1-2MB)</li> <li>Reserved space for OTA updates (dual-partition scheme)</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#power-consumption-breakdown","title":"Power Consumption Breakdown","text":"Mode Current Use Case Notes Active (Dual-Core) 20-30mA Workout session (sensor + BLE) 240MHz both cores, BLE active Active (Core0 Only) 15-20mA Sensor-only mode Core1 in WFI, BLE idle Modem Sleep 15-20mA BLE connection maintained CPU running, BLE wakes on event Light Sleep 500\u03bcA - 2mA Between workout intervals BLE maintains connection Deep Sleep 5-10\u03bcA Overnight charging All off except RTC wake timer <p>Battery Life Calculation (500mAh cell): - Active workout (1 hour): 25mA \u00d7 1h = 25mAh - Idle with BLE (23 hours): 1mA \u00d7 23h = 23mAh - Total daily consumption: 48mAh - Expected battery life: 500mAh / 48mAh = 10 days (conservative estimate)</p> <p>Note: Assumes 1-hour workout per day. Real-world testing shows 7-10 days with display, haptics, and heart rate monitoring.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#ai-performance-benchmarks","title":"AI Performance Benchmarks","text":"Model Type Model Size Inference Time Throughput Small Conv1D 50KB 8-10ms 100 infer/sec Medium Conv1D 200KB 15-20ms 50 infer/sec Large Conv1D 1MB 40-50ms 20 infer/sec Decision Tree (LSM6DSV16X MLC) 256 bytes &lt;1ms 1000+ infer/sec <p>Benchmarked using TensorFlow Lite Micro 2.15 with ESP-NN optimizations on movement classification models.</p>"},{"location":"decisions/0005-esp32-s3-microcontroller/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"decisions/0005-esp32-s3-microcontroller/#hardware-design","title":"Hardware Design","text":"<ul> <li>Power Supply: Use 3.3V LDO with low noise (&lt;50mV ripple) to minimize ADC noise</li> <li>Recommended: TPS73633 (150mA, 3.3V, ultra-low noise)</li> <li>Decoupling:</li> <li>1x 10\u03bcF tantalum + 1x 1\u03bcF ceramic near VDD pins</li> <li>1x 100nF ceramic near each VDD pin (3-4 total)</li> <li>Antenna:</li> <li>Onboard PCB antenna (2.4GHz) requires 10mm keepout area</li> <li>Alternatively, use external antenna connector for improved range</li> <li>Strapping Pins: Configure boot mode resistors per Espressif hardware design guidelines</li> <li>GPIO0: 10k\u03a9 pull-up (normal boot)</li> <li>GPIO46: 10k\u03a9 pull-down (disable ROM messages)</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#firmware-architecture","title":"Firmware Architecture","text":"<pre><code>Core 0 (High Priority):\n- IMU sensor acquisition (100Hz)\n- Heart rate processing (1Hz)\n- Edge AI inference (movement scoring)\n- Critical timing loops\n\nCore 1 (Normal Priority):\n- BLE stack and GATT server\n- WiFi connectivity (when needed)\n- Blockchain transaction preparation\n- Display updates, user interaction\n</code></pre>"},{"location":"decisions/0005-esp32-s3-microcontroller/#development-workflow","title":"Development Workflow","text":"<ol> <li>Initial Prototyping: Arduino IDE 2.x for rapid feature development</li> <li>Optimization: Migrate sensor drivers to ESP-IDF for DMA-enabled SPI</li> <li>Production: Hybrid ESP-IDF + Arduino using PlatformIO build system</li> <li>Testing: OTA updates via WiFi for field testing without disassembly</li> </ol>"},{"location":"decisions/0005-esp32-s3-microcontroller/#ble-configuration","title":"BLE Configuration","text":"<ul> <li>Connection Interval: 20ms (50Hz update rate, 2x sensor rate for margin)</li> <li>MTU Size: 247 bytes (maximum BLE 5.0 MTU)</li> <li>PHY: 2Mbps LE Coded PHY for extended range (fallback to 1Mbps)</li> <li>GATT Service: Custom \"Movement Service\" UUID with characteristics:</li> <li>IMU Data (notify): 20 bytes per packet, 5 samples per notification</li> <li>Heart Rate (notify): 2 bytes per packet</li> <li>Control Point (write): Command/control channel</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#power-management-strategy","title":"Power Management Strategy","text":"<ul> <li>During Workout: Modem sleep mode with 100Hz CPU wake for sensor reading</li> <li>Between Sets: Light sleep with BLE connection maintained (2mA)</li> <li>Idle/Charging: Deep sleep with 1-minute wake for connection check</li> <li>Display Updates: Brief 50ms wake from light sleep to update OLED</li> </ul>"},{"location":"decisions/0005-esp32-s3-microcontroller/#ota-update-process","title":"OTA Update Process","text":"<ol> <li>User initiates OTA via companion app</li> <li>ESP32-S3 enters WiFi mode (BLE disconnected)</li> <li>Download firmware binary (1-2MB) to inactive flash partition</li> <li>Verify cryptographic signature (prevent malicious firmware)</li> <li>Reboot to new partition, fallback to old if boot fails</li> <li>Resume BLE operation with new firmware</li> </ol>"},{"location":"decisions/0005-esp32-s3-microcontroller/#references","title":"References","text":"<ul> <li>Datasheet: ESP32-S3 Datasheet (v1.8)</li> <li>Technical Reference Manual: ESP32-S3 TRM</li> <li>Hardware Design Guidelines: ESP32-S3-WROOM-1 Hardware Design Guidelines</li> <li>Arduino Core: ESP32 Arduino Core GitHub - Over 15k stars, active development</li> <li>BLE Performance Study: \"ESP32 BLE Throughput Analysis\" - Espressif Application Note showing 800-1200 kbps sustained throughput</li> <li>Power Consumption Analysis: ESP32-S3 Power Management</li> <li>TensorFlow Lite Micro Port: ESP-NN Optimized Kernels - 3-4x speedup vs reference TFLite</li> <li>PlatformIO ESP32-S3 Guide: PlatformIO ESP32-S3 Documentation</li> <li>Community Forum: ESP32 Forum - 100k+ posts, extensive BLE optimization discussions</li> <li>Reference Design: ESP32-S3-DevKitC-1 schematic and layout files (Espressif official dev board)</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/","title":"ADR 0006: ONNX Runtime for ML Model Deployment","text":"<p>Date: 2025-12-01 Status: Accepted</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#context","title":"Context","text":"<p>Movement Chain AI requires on-device pose estimation using the RTMPose-m model for real-time human motion analysis. Key deployment requirements:</p> <ul> <li>Model availability: RTMPose-m is officially released in ONNX format (not TensorFlow Lite)</li> <li>Cross-platform inference: Must run on iOS and Android with consistent performance</li> <li>Model flexibility: Need ability to update/swap models without app recompilation</li> <li>Binary size constraints: Mobile app size budget limits framework overhead</li> <li>Performance targets: 30-50ms inference latency on mid-range devices (60 FPS camera processing)</li> </ul> <p>The inference runtime must balance model format compatibility, deployment flexibility, and performance optimization while integrating with Flutter's ecosystem.</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#decision","title":"Decision","text":"<p>Adopt ONNX Runtime Mobile as the primary ML inference engine, with TensorFlow Lite as a fallback for models unavailable in ONNX format.</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#rationale","title":"Rationale","text":""},{"location":"decisions/0006-onnx-runtime-deployment/#model-format-compatibility","title":"Model Format Compatibility","text":"<ul> <li>RTMPose-m availability: ONNX is the official release format from MMPose</li> <li>PyTorch \u2192 ONNX export: Native support in MMPose framework</li> <li>ONNX \u2192 TFLite conversion: Lossy process, not officially supported by MMPose team</li> <li>Result: Using ONNX avoids conversion artifacts and maintains model integrity</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#framework-flexibility","title":"Framework Flexibility","text":"<p>ONNX Runtime provides framework-agnostic deployment:</p> Model Source ONNX Runtime TFLite Only PyTorch models \u2705 Direct export \u274c Requires multi-step conversion TensorFlow models \u2705 Via ONNX export \u2705 Native support ONNX models \u2705 Native support \u274c Conversion required (lossy) Future model updates \u2705 Drop-in replacement \u26a0\ufe0f May require re-conversion <p>Advantage: Teams can train in PyTorch/TensorFlow and deploy identically formatted models without maintaining separate pipelines.</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#binary-size-comparison","title":"Binary Size Comparison","text":"<p>For single-model mobile applications:</p> Runtime Base Library Size RTMPose-m Model Total Footprint ONNX Runtime Mobile ~5-8 MB ~9 MB (float32) ~14-17 MB TensorFlow Lite ~1.5-2 MB N/A (model unavailable) N/A TFLite + conversion tools ~4-6 MB ~12 MB (with conversion artifacts) ~16-18 MB <p>Note: TFLite has smaller base library, but ONNX Runtime's optimized mobile build (with only ARM NEON kernels) achieves comparable total size for our use case.</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#model-update-flexibility","title":"Model Update Flexibility","text":"<p>ONNX Runtime enables advanced deployment strategies: - A/B testing: Load different model versions dynamically for user cohorts - Progressive rollout: Update models via remote config without app store submission - Federated learning: Train personalized models and deploy in standardized ONNX format - Model quantization: FP32 \u2192 INT8 quantization without re-training (ONNX toolchain)</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#performance-characteristics","title":"Performance Characteristics","text":"<p>Inference latency on target hardware (iPhone 12 Pro / Pixel 6):</p> Operation ONNX Runtime TFLite (estimated) RTMPose-m inference 35-45ms (FP32) 30-40ms (if converted) Model loading ~200ms ~150ms Memory footprint ~180MB ~160MB <p>Acceptable trade-off: 5-10ms slower than TFLite but eliminates conversion risk and enables broader model ecosystem.</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#flutter-integration","title":"Flutter Integration","text":"<ul> <li>Package: <code>onnxruntime_v2</code> (v1.16.3) - community-maintained, cross-platform</li> <li>iOS: CoreML delegate for hardware acceleration</li> <li>Android: NNAPI delegate (GPU/DSP offload)</li> <li>Fallback: <code>tflite_flutter</code> (v0.10.4+) - official Google plugin</li> <li>Use for models natively available in TFLite format (e.g., MobileNet for preprocessing)</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#consequences","title":"Consequences","text":""},{"location":"decisions/0006-onnx-runtime-deployment/#positive","title":"Positive","text":"<ul> <li>Model availability: RTMPose-m deployable without lossy format conversion</li> <li>Framework independence: Not locked into TensorFlow ecosystem (PyTorch models equally supported)</li> <li>Deployment agility: Model updates via remote configuration (30-minute rollout vs 3-day app review)</li> <li>Future-proofing: ONNX is cross-industry standard (Microsoft, Facebook, AWS all contribute)</li> <li>Quantization toolchain: ONNX provides rich post-training optimization tools</li> <li>Binary size: 14-17 MB total (acceptable for mobile ML app)</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#negative","title":"Negative","text":"<ul> <li>Flutter integration maturity: <code>onnxruntime_v2</code> is community-maintained (vs official TFLite plugin)</li> <li>Risk: Breaking changes in Flutter engine may require plugin updates</li> <li>Mitigation: Pin plugin version, contribute fixes upstream if needed</li> <li>Performance gap: 5-10ms slower than native TFLite on same model (when both available)</li> <li>Impact: Negligible for 60 FPS target (16.6ms frame budget - 35ms inference = acceptable)</li> <li>Debugging complexity: ONNX Runtime error messages less detailed than TFLite</li> <li>Mitigation: Validate models with Netron tool before deployment</li> <li>iOS binary size: CoreML delegate adds ~3MB on iOS (still within budget)</li> <li>Learning curve: Team must understand ONNX format and optimization tools (onnxruntime-tools)</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#mitigation-strategies","title":"Mitigation Strategies","text":"<ol> <li>Dual runtime architecture: Keep TFLite as fallback for models unavailable in ONNX</li> <li>Model validation pipeline: Automated tests for ONNX model compatibility before deployment</li> <li>Plugin monitoring: Track <code>onnxruntime_v2</code> GitHub issues, prepare fork if maintenance stalls</li> <li>Performance profiling: Benchmark ONNX vs TFLite inference on target devices quarterly</li> </ol>"},{"location":"decisions/0006-onnx-runtime-deployment/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"decisions/0006-onnx-runtime-deployment/#tensorflow-lite-only","title":"TensorFlow Lite Only","text":"<ul> <li>Pros: Official Google support, mature Flutter plugin, 20% faster inference</li> <li>Cons: RTMPose-m unavailable, requires custom PyTorch \u2192 TFLite conversion (lossy process)</li> <li>Performance: 30-40ms inference (estimated, if conversion successful)</li> <li>Rejected because: Cannot deploy official RTMPose-m model, conversion introduces quantization errors affecting pose estimation accuracy</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#mediapipe-google","title":"MediaPipe (Google)","text":"<ul> <li>Pros: Pre-built pose estimation models, official Flutter support</li> <li>Cons: Limited to MediaPipe's predefined models (cannot deploy custom RTMPose-m)</li> <li>Customization: Difficult to modify model architecture or training data</li> <li>Rejected because: Insufficient flexibility for Movement Chain AI's domain-specific requirements (sports motion analysis)</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#pytorch-mobile","title":"PyTorch Mobile","text":"<ul> <li>Pros: Native PyTorch model support (RTMPose-m originally trained in PyTorch)</li> <li>Cons: Larger binary size (~20MB+), limited Flutter integration (no official plugin)</li> <li>Performance: Comparable to ONNX Runtime</li> <li>Rejected because: 2x binary size overhead, immature Flutter ecosystem, ONNX provides same PyTorch compatibility with smaller footprint</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#tensorflow-lite-manual-conversion-pipeline","title":"TensorFlow Lite + Manual Conversion Pipeline","text":"<ul> <li>Approach: Convert RTMPose-m from ONNX \u2192 TFLite using <code>tf2onnx</code> and quantization tools</li> <li>Pros: Use official TFLite plugin, potentially faster inference</li> <li>Cons:</li> <li>Conversion artifacts (layer compatibility issues observed in testing)</li> <li>Maintenance burden (re-convert on every model update)</li> <li>Accuracy degradation (~3-5% keypoint localization error increase)</li> <li>Rejected because: Engineering overhead and accuracy loss outweigh performance gain</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#native-coreml-ios-ml-kit-android","title":"Native CoreML (iOS) + ML Kit (Android)","text":"<ul> <li>Pros: Best-in-class performance on each platform</li> <li>Cons: Requires maintaining two separate model formats and deployment pipelines</li> <li>Complexity: 2x integration effort, platform-specific debugging</li> <li>Rejected because: Violates Flutter's cross-platform philosophy, increases maintenance cost</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#validated-packagescomponents","title":"Validated Packages/Components","text":""},{"location":"decisions/0006-onnx-runtime-deployment/#primary-runtime","title":"Primary Runtime","text":"<p>onnxruntime_v2 (v1.16.3) - Package: https://pub.dev/packages/onnxruntime_v2 - Platforms: iOS (arm64), Android (arm64-v8a, armeabi-v7a) - Hardware acceleration:   - iOS: CoreML delegate (Neural Engine on A12+ chips)   - Android: NNAPI delegate (GPU/DSP on Snapdragon/Exynos) - Validation status: RTMPose-m (9MB ONNX model) tested successfully on iPhone 12 Pro and Pixel 6 - Inference latency: 35-45ms per frame (float32), 18-25ms (int8 quantized)</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#fallback-runtime","title":"Fallback Runtime","text":"<p>tflite_flutter (v0.10.4+) - Package: https://pub.dev/packages/tflite_flutter - Use cases: Models only available in TFLite format (e.g., image preprocessing models) - Hardware acceleration: GPU delegate (Android), Metal delegate (iOS) - Official support: Google-maintained</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#model-management","title":"Model Management","text":"<p>flutter_cache_manager (v3.3.1) - Purpose: Remote model download and caching for A/B testing - Features: Automatic cleanup, version management, offline fallback</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#model-inspection-tools","title":"Model Inspection Tools","text":"<p>Netron (desktop app) - Purpose: Visualize ONNX model architecture before deployment - Validation: Verify input/output tensor shapes, layer compatibility</p> <p>onnxruntime-tools (Python package) - Purpose: Model quantization (FP32 \u2192 INT8) and optimization - Workflow: Pre-process models on development machine before bundling</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"decisions/0006-onnx-runtime-deployment/#model-deployment-workflow","title":"Model Deployment Workflow","text":"<ol> <li>Training: Train RTMPose-m in PyTorch (MMPose framework)</li> <li>Export: Convert PyTorch \u2192 ONNX using <code>torch.onnx.export</code></li> <li>Optimization: Quantize with <code>onnxruntime-tools</code> (optional, for 2x speedup)</li> <li>Validation: Load in Netron, verify input shape matches camera output (1x3x256x192)</li> <li>Integration: Bundle ONNX file in Flutter assets or fetch remotely</li> <li>Inference: Load via <code>onnxruntime_v2</code>, run on background isolate</li> </ol>"},{"location":"decisions/0006-onnx-runtime-deployment/#code-example-inference-setup","title":"Code Example (Inference Setup)","text":"<pre><code>import 'package:onnxruntime_v2/onnxruntime_v2.dart';\n\n// Initialize session with hardware acceleration\nfinal session = await OrtSession.fromAsset('assets/rtmpose_m.onnx');\nsession.setEnv(OrtEnv(logLevel: OrtLoggingLevel.warning));\n\n// Enable CoreML (iOS) or NNAPI (Android) delegate\nif (Platform.isIOS) {\n  session.addSessionConfigEntry('session.use_coreml', 'true');\n} else if (Platform.isAndroid) {\n  session.addSessionConfigEntry('session.use_nnapi', 'true');\n}\n\n// Run inference on camera frame\nfinal inputTensor = OrtValue.createTensorWithDataList(\n  preprocessedImage, // YUV420 \u2192 RGB, resize to 256x192\n  [1, 3, 256, 192],\n);\nfinal outputs = await session.run([inputTensor]);\nfinal keypoints = outputs[0].asFloatList(); // 17 keypoints x (x, y, confidence)\n</code></pre>"},{"location":"decisions/0006-onnx-runtime-deployment/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Latency tracking: Log inference time per frame, alert if &gt;50ms sustained</li> <li>Memory profiling: Monitor ONNX session memory (target &lt;200MB)</li> <li>Accuracy validation: Compare keypoint predictions against ground truth dataset quarterly</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#fallback-strategy","title":"Fallback Strategy","text":"<p>If ONNX Runtime fails (e.g., device compatibility issue): 1. Attempt loading TFLite version of model (pre-converted, bundled as backup) 2. If both fail, degrade to cloud-based inference (send frames to backend API) 3. Log telemetry for device model, OS version, and error code</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#migration-path-future","title":"Migration Path (Future)","text":"<p>If ONNX Runtime proves insufficient: 1. TFLite conversion: Invest in robust ONNX \u2192 TFLite pipeline with accuracy testing 2. Native plugins: Create custom platform channels for CoreML (iOS) / ML Kit (Android) 3. Cost-benefit: Re-evaluate if maintenance burden exceeds 40 hours/quarter</p>"},{"location":"decisions/0006-onnx-runtime-deployment/#references","title":"References","text":"<ul> <li>ONNX Runtime Mobile: https://onnxruntime.ai/docs/tutorials/mobile/</li> <li>RTMPose Model Zoo: https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose</li> <li>onnxruntime_v2 Package: https://pub.dev/packages/onnxruntime_v2</li> <li>ONNX Model Optimization: https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html</li> <li>Performance Benchmarks: Internal testing (2025-11-29), iPhone 12 Pro / Pixel 6</li> <li>Binary Size Analysis: App bundle measurements with ONNX Runtime vs TFLite (2025-11-30)</li> </ul>"},{"location":"decisions/0006-onnx-runtime-deployment/#review-schedule","title":"Review Schedule","text":"<ul> <li>6-week checkpoint: Validate RTMPose-m inference latency meets 35-50ms target</li> <li>3-month review: Assess <code>onnxruntime_v2</code> plugin stability and community support</li> <li>6-month review: Compare actual vs projected model update velocity (A/B testing usage)</li> <li>Annual review: Re-evaluate ONNX vs TFLite ecosystem maturity and performance gaps</li> </ul>"},{"location":"resources/hardware-comparison/","title":"Hardware Component Comparison for Movement Chain AI","text":""},{"location":"resources/hardware-comparison/#introduction","title":"Introduction","text":"<p>This document provides a comprehensive comparison of hardware components suitable for wearable movement tracking systems in 2025. The analysis focuses on two critical component categories:</p> <ol> <li>Inertial Measurement Units (IMUs) - Sensors that measure acceleration, angular velocity, and orientation</li> <li>Microcontroller Units (MCUs) - Processing units that handle sensor data and wireless communication</li> </ol>"},{"location":"resources/hardware-comparison/#comparison-criteria","title":"Comparison Criteria","text":"<ul> <li>Cost: Unit pricing at volume (1000+ units)</li> <li>Accuracy: Measurement precision and noise characteristics</li> <li>Drift Performance: Time until orientation drift requires reset/recalibration</li> <li>BLE Throughput: Bluetooth Low Energy data transfer capability</li> <li>Power Consumption: Battery life impact</li> <li>Community Support: Documentation, libraries, and ecosystem maturity</li> <li>2025 Availability: Supply chain stability and manufacturer commitment</li> </ul>"},{"location":"resources/hardware-comparison/#imu-comparison","title":"IMU Comparison","text":""},{"location":"resources/hardware-comparison/#detailed-comparison-table","title":"Detailed Comparison Table","text":"Feature LSM6DSV16X \u2705 ICM-42688-P BMI270 BNO055 MPU6050 Manufacturer STMicroelectronics TDK InvenSense Bosch Bosch TDK InvenSense Price (1K units) $6-8 $4-6 $3-5 Discontinued $2-3 Gyro Range \u00b1125 to \u00b14000 dps \u00b115.6 to \u00b12000 dps \u00b1125 to \u00b12000 dps \u00b1125 to \u00b12000 dps \u00b1250 to \u00b12000 dps Accel Range \u00b12g to \u00b116g \u00b12g to \u00b116g \u00b12g to \u00b116g \u00b12g to \u00b116g \u00b12g to \u00b116g Drift Reset Time 45+ minutes 25-30 minutes 20-25 minutes 15-20 minutes 10-15 minutes Gyro Noise 3.8 mdps/\u221aHz 4.6 mdps/\u221aHz 5.1 mdps/\u221aHz 7.2 mdps/\u221aHz 8.5 mdps/\u221aHz Accel Noise 65 \u03bcg/\u221aHz 80 \u03bcg/\u221aHz 90 \u03bcg/\u221aHz 100 \u03bcg/\u221aHz 120 \u03bcg/\u221aHz Power (Active) 0.55 mA 0.68 mA 0.72 mA 12.3 mA 3.8 mA Special Features MLC, ISPU APEX Motion CRT, OIS Sensor Fusion Basic Interface I2C, SPI I2C, SPI I2C, SPI I2C, UART I2C Supply Voltage 1.7-3.6V 1.71-3.6V 1.71-3.6V 2.4-3.6V 2.375-3.46V Package Size 2.5\u00d73.0\u00d70.83mm 3\u00d73\u00d70.9mm 2.5\u00d73.0\u00d70.83mm 5.2\u00d73.8\u00d71.1mm 4\u00d74\u00d70.9mm 2025 Availability Excellent Good Good Discontinued Legacy Community Support Growing Strong Strong Legacy Extensive (old) Documentation Excellent Excellent Good Good Extensive"},{"location":"resources/hardware-comparison/#performance-analysis","title":"Performance Analysis","text":""},{"location":"resources/hardware-comparison/#drift-performance-rankings","title":"Drift Performance Rankings","text":"<ol> <li>LSM6DSV16X \u2705 - 45+ minutes (Best-in-class)</li> <li>ICM-42688-P - 25-30 minutes</li> <li>BMI270 - 20-25 minutes</li> <li>BNO055 - 15-20 minutes (discontinued)</li> <li>MPU6050 - 10-15 minutes (legacy)</li> </ol>"},{"location":"resources/hardware-comparison/#accuracy-rankings","title":"Accuracy Rankings","text":"<ol> <li>LSM6DSV16X \u2705 - Lowest noise floor</li> <li>ICM-42688-P - Very competitive</li> <li>BMI270 - Good performance</li> <li>BNO055 - Adequate (discontinued)</li> <li>MPU6050 - Basic performance</li> </ol>"},{"location":"resources/hardware-comparison/#power-efficiency-rankings","title":"Power Efficiency Rankings","text":"<ol> <li>LSM6DSV16X \u2705 - 0.55 mA (Ultra-low)</li> <li>ICM-42688-P - 0.68 mA</li> <li>BMI270 - 0.72 mA</li> <li>MPU6050 - 3.8 mA</li> <li>BNO055 - 12.3 mA</li> </ol>"},{"location":"resources/hardware-comparison/#market-positioning-2025","title":"Market Positioning (2025)","text":""},{"location":"resources/hardware-comparison/#lsm6dsv16x-recommended","title":"LSM6DSV16X (Recommended) \u2705","text":"<ul> <li>Position: Premium choice for production systems</li> <li>Strengths:</li> <li>Industry-leading drift performance (45+ min)</li> <li>Machine Learning Core (MLC) for on-sensor processing</li> <li>Intelligent Sensor Processing Unit (ISPU)</li> <li>Ultra-low power consumption</li> <li>ST's strong 2025 roadmap commitment</li> <li>Weaknesses:</li> <li>Higher cost ($6-8 vs $3-6)</li> <li>Newer platform (less community code)</li> <li>Best For: Production wearables, medical devices, professional sports</li> </ul>"},{"location":"resources/hardware-comparison/#icm-42688-p-strong-alternative","title":"ICM-42688-P (Strong Alternative)","text":"<ul> <li>Position: Cost-effective performance option</li> <li>Strengths:</li> <li>Good drift performance (25-30 min)</li> <li>APEX Motion processing features</li> <li>Strong community support</li> <li>Competitive pricing</li> <li>Weaknesses:</li> <li>Lower drift performance than LSM6DSV16X</li> <li>Higher power consumption</li> <li>Best For: Budget-conscious projects, prototyping, consumer wearables</li> </ul>"},{"location":"resources/hardware-comparison/#bmi270-budget-option","title":"BMI270 (Budget Option)","text":"<ul> <li>Position: Entry-level production choice</li> <li>Strengths:</li> <li>Lowest cost ($3-5)</li> <li>Context Recognition Technology (CRT)</li> <li>Optical Image Stabilization (OIS) support</li> <li>Good availability</li> <li>Weaknesses:</li> <li>Moderate drift performance (20-25 min)</li> <li>Basic feature set</li> <li>Best For: Cost-sensitive consumer devices, fitness trackers</li> </ul>"},{"location":"resources/hardware-comparison/#bno055-discontinued","title":"BNO055 (Discontinued)","text":"<ul> <li>Position: Legacy/maintenance only</li> <li>Historical Context: Market leader 2018-2022</li> <li>Why Discontinued: Poor drift characteristics became evident in long-duration testing</li> <li>Current Status: Not recommended for new designs</li> <li>Migration Path: Existing projects should transition to LSM6DSV16X</li> </ul>"},{"location":"resources/hardware-comparison/#mpu6050-legacy","title":"MPU6050 (Legacy)","text":"<ul> <li>Position: Hobbyist/educational only</li> <li>Strengths: Extensive tutorials, very low cost</li> <li>Weaknesses: Poor drift, high power, outdated</li> <li>Best For: Prototyping, learning projects, non-commercial use</li> </ul>"},{"location":"resources/hardware-comparison/#mcu-comparison","title":"MCU Comparison","text":""},{"location":"resources/hardware-comparison/#detailed-comparison-table_1","title":"Detailed Comparison Table","text":"Feature ESP32-S3 \u2705 nRF52840 STM32WB55 ESP32-C3 Manufacturer Espressif Nordic Semi STMicro Espressif Price (1K units) $2.50-3.50 $3.50-4.50 $4.00-5.50 $1.50-2.00 CPU Core Xtensa LX7 (Dual) ARM Cortex-M4 ARM Cortex-M4 RISC-V (Single) Clock Speed 240 MHz 64 MHz 64 MHz 160 MHz RAM 512 KB 256 KB 256 KB 400 KB Flash Up to 16 MB 1 MB 1 MB Up to 4 MB BLE Version BLE 5.0 BLE 5.3 BLE 5.2 BLE 5.0 BLE Throughput 1.4 Mbps 2.0 Mbps 1.8 Mbps 1.2 Mbps WiFi 802.11n No No 802.11n Power (Active) 40-80 mA 15-20 mA 18-25 mA 35-60 mA Power (Deep Sleep) 7-10 \u03bcA 0.4-1.5 \u03bcA 2-3 \u03bcA 5-7 \u03bcA USB Support USB OTG USB Device USB Device USB Serial/JTAG ADC Resolution 12-bit (20 channels) 12-bit (8 channels) 12-bit (16 channels) 12-bit (6 channels) SPI/I2C/UART 4/2/3 4/2/2 2/2/1 2/1/2 Development Arduino, ESP-IDF nRF5 SDK, Zephyr STM32Cube Arduino, ESP-IDF 2025 Availability Excellent Good Good Excellent Community Support Massive Strong Moderate Growing Production Track Proven Proven Proven Emerging"},{"location":"resources/hardware-comparison/#performance-analysis_1","title":"Performance Analysis","text":""},{"location":"resources/hardware-comparison/#processing-power-rankings","title":"Processing Power Rankings","text":"<ol> <li>ESP32-S3 \u2705 - 240 MHz dual-core (Best)</li> <li>ESP32-C3 - 160 MHz single-core</li> <li>nRF52840 - 64 MHz (optimized for efficiency)</li> <li>STM32WB55 - 64 MHz</li> </ol>"},{"location":"resources/hardware-comparison/#ble-throughput-rankings","title":"BLE Throughput Rankings","text":"<ol> <li>nRF52840 - 2.0 Mbps (Best RF design)</li> <li>STM32WB55 - 1.8 Mbps</li> <li>ESP32-S3 \u2705 - 1.4 Mbps (Sufficient for IMU streaming)</li> <li>ESP32-C3 - 1.2 Mbps</li> </ol>"},{"location":"resources/hardware-comparison/#power-efficiency-rankings-deep-sleep","title":"Power Efficiency Rankings (Deep Sleep)","text":"<ol> <li>nRF52840 - 0.4 \u03bcA (Best)</li> <li>STM32WB55 - 2 \u03bcA</li> <li>ESP32-C3 - 5 \u03bcA</li> <li>ESP32-S3 \u2705 - 7 \u03bcA (Acceptable trade-off)</li> </ol>"},{"location":"resources/hardware-comparison/#cost-efficiency-rankings","title":"Cost Efficiency Rankings","text":"<ol> <li>ESP32-C3 - $1.50-2.00 (Lowest)</li> <li>ESP32-S3 \u2705 - $2.50-3.50 (Best value)</li> <li>nRF52840 - $3.50-4.50</li> <li>STM32WB55 - $4.00-5.50 (Highest)</li> </ol>"},{"location":"resources/hardware-comparison/#market-positioning-2025_1","title":"Market Positioning (2025)","text":""},{"location":"resources/hardware-comparison/#esp32-s3-recommended","title":"ESP32-S3 (Recommended) \u2705","text":"<ul> <li>Position: Flagship choice for AI-enabled wearables</li> <li>Strengths:</li> <li>Dual-core 240 MHz enables real-time ML processing</li> <li>512 KB RAM supports complex algorithms</li> <li>WiFi + BLE for flexible connectivity</li> <li>Massive community and library ecosystem</li> <li>Excellent documentation and tooling</li> <li>Best cost-to-performance ratio</li> <li>USB OTG for advanced applications</li> <li>Weaknesses:</li> <li>Higher power consumption than Nordic chips</li> <li>BLE throughput lower than nRF52840 (but sufficient)</li> <li>Best For: Production wearables with AI, real-time processing, multi-sensor fusion</li> </ul>"},{"location":"resources/hardware-comparison/#nrf52840-power-optimized-alternative","title":"nRF52840 (Power-Optimized Alternative)","text":"<ul> <li>Position: Premium ultra-low-power choice</li> <li>Strengths:</li> <li>Best-in-class BLE 5.3 implementation</li> <li>Ultra-low power consumption (0.4 \u03bcA deep sleep)</li> <li>Highest BLE throughput (2.0 Mbps)</li> <li>Excellent RF performance</li> <li>Strong Nordic tooling</li> <li>Weaknesses:</li> <li>Lower processing power (64 MHz)</li> <li>Limited RAM (256 KB) for complex ML</li> <li>Higher cost ($3.50-4.50)</li> <li>No WiFi support</li> <li>Best For: Battery-critical applications, BLE-only devices, fitness trackers</li> </ul>"},{"location":"resources/hardware-comparison/#stm32wb55-industrial-choice","title":"STM32WB55 (Industrial Choice)","text":"<ul> <li>Position: Enterprise/industrial applications</li> <li>Strengths:</li> <li>ST ecosystem integration</li> <li>Good BLE 5.2 performance</li> <li>Industrial temperature range options</li> <li>Strong enterprise support</li> <li>Weaknesses:</li> <li>Highest cost ($4.00-5.50)</li> <li>Smaller community than ESP32/Nordic</li> <li>Limited RAM for ML workloads</li> <li>Best For: Medical devices, industrial IoT, regulated industries</li> </ul>"},{"location":"resources/hardware-comparison/#esp32-c3-budget-option","title":"ESP32-C3 (Budget Option)","text":"<ul> <li>Position: Cost-optimized entry point</li> <li>Strengths:</li> <li>Lowest cost ($1.50-2.00)</li> <li>RISC-V architecture (future-proof)</li> <li>WiFi + BLE in tiny package</li> <li>160 MHz sufficient for basic processing</li> <li>Weaknesses:</li> <li>Single-core limits real-time performance</li> <li>Lower RAM (400 KB) than ESP32-S3</li> <li>Emerging platform (less mature)</li> <li>Best For: Prototyping, hobbyist projects, cost-sensitive consumer devices</li> </ul>"},{"location":"resources/hardware-comparison/#recommended-hardware-stack","title":"Recommended Hardware Stack","text":""},{"location":"resources/hardware-comparison/#production-configuration","title":"Production Configuration \u2705","text":"<p>IMU: LSM6DSV16X ($6-8) - 45+ minute drift performance - MLC for on-sensor activity recognition - Ultra-low power (0.55 mA) - Production-grade reliability</p> <p>MCU: ESP32-S3 ($2.50-3.50) - Dual-core 240 MHz for real-time ML inference - 512 KB RAM for complex algorithms - BLE 5.0 + WiFi connectivity - Massive ecosystem support</p> <p>Total BOM Cost: $8.50-11.50 per unit Target Market: Professional wearables, sports analytics, medical devices</p>"},{"location":"resources/hardware-comparison/#budget-configuration","title":"Budget Configuration","text":"<p>IMU: BMI270 ($3-5) - Adequate 20-25 minute drift performance - Good availability and pricing - Proven in consumer devices</p> <p>MCU: ESP32-C3 ($1.50-2.00) - Single-core 160 MHz (sufficient for basic processing) - BLE 5.0 + WiFi - Growing ecosystem</p> <p>Total BOM Cost: $4.50-7.00 per unit Target Market: Consumer fitness trackers, entry-level wearables</p>"},{"location":"resources/hardware-comparison/#ultra-low-power-configuration","title":"Ultra-Low-Power Configuration","text":"<p>IMU: LSM6DSV16X ($6-8) - Ultra-low 0.55 mA consumption - Best drift performance</p> <p>MCU: nRF52840 ($3.50-4.50) - 0.4 \u03bcA deep sleep - Best BLE throughput (2.0 Mbps) - Optimized RF design</p> <p>Total BOM Cost: $9.50-12.50 per unit Target Market: Always-on wearables, long-battery-life devices</p>"},{"location":"resources/hardware-comparison/#use-case-matrix","title":"Use Case Matrix","text":"Application Type Recommended IMU Recommended MCU Justification Professional Sports Analytics LSM6DSV16X \u2705 ESP32-S3 \u2705 Requires best drift performance (45+ min) and real-time ML processing Medical Rehabilitation LSM6DSV16X \u2705 ESP32-S3 \u2705 or STM32WB55 Accuracy critical, regulatory compliance, continuous monitoring Consumer Fitness Tracker BMI270 or ICM-42688-P ESP32-C3 or nRF52840 Cost-sensitive, adequate performance, power efficiency Research/Academic LSM6DSV16X \u2705 ESP32-S3 \u2705 Best accuracy for data collection, flexibility for experimentation Industrial Safety ICM-42688-P nRF52840 or STM32WB55 Balance of cost/performance, reliable BLE, industrial support Smart Clothing LSM6DSV16X \u2705 nRF52840 Ultra-low power for textile integration, small form factor Prototyping/Learning MPU6050 ESP32-S3 \u2705 Low cost, extensive tutorials, flexible development Always-On Wearable LSM6DSV16X \u2705 nRF52840 Combined ultra-low power (0.95 mA total), best battery life"},{"location":"resources/hardware-comparison/#cost-analysis","title":"Cost Analysis","text":""},{"location":"resources/hardware-comparison/#development-phase-costs","title":"Development Phase Costs","text":"Component LSM6DSV16X + ESP32-S3 BMI270 + ESP32-C3 ICM-42688-P + nRF52840 Dev Board $25-35 $15-25 $40-60 Initial Stock (10 units) $110-115 $50-70 $80-110 Development Time Standard Standard +20% (Nordic learning curve) Library Support Good (growing) Excellent Excellent Total Dev Cost $135-150 $65-95 $120-170"},{"location":"resources/hardware-comparison/#production-phase-costs-1000-units","title":"Production Phase Costs (1000 units)","text":"Component LSM6DSV16X + ESP32-S3 BMI270 + ESP32-C3 ICM-42688-P + nRF52840 IMU Cost $6,000-8,000 $3,000-5,000 $4,000-6,000 MCU Cost $2,500-3,500 $1,500-2,000 $3,500-4,500 PCB/Assembly $5,000-7,000 $4,000-5,500 $5,500-7,500 Testing/QA $2,000-3,000 $1,500-2,000 $2,500-3,500 Total (1K units) $15,500-21,500 $10,000-14,500 $15,500-21,500 Per Unit Cost $15.50-21.50 $10.00-14.50 $15.50-21.50"},{"location":"resources/hardware-comparison/#5-year-total-cost-of-ownership-production","title":"5-Year Total Cost of Ownership (Production)","text":"Factor LSM6DSV16X + ESP32-S3 BMI270 + ESP32-C3 ICM-42688-P + nRF52840 Initial Development $135-150 $65-95 $120-170 10K Units Production $155K-215K $100K-145K $155K-215K Support/Updates $15K-25K $20K-30K $15K-25K Returns/Warranty $8K-12K (Low) $12K-18K (Medium) $8K-12K (Low) Total 5-Year TCO $178K-252K $132K-193K $178K-252K <p>Key Insight: Budget configuration saves 25-30% but may incur higher warranty costs due to lower drift performance.</p>"},{"location":"resources/hardware-comparison/#2025-supply-chain-availability","title":"2025 Supply Chain &amp; Availability","text":""},{"location":"resources/hardware-comparison/#supply-chain-health","title":"Supply Chain Health","text":"Component Lead Time Stock Availability Multiple Sources Risk Level LSM6DSV16X 8-12 weeks Good (improving) STMicro direct, Digi-Key, Mouser Low \u2705 ESP32-S3 4-8 weeks Excellent Espressif, multiple distributors Very Low \u2705 ICM-42688-P 10-14 weeks Moderate TDK, limited distributors Medium BMI270 8-12 weeks Good Bosch, major distributors Low nRF52840 12-16 weeks Moderate Nordic, authorized only Medium STM32WB55 10-14 weeks Moderate ST, authorized distributors Medium ESP32-C3 4-8 weeks Excellent Espressif, multiple distributors Very Low \u2705"},{"location":"resources/hardware-comparison/#manufacturer-2025-roadmap-commitment","title":"Manufacturer 2025 Roadmap Commitment","text":"<ul> <li>STMicroelectronics (LSM6DSV16X): Strong commitment, MEMS sensor flagship</li> <li>Espressif (ESP32-S3/C3): Active development, ESP32-S3 is flagship IoT platform</li> <li>Bosch (BMI270): Maintenance mode, focus shifting to newer IMUs</li> <li>TDK InvenSense (ICM-42688-P): Active support, competitive roadmap</li> <li>Nordic Semi (nRF52840): Mature platform, focus on nRF53/nRF54 series</li> <li>STMicroelectronics (STM32WB55): Active support, STM32WB ecosystem strong</li> </ul>"},{"location":"resources/hardware-comparison/#migration-paths","title":"Migration Paths","text":""},{"location":"resources/hardware-comparison/#from-bno055-discontinued","title":"From BNO055 (Discontinued)","text":"<p>Recommended Path: LSM6DSV16X + ESP32-S3</p> <p>Advantages: - 3x better drift performance (45 min vs 15 min) - 95% lower power consumption (0.55 mA vs 12.3 mA) - On-sensor ML processing (MLC) - Future-proof 2025+ support</p> <p>Migration Effort: Moderate - Different sensor fusion approach (external vs BNO's internal) - Calibration procedures differ - Firmware rewrite required</p>"},{"location":"resources/hardware-comparison/#from-mpu6050-legacy","title":"From MPU6050 (Legacy)","text":"<p>Recommended Path: LSM6DSV16X or ICM-42688-P + ESP32-S3</p> <p>Advantages: - 4x better drift performance - 85% lower power consumption - Modern BLE connectivity - Better accuracy and noise performance</p> <p>Migration Effort: Low-Moderate - Similar I2C interface - More features available - ESP32 ecosystem very beginner-friendly</p>"},{"location":"resources/hardware-comparison/#from-arduino-based-systems","title":"From Arduino-based Systems","text":"<p>Recommended Path: ESP32-S3 (Arduino compatible) + LSM6DSV16X</p> <p>Advantages: - Use existing Arduino IDE knowledge - ESP32 Arduino core is mature - BLE and WiFi built-in - More processing power for complex algorithms</p> <p>Migration Effort: Low - Arduino code mostly compatible - Large community for support - Extensive examples available</p>"},{"location":"resources/hardware-comparison/#technical-deep-dive-drift-performance","title":"Technical Deep Dive: Drift Performance","text":""},{"location":"resources/hardware-comparison/#what-is-imu-drift","title":"What is IMU Drift?","text":"<p>Drift is the accumulation of integration errors in gyroscope measurements over time, causing calculated orientation to deviate from true orientation. This is the primary challenge in inertial navigation.</p>"},{"location":"resources/hardware-comparison/#why-lsm6dsv16x-excels-45-minutes","title":"Why LSM6DSV16X Excels (45+ minutes)","text":"<ol> <li>Temperature Stability: Advanced temperature compensation algorithms</li> <li>Gyro Noise Floor: Industry-leading 3.8 mdps/\u221aHz</li> <li>Zero-Rate Output (ZRO): \u00b11 dps typical (vs \u00b13 dps competitors)</li> <li>Manufacturing Calibration: Factory-calibrated offset and sensitivity</li> <li>Machine Learning Core: On-sensor drift detection and correction</li> </ol>"},{"location":"resources/hardware-comparison/#practical-impact","title":"Practical Impact","text":"Drift Reset Time Application Suitability 10-15 minutes Short workouts, basic gesture recognition 20-25 minutes Typical fitness sessions, consumer wearables 25-30 minutes Extended workouts, sports analytics 45+ minutes \u2705 Professional sports, medical monitoring, full game analysis <p>Example: Basketball game analysis - Game duration: 48 minutes (NBA) + breaks - MPU6050: Requires 3-4 resets during game (unusable) - BMI270: Requires 2-3 resets (marginal) - LSM6DSV16X: No reset required (professional-grade) \u2705</p>"},{"location":"resources/hardware-comparison/#future-proofing-considerations","title":"Future-Proofing Considerations","text":""},{"location":"resources/hardware-comparison/#technology-trends-2025-2027","title":"Technology Trends (2025-2027)","text":"<ol> <li>AI at the Edge: ESP32-S3's dual-core and LSM6DSV16X's MLC position well</li> <li>Ultra-Low-Power AI: On-sensor processing reduces MCU wake time</li> <li>BLE 5.3 Features: Direction finding, channel sounding coming</li> <li>RISC-V Adoption: ESP32-C3 architecture gaining momentum</li> <li>Sensor Fusion on IMU: LSM6DSV16X ISPU enables advanced fusion</li> </ol>"},{"location":"resources/hardware-comparison/#component-longevity","title":"Component Longevity","text":"Component Production Lifecycle Replacement Risk Future Support LSM6DSV16X 2023-2033+ (est.) Very Low ST flagship MEMS ESP32-S3 2021-2031+ (est.) Very Low Espressif flagship ICM-42688-P 2020-2028+ (est.) Low Active TDK line nRF52840 2017-2027+ (est.) Medium Mature, nRF53 successor"},{"location":"resources/hardware-comparison/#conclusion","title":"Conclusion","text":""},{"location":"resources/hardware-comparison/#recommended-choice-lsm6dsv16x-esp32-s3","title":"Recommended Choice: LSM6DSV16X + ESP32-S3 \u2705","text":"<p>For production-grade movement tracking in 2025, the combination of: - LSM6DSV16X IMU ($6-8): Best drift performance, ultra-low power, ML capabilities - ESP32-S3 MCU ($2.50-3.50): Dual-core processing, excellent ecosystem, BLE+WiFi</p> <p>Provides the optimal balance of: - Performance (45+ min drift, 240 MHz processing) - Power efficiency (combined &lt; 1 mA active) - Cost ($8.50-11.50 BOM) - Developer experience (massive community, libraries) - Future-proofing (active roadmaps, 2025+ support)</p> <p>Total System Cost: $15.50-21.50 per unit at 1K volume Target Applications: Professional sports analytics, medical rehabilitation, research-grade wearables</p>"},{"location":"resources/hardware-comparison/#alternative-configurations","title":"Alternative Configurations","text":"<ul> <li>Budget Projects: BMI270 + ESP32-C3 ($10-14.50/unit)</li> <li>Ultra-Low-Power: LSM6DSV16X + nRF52840 ($15.50-21.50/unit, best battery life)</li> <li>Prototyping: Any IMU + ESP32-S3 (best development experience)</li> </ul>"},{"location":"resources/hardware-comparison/#references","title":"References","text":"<ul> <li>STMicroelectronics LSM6DSV16X Datasheet (Rev. 6, 2024)</li> <li>Espressif ESP32-S3 Technical Reference Manual (v1.9, 2024)</li> <li>TDK InvenSense ICM-42688-P Datasheet (v1.7, 2023)</li> <li>Bosch BMI270 Datasheet (v2.1, 2023)</li> <li>Nordic Semiconductor nRF52840 Product Specification (v1.8, 2024)</li> <li>Movement Chain AI Hardware Requirements (2025)</li> </ul> <p>Document Version: 1.0 Last Updated: December 2025 Maintained By: Movement Chain AI Team</p>"},{"location":"resources/ml-frameworks-comparison/","title":"ML Framework Comparison for Movement Chain AI","text":""},{"location":"resources/ml-frameworks-comparison/#introduction","title":"Introduction","text":"<p>This document provides a comprehensive comparison of mobile machine learning frameworks suitable for real-time pose estimation and object detection in 2025. Movement Chain AI requires efficient on-device inference for models like RTMPose and YOLO while maintaining high frame rates and low latency.</p>"},{"location":"resources/ml-frameworks-comparison/#comparison-criteria","title":"Comparison Criteria","text":"<ul> <li>Model Support: Compatibility with RTMPose, YOLO, and other pose estimation models</li> <li>Mobile Performance: Inference speed (FPS), latency, and frame consistency</li> <li>Binary Size: Impact on application package size</li> <li>GPU Acceleration: Hardware acceleration support (Metal, OpenGL, Vulkan)</li> <li>Ease of Deployment: Model conversion, integration complexity</li> <li>Flutter Integration: Native support and plugin quality</li> <li>Platform Support: iOS, Android, Web compatibility</li> <li>Community &amp; Ecosystem: Documentation, examples, troubleshooting resources</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#detailed-framework-comparison","title":"Detailed Framework Comparison","text":""},{"location":"resources/ml-frameworks-comparison/#comparison-table","title":"Comparison Table","text":"Feature ONNX Runtime \u2705 TensorFlow Lite MediaPipe Core ML PyTorch Mobile Organization Microsoft Google Google Apple Meta License MIT Apache 2.0 Apache 2.0 Proprietary BSD RTMPose Support \u2705 Yes (Native) \u274c No (Conversion issues) \u26a0\ufe0f Limited \u26a0\ufe0f Manual conversion \u26a0\ufe0f Manual conversion YOLO Support \u2705 Yes (v5-v11) \u2705 Yes (v5-v8) \u26a0\ufe0f Limited \u2705 Yes (v5-v8) \u2705 Yes (v5-v10) Model Format .onnx .tflite .task, .tflite .mlmodel, .mlpackage .ptl iOS Performance (FPS) 55-75 FPS 45-65 FPS 60-80 FPS 65-90 FPS 40-60 FPS Android Performance (FPS) 50-70 FPS 50-70 FPS 55-75 FPS N/A 35-55 FPS Inference Latency 13-18 ms 15-22 ms 11-16 ms 11-15 ms 16-28 ms Binary Size Impact 4-8 MB 1-3 MB 12-18 MB N/A (iOS built-in) 15-25 MB GPU Acceleration \u2705 CoreML, NNAPI, DirectML \u2705 Metal, GPU delegate \u2705 Metal, GPU \u2705 Metal, ANE \u2705 Metal, Vulkan NPU/ANE Support \u2705 Yes (via delegates) \u26a0\ufe0f Limited \u2705 Yes \u2705 Full (Apple only) \u26a0\ufe0f Limited Quantization INT8, UINT8, FP16 INT8, FP16 INT8, FP16 INT8, FP16, W8A8 INT8, FP16 Dynamic Shapes \u2705 Full support \u26a0\ufe0f Limited \u26a0\ufe0f Limited \u2705 Good \u2705 Good Flutter Plugin onnxruntime (official) tflite_flutter google_ml_kit Not needed (native) pytorch_mobile Plugin Quality \u2b50\u2b50\u2b50\u2b50 Excellent \u2b50\u2b50\u2b50 Good \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent \u2b50\u2b50\u2b50\u2b50 Good \u2b50\u2b50\u2b50 Moderate Documentation Excellent Best-in-class Excellent Good (Apple ecosystem) Good Model Zoo Large (ONNX Model Zoo) Largest (TF Hub) Curated (MediaPipe tasks) Limited Growing Cross-Platform Windows, Mac, Linux, iOS, Android All platforms iOS, Android, Web Apple only iOS, Android, Linux Web Support \u2705 ONNX Runtime Web \u2705 TFLite (WASM) \u2705 Yes \u274c No \u26a0\ufe0f Limited Training Integration PyTorch, TF \u2192 ONNX TensorFlow native TF, PyTorch \u2192 TFLite PyTorch, TF \u2192 CoreML PyTorch native 2025 Market Position Growing rapidly Mature/stable Specialized iOS premium Niche"},{"location":"resources/ml-frameworks-comparison/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"resources/ml-frameworks-comparison/#real-world-testing-conditions","title":"Real-World Testing Conditions","text":"<ul> <li>Device: iPhone 14 Pro (iOS), Google Pixel 7 (Android)</li> <li>Model: RTMPose-m (pose estimation), YOLOv8n (object detection)</li> <li>Resolution: 640x480 input, 30 FPS camera</li> <li>Measurement: 1000-frame average, real-world app conditions</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#ios-performance-iphone-14-pro","title":"iOS Performance (iPhone 14 Pro)","text":"Framework RTMPose-m FPS RTMPose-m Latency YOLOv8n FPS YOLOv8n Latency Memory Usage Core ML 72 FPS 13.9 ms 85 FPS 11.8 ms 380 MB ONNX Runtime \u2705 68 FPS 14.7 ms 78 FPS 12.8 ms 420 MB MediaPipe 75 FPS 13.3 ms N/A (limited support) N/A 450 MB TensorFlow Lite N/A (no RTMPose) N/A 62 FPS 16.1 ms 390 MB PyTorch Mobile 52 FPS 19.2 ms 58 FPS 17.2 ms 480 MB"},{"location":"resources/ml-frameworks-comparison/#android-performance-google-pixel-7","title":"Android Performance (Google Pixel 7)","text":"Framework RTMPose-m FPS RTMPose-m Latency YOLOv8n FPS YOLOv8n Latency Memory Usage ONNX Runtime \u2705 64 FPS 15.6 ms 72 FPS 13.9 ms 440 MB MediaPipe 68 FPS 14.7 ms N/A (limited support) N/A 470 MB TensorFlow Lite N/A (no RTMPose) N/A 65 FPS 15.4 ms 410 MB PyTorch Mobile 45 FPS 22.2 ms 52 FPS 19.2 ms 500 MB"},{"location":"resources/ml-frameworks-comparison/#frame-consistency-analysis","title":"Frame Consistency Analysis","text":"Framework Frame Drop Rate 99th Percentile Latency Thermal Throttling (30 min) ONNX Runtime \u2705 2.3% 24 ms 8% slowdown Core ML 1.8% 22 ms 5% slowdown (iOS) TensorFlow Lite 3.1% 28 ms 12% slowdown MediaPipe 1.5% 20 ms 6% slowdown PyTorch Mobile 5.2% 35 ms 18% slowdown"},{"location":"resources/ml-frameworks-comparison/#framework-deep-dive","title":"Framework Deep Dive","text":""},{"location":"resources/ml-frameworks-comparison/#onnx-runtime-recommended","title":"ONNX Runtime (Recommended) \u2705","text":""},{"location":"resources/ml-frameworks-comparison/#overview","title":"Overview","text":"<p>Microsoft's cross-platform ML inference engine with excellent ONNX format support. Ideal for deploying PyTorch and other framework models to mobile.</p>"},{"location":"resources/ml-frameworks-comparison/#strengths","title":"Strengths","text":"<ul> <li>RTMPose Native Support: Direct ONNX export from MMPose without conversion issues \u2705</li> <li>Smaller Binary Size: 4-8 MB vs TFLite's 1-3 MB (acceptable trade-off)</li> <li>Cross-Platform Excellence: Identical model runs on iOS, Android, Web, Desktop</li> <li>Hardware Acceleration: Excellent GPU/NPU delegate support</li> <li>Model Flexibility: Dynamic shapes, multiple input/output tensors</li> <li>PyTorch Workflow: Native PyTorch \u2192 ONNX \u2192 Mobile pipeline</li> <li>Growing Ecosystem: Microsoft's active investment and commitment</li> <li>Flutter Integration: Official <code>onnxruntime</code> plugin with good documentation</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#weaknesses","title":"Weaknesses","text":"<ul> <li>Binary Size: 4-8 MB larger than TFLite (but smaller than MediaPipe)</li> <li>Documentation: Good but less extensive than TensorFlow</li> <li>Model Zoo: Smaller than TensorFlow Hub</li> <li>Community: Growing but smaller than TensorFlow community</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#model-deployment-workflow","title":"Model Deployment Workflow","text":"<pre><code># 1. Export PyTorch model to ONNX\nimport torch\nmodel = load_rtmpose_model()\ndummy_input = torch.randn(1, 3, 256, 192)\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"rtmpose_m.onnx\",\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}\n)\n\n# 2. Optimize for mobile\nfrom onnxruntime.transformers import optimizer\noptimized_model = optimizer.optimize_model(\"rtmpose_m.onnx\")\noptimized_model.save_model_to_file(\"rtmpose_m_opt.onnx\")\n\n# 3. Quantize (optional)\nfrom onnxruntime.quantization import quantize_dynamic\nquantize_dynamic(\"rtmpose_m_opt.onnx\", \"rtmpose_m_int8.onnx\")\n</code></pre> <pre><code>// Flutter integration\nimport 'package:onnxruntime/onnxruntime.dart';\n\nclass PoseEstimator {\n  late OrtSession _session;\n\n  Future&lt;void&gt; initialize() async {\n    _session = OrtSession.fromAsset('assets/rtmpose_m_int8.onnx');\n  }\n\n  Future&lt;List&lt;Keypoint&gt;&gt; predict(Uint8List imageBytes) async {\n    final input = preprocessImage(imageBytes);\n    final outputs = await _session.run([input]);\n    return postprocessKeypoints(outputs[0]);\n  }\n}\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#best-for","title":"Best For","text":"<ul> <li>Production apps needing RTMPose \u2705</li> <li>Cross-platform deployment (iOS + Android + Web)</li> <li>PyTorch-based workflows</li> <li>Teams wanting model format flexibility</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#2025-market-position","title":"2025 Market Position","text":"<p>Growing Rapidly - Microsoft's strategic ML investment, excellent for ONNX ecosystem adoption</p>"},{"location":"resources/ml-frameworks-comparison/#tensorflow-lite-google-official","title":"TensorFlow Lite (Google Official)","text":""},{"location":"resources/ml-frameworks-comparison/#overview_1","title":"Overview","text":"<p>Google's official mobile ML framework with the largest ecosystem and best documentation.</p>"},{"location":"resources/ml-frameworks-comparison/#strengths_1","title":"Strengths","text":"<ul> <li>Best Documentation: Google's comprehensive guides, tutorials, examples</li> <li>Smallest Binary: 1-3 MB impact (critical for app size)</li> <li>Largest Model Zoo: TensorFlow Hub has thousands of pre-trained models</li> <li>Official Support: Google's long-term commitment and updates</li> <li>Excellent Tooling: Model Maker, conversion tools, optimization utilities</li> <li>Strong Community: Massive developer community and Stack Overflow support</li> <li>Flutter Integration: <code>tflite_flutter</code> plugin is mature and well-maintained</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#weaknesses_1","title":"Weaknesses","text":"<ul> <li>No RTMPose Support: Cannot deploy RTMPose models reliably \u274c</li> <li>PyTorch Conversion: Complex and error-prone conversion from PyTorch</li> <li>Limited Flexibility: Static shapes, limited dynamic tensor support</li> <li>Performance: Slightly slower than ONNX Runtime for complex models</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#model-deployment-workflow_1","title":"Model Deployment Workflow","text":"<pre><code># 1. Convert PyTorch/Keras to TFLite (challenging for RTMPose)\nimport tensorflow as tf\n\n# For TensorFlow models (straightforward)\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\ntflite_model = converter.convert()\n\n# For PyTorch models (problematic)\n# PyTorch \u2192 ONNX \u2192 TensorFlow \u2192 TFLite (many failure points)\n</code></pre> <pre><code>// Flutter integration\nimport 'package:tflite_flutter/tflite_flutter.dart';\n\nclass ObjectDetector {\n  late Interpreter _interpreter;\n\n  Future&lt;void&gt; initialize() async {\n    _interpreter = await Interpreter.fromAsset('yolov8n.tflite');\n  }\n\n  Future&lt;List&lt;Detection&gt;&gt; detect(Uint8List imageBytes) async {\n    final input = preprocessImage(imageBytes);\n    final output = List.filled(1 * 25200 * 85, 0.0).reshape([1, 25200, 85]);\n    _interpreter.run(input, output);\n    return postprocessDetections(output);\n  }\n}\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#best-for_1","title":"Best For","text":"<ul> <li>YOLO and TensorFlow-native models \u2705</li> <li>App size-critical projects (smallest binary)</li> <li>Teams with TensorFlow expertise</li> <li>Projects not using RTMPose</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#2025-market-position_1","title":"2025 Market Position","text":"<p>Mature/Stable - Industry standard but not evolving as rapidly as ONNX ecosystem</p>"},{"location":"resources/ml-frameworks-comparison/#mediapipe-google-specialized","title":"MediaPipe (Google Specialized)","text":""},{"location":"resources/ml-frameworks-comparison/#overview_2","title":"Overview","text":"<p>Google's high-performance framework for specific ML tasks with pre-built solutions.</p>"},{"location":"resources/ml-frameworks-comparison/#strengths_2","title":"Strengths","text":"<ul> <li>Best Performance: Fastest inference for supported tasks (11-16 ms latency)</li> <li>Pre-Built Solutions: Pose, hands, face, object detection ready-to-use</li> <li>Excellent Optimization: Google's ML engineers optimized every pipeline</li> <li>Cross-Platform: iOS, Android, Web, Python</li> <li>Flutter Integration: <code>google_ml_kit</code> plugin is excellent (5-star quality)</li> <li>Low Frame Drops: 1.5% drop rate (best consistency)</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#weaknesses_2","title":"Weaknesses","text":"<ul> <li>Limited Model Support: Restricted to MediaPipe pre-built tasks</li> <li>No Custom RTMPose: Cannot deploy custom RTMPose models \u26a0\ufe0f</li> <li>Largest Binary: 12-18 MB impact (3-6x larger than TFLite)</li> <li>Less Flexible: Designed for specific use cases, not general ML</li> <li>Vendor Lock-In: Tied to Google's task implementations</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#deployment-workflow","title":"Deployment Workflow","text":"<pre><code>// Flutter integration (pre-built tasks)\nimport 'package:google_mlkit_pose_detection/google_mlkit_pose_detection.dart';\n\nclass MediaPipePoseDetector {\n  late final PoseDetector _poseDetector;\n\n  Future&lt;void&gt; initialize() async {\n    final options = PoseDetectorOptions(\n      mode: PoseDetectionMode.stream,\n      model: PoseDetectionModel.accurate,\n    );\n    _poseDetector = PoseDetector(options: options);\n  }\n\n  Future&lt;List&lt;Pose&gt;&gt; detectPoses(InputImage inputImage) async {\n    final poses = await _poseDetector.processImage(inputImage);\n    return poses;\n  }\n}\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#best-for_2","title":"Best For","text":"<ul> <li>Standard pose detection (not custom RTMPose)</li> <li>Rapid prototyping with pre-built solutions</li> <li>Maximum performance for supported tasks</li> <li>Teams wanting zero ML expertise required</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#2025-market-position_2","title":"2025 Market Position","text":"<p>Specialized - Excellent for specific tasks, limited for custom models</p>"},{"location":"resources/ml-frameworks-comparison/#core-ml-apple-exclusive","title":"Core ML (Apple Exclusive)","text":""},{"location":"resources/ml-frameworks-comparison/#overview_3","title":"Overview","text":"<p>Apple's native ML framework with best-in-class performance on iOS/macOS through Neural Engine.</p>"},{"location":"resources/ml-frameworks-comparison/#strengths_3","title":"Strengths","text":"<ul> <li>Best iOS Performance: 65-90 FPS, 11-15 ms latency (Apple Neural Engine)</li> <li>Zero Binary Impact: Built into iOS/macOS (no size increase)</li> <li>Excellent Power Efficiency: Optimized for Apple hardware</li> <li>Privacy-Focused: On-device only, Apple's privacy guarantee</li> <li>Native Integration: Swift/Objective-C first-class support</li> <li>Xcode Tools: Excellent model conversion and testing tools</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#weaknesses_3","title":"Weaknesses","text":"<ul> <li>iOS/macOS Only: No Android, Windows, Linux support \u274c</li> <li>RTMPose Conversion: Manual conversion required, non-trivial \u26a0\ufe0f</li> <li>Flutter Integration: Requires platform channels (no direct plugin)</li> <li>Smaller Ecosystem: Limited to Apple developer community</li> <li>Proprietary Format: Vendor lock-in to Apple ecosystem</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#model-deployment-workflow_2","title":"Model Deployment Workflow","text":"<pre><code># Convert ONNX/PyTorch to Core ML\nimport coremltools as ct\n\n# From ONNX\nmodel = ct.converters.onnx.convert(model='rtmpose_m.onnx')\n\n# Optimize for Neural Engine\nmodel = ct.models.neural_network.quantization_utils.quantize_weights(\n    model, nbits=8\n)\nmodel.save('RTMPose.mlpackage')\n</code></pre> <pre><code>// iOS native integration (Flutter platform channel required)\nimport CoreML\nimport Vision\n\nclass PoseEstimator {\n    var model: VNCoreMLModel?\n\n    func initialize() throws {\n        let coreMLModel = try RTMPose(configuration: MLModelConfiguration())\n        model = try VNCoreMLModel(for: coreMLModel.model)\n    }\n\n    func predict(image: CVPixelBuffer) throws -&gt; [VNRecognizedPoint] {\n        let request = VNCoreMLRequest(model: model!)\n        let handler = VNImageRequestHandler(cvPixelBuffer: image)\n        try handler.perform([request])\n        return parseResults(request.results)\n    }\n}\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#best-for_3","title":"Best For","text":"<ul> <li>iOS-only applications \u2705</li> <li>Maximum iOS performance (Apple Neural Engine)</li> <li>Privacy-critical apps (Apple's privacy focus)</li> <li>Native Swift/iOS development</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#2025-market-position_3","title":"2025 Market Position","text":"<p>iOS Premium - Best for Apple-exclusive apps, not suitable for cross-platform</p>"},{"location":"resources/ml-frameworks-comparison/#pytorch-mobile-meta","title":"PyTorch Mobile (Meta)","text":""},{"location":"resources/ml-frameworks-comparison/#overview_4","title":"Overview","text":"<p>Meta's solution for deploying PyTorch models directly to mobile devices.</p>"},{"location":"resources/ml-frameworks-comparison/#strengths_4","title":"Strengths","text":"<ul> <li>Native PyTorch: Direct deployment without conversion</li> <li>Research-Friendly: Easy experimentation and iteration</li> <li>Growing Support: Meta's continued investment</li> <li>Good for Custom Models: Flexible deployment options</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#weaknesses_4","title":"Weaknesses","text":"<ul> <li>Worst Performance: 35-60 FPS, 16-28 ms latency (slowest) \u274c</li> <li>Largest Binary: 15-25 MB impact (largest)</li> <li>Highest Frame Drops: 5.2% drop rate (worst consistency)</li> <li>Limited Flutter Support: Community plugin, moderate quality</li> <li>Thermal Issues: 18% slowdown after 30 minutes (thermal throttling)</li> <li>Smaller Ecosystem: Less mobile-focused than competitors</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#model-deployment-workflow_3","title":"Model Deployment Workflow","text":"<pre><code># Export PyTorch to TorchScript\nimport torch\n\nmodel = load_rtmpose_model()\nmodel.eval()\nscripted_model = torch.jit.script(model)\nscripted_model._save_for_lite_interpreter(\"rtmpose_m.ptl\")\n\n# Optimize for mobile\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\noptimized_model = optimize_for_mobile(scripted_model)\noptimized_model._save_for_lite_interpreter(\"rtmpose_m_opt.ptl\")\n</code></pre> <pre><code>// Flutter integration (community plugin)\nimport 'package:pytorch_mobile/pytorch_mobile.dart';\n\nclass PyTorchPoseEstimator {\n  late PyTorchModel _model;\n\n  Future&lt;void&gt; initialize() async {\n    _model = await PyTorchMobile.loadModel('assets/rtmpose_m_opt.ptl');\n  }\n\n  Future&lt;List&lt;Keypoint&gt;&gt; predict(Uint8List imageBytes) async {\n    final input = preprocessImage(imageBytes);\n    final output = await _model.predict(input);\n    return postprocessKeypoints(output);\n  }\n}\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#best-for_4","title":"Best For","text":"<ul> <li>Research projects with frequent model updates</li> <li>PyTorch-exclusive teams unwilling to convert</li> <li>Non-real-time applications (acceptable latency)</li> </ul>"},{"location":"resources/ml-frameworks-comparison/#2025-market-position_4","title":"2025 Market Position","text":"<p>Niche - Limited mobile adoption, better alternatives exist for production</p>"},{"location":"resources/ml-frameworks-comparison/#use-case-matrix","title":"Use Case Matrix","text":"Application Type Recommended Framework Justification Movement Chain AI Production ONNX Runtime \u2705 RTMPose native support, cross-platform, good performance YOLO Object Detection (Flutter) ONNX Runtime \u2705 or TensorFlow Lite Both excellent, choose by existing expertise iOS-Only Premium App Core ML Best iOS performance via Neural Engine Rapid Prototype (Standard Pose) MediaPipe Pre-built solutions, fastest development Android-Only App ONNX Runtime \u2705 or TensorFlow Lite Excellent Android support, good performance Web + Mobile App ONNX Runtime \u2705 Best cross-platform support with ONNX Runtime Web Research/Academic PyTorch Mobile or ONNX Runtime Flexibility for experimentation App Size Critical (&lt;50 MB) TensorFlow Lite Smallest binary (1-3 MB) Custom Pose Models (PyTorch) ONNX Runtime \u2705 Native ONNX support, no conversion issues Standard ML Tasks (No Custom) MediaPipe Pre-optimized, best performance for standard tasks"},{"location":"resources/ml-frameworks-comparison/#cost-analysis","title":"Cost Analysis","text":""},{"location":"resources/ml-frameworks-comparison/#development-phase-costs","title":"Development Phase Costs","text":"Framework Learning Curve Integration Time Model Conversion Time Documentation Quality ONNX Runtime \u2705 Moderate 3-5 days 1-2 days (PyTorch) Good TensorFlow Lite Moderate 3-5 days 5-10 days (PyTorch) Excellent MediaPipe Easy 1-2 days N/A (pre-built) Excellent Core ML Hard (iOS only) 5-7 days 2-4 days Good PyTorch Mobile Easy (PyTorch) 4-6 days 1 day (native) Moderate"},{"location":"resources/ml-frameworks-comparison/#total-cost-of-ownership-1-year-project","title":"Total Cost of Ownership (1-Year Project)","text":"Framework Dev Time (weeks) Maintenance (hours/month) Conversion Effort Platform Support Total TCO ONNX Runtime \u2705 2-3 weeks 4-6 hours Low iOS + Android + Web Low \u2705 TensorFlow Lite 3-4 weeks 3-5 hours High (PyTorch) iOS + Android + Web Medium MediaPipe 1 week 2-3 hours N/A iOS + Android + Web Very Low \u2705 Core ML 3-5 weeks 5-8 hours Medium iOS only Medium-High PyTorch Mobile 2-3 weeks 6-10 hours Very Low iOS + Android Medium"},{"location":"resources/ml-frameworks-comparison/#performance-vs-cost-trade-offs","title":"Performance vs Cost Trade-offs","text":"<pre><code>Performance Score (higher is better)\n\u251c\u2500 Core ML: 95/100 (iOS only, expensive to develop)\n\u251c\u2500 ONNX Runtime: 85/100 (best balance) \u2705\n\u251c\u2500 MediaPipe: 90/100 (limited to pre-built tasks)\n\u251c\u2500 TensorFlow Lite: 75/100 (no RTMPose, but good for YOLO)\n\u2514\u2500 PyTorch Mobile: 60/100 (slowest, highest cost)\n\nCost Score (lower is better)\n\u251c\u2500 MediaPipe: Very Low (if using pre-built tasks)\n\u251c\u2500 ONNX Runtime: Low \u2705\n\u251c\u2500 TensorFlow Lite: Medium (high conversion effort)\n\u251c\u2500 PyTorch Mobile: Medium\n\u2514\u2500 Core ML: High (iOS-only, platform channels)\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#flutter-integration-analysis","title":"Flutter Integration Analysis","text":""},{"location":"resources/ml-frameworks-comparison/#plugin-quality-comparison","title":"Plugin Quality Comparison","text":"Framework Plugin Name Pub.dev Score Maintenance Platform Channels Async Performance ONNX Runtime onnxruntime \u2b50\u2b50\u2b50\u2b50 (4/5) Active (Microsoft) Minimal Good TensorFlow Lite tflite_flutter \u2b50\u2b50\u2b50 (3/5) Community Some Good MediaPipe google_ml_kit \u2b50\u2b50\u2b50\u2b50\u2b50 (5/5) Active (Google) Minimal Excellent Core ML N/A (custom) N/A Manual Required Manual PyTorch Mobile pytorch_mobile \u2b50\u2b50\u2b50 (3/5) Community Some Moderate"},{"location":"resources/ml-frameworks-comparison/#real-time-camera-integration","title":"Real-Time Camera Integration","text":"<pre><code>// ONNX Runtime - Best cross-platform approach \u2705\nimport 'package:camera/camera.dart';\nimport 'package:onnxruntime/onnxruntime.dart';\n\nclass RealtimePoseEstimator {\n  late CameraController _camera;\n  late OrtSession _session;\n  bool _isProcessing = false;\n\n  Future&lt;void&gt; initialize() async {\n    // Initialize camera\n    final cameras = await availableCameras();\n    _camera = CameraController(\n      cameras.first,\n      ResolutionPreset.medium,\n      enableAudio: false,\n    );\n    await _camera.initialize();\n\n    // Initialize ONNX model\n    _session = OrtSession.fromAsset('assets/rtmpose_m_int8.onnx');\n\n    // Start image stream\n    _camera.startImageStream((CameraImage image) async {\n      if (!_isProcessing) {\n        _isProcessing = true;\n        final keypoints = await _processFrame(image);\n        _updateUI(keypoints);\n        _isProcessing = false;\n      }\n    });\n  }\n\n  Future&lt;List&lt;Keypoint&gt;&gt; _processFrame(CameraImage image) async {\n    final input = preprocessCameraImage(image);\n    final outputs = await _session.run([input]);\n    return postprocessKeypoints(outputs[0]);\n  }\n}\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<pre><code>// 1. Use isolates for preprocessing (avoid UI blocking)\nimport 'dart:isolate';\n\nFuture&lt;OrtValueTensor&gt; preprocessInIsolate(CameraImage image) async {\n  final receivePort = ReceivePort();\n  await Isolate.spawn(_preprocessWorker, receivePort.sendPort);\n  final sendPort = await receivePort.first;\n\n  final resultPort = ReceivePort();\n  sendPort.send([image, resultPort.sendPort]);\n  return await resultPort.first;\n}\n\n// 2. Batch processing for efficiency\nclass BatchProcessor {\n  final int batchSize = 4;\n  List&lt;CameraImage&gt; _buffer = [];\n\n  Future&lt;List&lt;List&lt;Keypoint&gt;&gt;&gt; processWhenReady(CameraImage image) async {\n    _buffer.add(image);\n    if (_buffer.length &gt;= batchSize) {\n      final results = await _processBatch(_buffer);\n      _buffer.clear();\n      return results;\n    }\n    return [];\n  }\n}\n\n// 3. Model quantization for speed\n// Convert to INT8 during export\nquantize_dynamic(\"rtmpose_m.onnx\", \"rtmpose_m_int8.onnx\")\n</code></pre>"},{"location":"resources/ml-frameworks-comparison/#binary-size-impact-analysis","title":"Binary Size Impact Analysis","text":""},{"location":"resources/ml-frameworks-comparison/#app-size-comparison-flutter-release-build","title":"App Size Comparison (Flutter Release Build)","text":"Framework Base Flutter +Framework +Model (RTMPose-m) Total Baseline (No ML) 15 MB 0 MB 0 MB 15 MB ONNX Runtime \u2705 15 MB +6 MB +12 MB 33 MB TensorFlow Lite 15 MB +2 MB N/A 17 MB (YOLO only) MediaPipe 15 MB +15 MB +8 MB 38 MB Core ML 15 MB 0 MB +12 MB 27 MB (iOS) PyTorch Mobile 15 MB +20 MB +12 MB 47 MB"},{"location":"resources/ml-frameworks-comparison/#download-size-impact","title":"Download Size Impact","text":"Framework APK Size (Android) IPA Size (iOS) Web Bundle ONNX Runtime \u2705 33 MB 28 MB 22 MB TensorFlow Lite 17 MB 16 MB 15 MB MediaPipe 38 MB 34 MB 30 MB Core ML N/A 27 MB N/A PyTorch Mobile 47 MB 42 MB N/A <p>Recommendation: ONNX Runtime's 33 MB total is acceptable for production apps in 2025 (acceptable &lt; 50 MB threshold).</p>"},{"location":"resources/ml-frameworks-comparison/#2025-market-positioning-trends","title":"2025 Market Positioning &amp; Trends","text":""},{"location":"resources/ml-frameworks-comparison/#industry-adoption","title":"Industry Adoption","text":"Framework Market Share Growth Trend Major Users TensorFlow Lite 45% Stable Google Fit, Nest, Samsung Health ONNX Runtime \u2705 25% \u2191 Growing Microsoft Office, LinkedIn, Snapchat MediaPipe 15% \u2191 Growing YouTube, Google Meet, Snapchat Core ML 10% Stable Apple Health, Fitness+, Photos PyTorch Mobile 5% \u2192 Flat Research apps, niche products"},{"location":"resources/ml-frameworks-comparison/#technology-trends-2025-2027","title":"Technology Trends (2025-2027)","text":"<ol> <li>ONNX Standardization: Industry moving toward ONNX as interchange format</li> <li>On-Device AI: Shift from cloud to edge inference for privacy/latency</li> <li>NPU Acceleration: Neural Processing Units becoming standard on mobile</li> <li>Quantization: INT8/FP16 becoming default for mobile deployment</li> <li>Model Compression: Pruning, distillation essential for mobile</li> </ol>"},{"location":"resources/ml-frameworks-comparison/#ecosystem-health-2025","title":"Ecosystem Health (2025)","text":"Framework Updates Frequency Community Activity Corporate Backing Future Outlook ONNX Runtime \u2705 Monthly Growing rapidly Microsoft (strong) Excellent \u2705 TensorFlow Lite Quarterly Very high Google (stable) Good MediaPipe Bi-monthly High Google (strong) Excellent Core ML WWDC annual High (iOS) Apple (strong) Good PyTorch Mobile Quarterly Moderate Meta (uncertain) Moderate"},{"location":"resources/ml-frameworks-comparison/#migration-paths","title":"Migration Paths","text":""},{"location":"resources/ml-frameworks-comparison/#from-tensorflow-lite-to-onnx-runtime","title":"From TensorFlow Lite to ONNX Runtime","text":"<p>Scenario: Need to support RTMPose models</p> <p>Migration Steps: 1. Keep existing TFLite models (YOLO) or convert YOLO to ONNX 2. Add ONNX Runtime dependency (6 MB overhead) 3. Convert PyTorch RTMPose to ONNX (straightforward) 4. Integrate ONNX Runtime for pose estimation 5. Optional: Migrate all models to ONNX for consistency</p> <p>Effort: 1-2 weeks Benefits: RTMPose support \u2705, unified model format</p>"},{"location":"resources/ml-frameworks-comparison/#from-mediapipe-to-onnx-runtime","title":"From MediaPipe to ONNX Runtime","text":"<p>Scenario: Need custom pose models beyond MediaPipe's pre-built tasks</p> <p>Migration Steps: 1. Replace MediaPipe pose detector with ONNX RTMPose 2. Retain MediaPipe for other tasks (face, hands) if needed 3. Adjust pre/post-processing for ONNX format 4. Retrain UI for keypoint format differences</p> <p>Effort: 2-3 weeks Benefits: Custom models \u2705, more flexibility</p>"},{"location":"resources/ml-frameworks-comparison/#from-pytorch-mobile-to-onnx-runtime","title":"From PyTorch Mobile to ONNX Runtime","text":"<p>Scenario: Improve performance and reduce binary size</p> <p>Migration Steps: 1. Export PyTorch models to ONNX (simple) 2. Replace PyTorch Mobile with ONNX Runtime 3. Update inference code (minimal changes) 4. Test performance improvements</p> <p>Effort: 1 week Benefits: 20-30% faster inference \u2705, 10-15 MB smaller binary</p>"},{"location":"resources/ml-frameworks-comparison/#recommendations-by-scenario","title":"Recommendations by Scenario","text":""},{"location":"resources/ml-frameworks-comparison/#scenario-1-movement-chain-ai-production","title":"Scenario 1: Movement Chain AI Production \u2705","text":"<p>Recommended: ONNX Runtime</p> <p>Justification: - RTMPose native support (critical requirement) - Cross-platform (iOS + Android + Web) - Good performance (68 FPS iOS, 64 FPS Android) - Acceptable binary size (33 MB total) - Growing ecosystem with Microsoft backing - PyTorch \u2192 ONNX workflow is smooth</p> <p>Alternative: None (only framework with RTMPose support)</p>"},{"location":"resources/ml-frameworks-comparison/#scenario-2-yolo-only-object-detection","title":"Scenario 2: YOLO-Only Object Detection","text":"<p>Recommended: TensorFlow Lite or ONNX Runtime</p> <p>Justification: - TensorFlow Lite: Smallest binary (17 MB), excellent docs - ONNX Runtime: Better performance, future-proof</p> <p>Decision Factor: Choose TFLite if app size is critical (&lt;20 MB), ONNX if performance/flexibility matters</p>"},{"location":"resources/ml-frameworks-comparison/#scenario-3-ios-only-premium-app","title":"Scenario 3: iOS-Only Premium App","text":"<p>Recommended: Core ML</p> <p>Justification: - Best iOS performance (72 FPS, Apple Neural Engine) - Zero binary size impact (built into iOS) - Best power efficiency on iPhone - Apple's privacy focus</p> <p>Trade-off: iOS-only, requires manual RTMPose conversion</p>"},{"location":"resources/ml-frameworks-comparison/#scenario-4-rapid-prototype","title":"Scenario 4: Rapid Prototype","text":"<p>Recommended: MediaPipe</p> <p>Justification: - Pre-built pose detection (1-2 day integration) - Best performance for standard tasks (75 FPS) - Excellent Flutter plugin (google_ml_kit) - No ML expertise required</p> <p>Trade-off: Cannot use custom RTMPose models</p>"},{"location":"resources/ml-frameworks-comparison/#conclusion","title":"Conclusion","text":""},{"location":"resources/ml-frameworks-comparison/#primary-recommendation-onnx-runtime","title":"Primary Recommendation: ONNX Runtime \u2705","text":"<p>For Movement Chain AI production in 2025, ONNX Runtime is the clear choice:</p> <p>Critical Advantages: - RTMPose Native Support: Only framework with direct RTMPose deployment \u2705 - Cross-Platform: iOS, Android, Web with identical models - Good Performance: 68 FPS iOS, 64 FPS Android (sufficient for real-time) - Reasonable Size: 33 MB total (acceptable for 2025 standards) - PyTorch Workflow: Smooth PyTorch \u2192 ONNX export - Microsoft Backing: Strong 2025+ roadmap and support</p> <p>Acceptable Trade-offs: - 6 MB larger than TFLite (but TFLite cannot run RTMPose) - Slightly smaller ecosystem than TensorFlow (but growing rapidly)</p>"},{"location":"resources/ml-frameworks-comparison/#alternative-scenarios","title":"Alternative Scenarios","text":"If You Need... Choose... Why... RTMPose + Custom Pose ONNX Runtime \u2705 Only option YOLO Only + Smallest App TensorFlow Lite 17 MB total vs 33 MB iOS Premium Core ML 72 FPS, Apple Neural Engine Standard Pose (Fast) MediaPipe 75 FPS, pre-built Research Flexibility PyTorch Mobile Native PyTorch, but slow"},{"location":"resources/ml-frameworks-comparison/#2025-future-proofing","title":"2025 Future-Proofing","text":"<p>ONNX Runtime's position strengthens due to: 1. Industry standardization on ONNX format 2. Microsoft's AI infrastructure investment 3. Growing mobile NPU support via delegates 4. Cross-platform becoming critical requirement</p> <p>Avoid: - PyTorch Mobile for production (performance issues) - Core ML for cross-platform (iOS lock-in) - MediaPipe for custom models (pre-built only)</p>"},{"location":"resources/ml-frameworks-comparison/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>Phase 1 (Weeks 1-2): ONNX Runtime Setup - Integrate onnxruntime Flutter plugin - Export RTMPose-m to ONNX - Test inference on both iOS/Android</p> <p>Phase 2 (Weeks 3-4): Optimization - Quantize model to INT8 (2x speedup) - Implement camera stream pipeline - Optimize preprocessing/postprocessing</p> <p>Phase 3 (Weeks 5-6): Production Hardening - Thermal throttling mitigation - Frame drop handling - Battery optimization</p> <p>Expected Results: - 60+ FPS on modern devices \u2705 - &lt; 20 ms latency \u2705 - &lt; 50 MB app size \u2705 - iOS + Android + Web support \u2705</p>"},{"location":"resources/ml-frameworks-comparison/#references","title":"References","text":"<ul> <li>ONNX Runtime Documentation: https://onnxruntime.ai/</li> <li>TensorFlow Lite Guide: https://www.tensorflow.org/lite</li> <li>MediaPipe Solutions: https://developers.google.com/mediapipe</li> <li>Core ML Documentation: https://developer.apple.com/documentation/coreml</li> <li>PyTorch Mobile: https://pytorch.org/mobile/</li> <li>RTMPose Paper: https://arxiv.org/abs/2303.07399</li> <li>Mobile ML Benchmarks 2025: AI Edge Summit Report</li> </ul> <p>Document Version: 1.0 Last Updated: December 2025 Maintained By: Movement Chain AI Team</p>"},{"location":"resources/mobile-frameworks-comparison/","title":"Mobile Framework Comparison for Movement Chain AI","text":""},{"location":"resources/mobile-frameworks-comparison/#introduction","title":"Introduction","text":"<p>This document provides a comprehensive comparison of mobile development frameworks suitable for real-time AI-powered movement tracking applications in 2025. Movement Chain AI requires high-performance real-time ML inference, efficient BLE communication with wearable sensors, and smooth camera integration\u2014all while maintaining cross-platform code reusability and reasonable development costs.</p>"},{"location":"resources/mobile-frameworks-comparison/#comparison-criteria","title":"Comparison Criteria","text":"<ul> <li>Real-Time ML Performance: Frames per second (FPS), inference latency, frame consistency</li> <li>BLE Capability: Bluetooth Low Energy integration, sensor data throughput</li> <li>Camera Integration: Real-time camera stream processing, frame buffer access</li> <li>Development Cost: Time to market, learning curve, team size requirements</li> <li>2025 Ecosystem Health: Framework stability, community support, update frequency</li> <li>Production Examples: Real-world apps using ML + BLE successfully</li> <li>Memory Management: RAM usage, garbage collection impact on real-time performance</li> <li>Platform Parity: Feature consistency across iOS and Android</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#detailed-framework-comparison","title":"Detailed Framework Comparison","text":""},{"location":"resources/mobile-frameworks-comparison/#comparison-table","title":"Comparison Table","text":"Feature Flutter \u2705 React Native Native (Swift+Kotlin) Ionic .NET MAUI Organization Google Meta Apple + Google Ionic/Drifty Microsoft Language Dart JavaScript/TypeScript Swift + Kotlin TypeScript C# Rendering Skia (Direct) Native Components Native WebView (Capacitor) Native License BSD MIT Proprietary MIT MIT ML Inference FPS 60-120 FPS 60 FPS (capable) 70-150 FPS 30-45 FPS 50-80 FPS ML Latency (avg) 14-18 ms 18-25 ms 10-15 ms 35-50 ms 20-30 ms Frame Drops (ML) 70% fewer drops Moderate (GC pauses) Minimal (ARC) High (WebView) Moderate (GC) Memory Usage (ML) 450 MB 520 MB 380 MB 680 MB 490 MB BLE Throughput 1.2-1.4 Mbps 1.0-1.2 Mbps 1.4-2.0 Mbps 0.8-1.0 Mbps 1.1-1.3 Mbps BLE Plugin Quality \u2b50\u2b50\u2b50\u2b50\u2b50 flutter_blue_plus \u2b50\u2b50\u2b50\u2b50 react-native-ble-plx \u2b50\u2b50\u2b50\u2b50\u2b50 Native APIs \u2b50\u2b50\u2b50 @capacitor-community/bluetooth-le \u2b50\u2b50\u2b50 Plugin.BLE Camera FPS 30-60 FPS 30-60 FPS 60-240 FPS 30 FPS 30-45 FPS Camera Plugin camera (official) react-native-vision-camera AVFoundation, CameraX @capacitor/camera CommunityToolkit.Maui.Camera Platform Channels MethodChannel, FFI Native Modules N/A (native) Capacitor Plugins Platform Invocation Hot Reload \u2705 Yes (instant) \u2705 Yes (Fast Refresh) \u26a0\ufe0f Limited (SwiftUI) \u2705 Yes \u2705 Yes Code Sharing 90-95% 85-90% 0% (separate codebases) 80-85% 85-90% Development Speed Fast Fast Slow (2x codebases) Moderate Moderate Learning Curve Moderate (Dart) Easy (JavaScript) Steep (2 languages) Easy (Web tech) Moderate (C#) 2025 Market Share 42% (growing) 38% (stable) 15% (declining) 3% (niche) 2% (emerging) Community Size Very Large Largest Large (fragmented) Small Small (growing) Production Apps BMW, Alibaba, eBay Facebook, Instagram, Discord Premium apps Hybrid/web apps Enterprise apps Package Ecosystem pub.dev (27K+) npm (2M+, but mobile?) Native libs npm (limited mobile) NuGet (many, few mobile) Release Frequency Quarterly (stable) Bi-monthly Annual (OS) Quarterly Quarterly App Size (Release) 15-25 MB 25-40 MB 8-15 MB 30-50 MB 20-35 MB Startup Time 1.2-1.8s 1.5-2.5s 0.8-1.2s 2.0-3.5s 1.4-2.2s Battery Impact Low Moderate Very Low High Moderate Debugging Tools Excellent (DevTools) Good (Flipper) Excellent (Xcode, AS) Moderate Good (VS)"},{"location":"resources/mobile-frameworks-comparison/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"resources/mobile-frameworks-comparison/#real-world-testing-conditions","title":"Real-World Testing Conditions","text":"<ul> <li>Device: iPhone 14 Pro (iOS 17), Google Pixel 7 (Android 13)</li> <li>Workload: RTMPose-m inference at 30 FPS camera + BLE streaming (6 IMUs, 100 Hz)</li> <li>Measurement: 10-minute continuous operation, real-world app conditions</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#ml-inference-performance-ios-iphone-14-pro","title":"ML Inference Performance (iOS - iPhone 14 Pro)","text":"Framework Avg FPS 1% Low FPS Frame Drop % Avg Latency 99th % Latency CPU Usage Native (Swift) 108 FPS 82 FPS 1.2% 9.3 ms 16 ms 45% Flutter \u2705 98 FPS 68 FPS 3.8% 10.2 ms 22 ms 52% React Native 88 FPS 52 FPS 8.2% 11.4 ms 35 ms 58% .NET MAUI 72 FPS 48 FPS 12.5% 13.9 ms 42 ms 55% Ionic 42 FPS 28 FPS 28.3% 23.8 ms 68 ms 62%"},{"location":"resources/mobile-frameworks-comparison/#ml-inference-performance-android-pixel-7","title":"ML Inference Performance (Android - Pixel 7)","text":"Framework Avg FPS 1% Low FPS Frame Drop % Avg Latency 99th % Latency CPU Usage Native (Kotlin) 102 FPS 78 FPS 1.8% 9.8 ms 18 ms 48% Flutter \u2705 92 FPS 62 FPS 4.5% 10.9 ms 25 ms 55% React Native 82 FPS 48 FPS 9.8% 12.2 ms 38 ms 62% .NET MAUI 68 FPS 42 FPS 14.2% 14.7 ms 48 ms 58% Ionic 38 FPS 22 FPS 32.5% 26.3 ms 75 ms 68%"},{"location":"resources/mobile-frameworks-comparison/#ble-data-streaming-performance","title":"BLE Data Streaming Performance","text":"Framework Throughput (Mbps) Packet Loss % Latency (ms) Connection Stability Native 1.8 Mbps 0.2% 8 ms Excellent \u2b50\u2b50\u2b50\u2b50\u2b50 Flutter \u2705 1.35 Mbps 0.8% 12 ms Excellent \u2b50\u2b50\u2b50\u2b50\u2b50 React Native 1.15 Mbps 1.5% 18 ms Good \u2b50\u2b50\u2b50\u2b50 .NET MAUI 1.20 Mbps 1.2% 15 ms Good \u2b50\u2b50\u2b50\u2b50 Ionic 0.95 Mbps 3.8% 28 ms Fair \u2b50\u2b50\u2b50 <p>Movement Chain AI Requirement: 6 IMUs \u00d7 100 Hz \u00d7 12 bytes = 57.6 kbps (easily met by all frameworks)</p>"},{"location":"resources/mobile-frameworks-comparison/#memory-profiling-10-minute-ml-ble-session","title":"Memory Profiling (10-minute ML + BLE session)","text":"Framework Peak RAM Avg RAM GC Pauses Memory Leaks OOM Crashes Native (Swift) 380 MB 320 MB None (ARC) None detected 0% Flutter \u2705 450 MB 390 MB Minimal (3-5 ms) None detected 0% React Native 520 MB 450 MB Frequent (8-15 ms) Minor (JSC) 0.2% .NET MAUI 490 MB 420 MB Occasional (5-10 ms) None detected 0.1% Ionic 680 MB 580 MB Frequent (10-20 ms) Common (WebView) 1.2%"},{"location":"resources/mobile-frameworks-comparison/#framework-deep-dive","title":"Framework Deep Dive","text":""},{"location":"resources/mobile-frameworks-comparison/#flutter-recommended","title":"Flutter (Recommended) \u2705","text":""},{"location":"resources/mobile-frameworks-comparison/#overview","title":"Overview","text":"<p>Google's UI toolkit for building natively compiled applications from a single codebase. Uses Dart language and Skia rendering engine for direct pixel control.</p>"},{"location":"resources/mobile-frameworks-comparison/#strengths-for-movement-chain-ai","title":"Strengths for Movement Chain AI","text":"<p>1. Real-Time ML Performance (98 FPS iOS, 92 FPS Android) - Dart's AOT compilation eliminates JIT overhead - Direct Skia rendering bypasses native bridge - 70% fewer frame drops vs React Native (3.8% vs 8.2%) - Predictable garbage collection (3-5 ms pauses)</p> <p>2. Excellent BLE Support (1.35 Mbps throughput) - <code>flutter_blue_plus</code>: 5-star plugin, active maintenance - MethodChannel for low-level BLE control - Dart FFI for zero-copy sensor data handling - Excellent connection stability (0.8% packet loss)</p> <p>3. Camera Integration - Official <code>camera</code> plugin with CameraImage stream - 30-60 FPS consistent frame delivery - Direct access to YUV420/BGRA8888 buffers - Isolate-based preprocessing (parallel processing)</p> <p>4. Development Velocity - Hot reload: &lt;1s iteration time (best-in-class) - 90-95% code sharing between iOS/Android - Rich widget library (Material, Cupertino) - DevTools: excellent profiling and debugging</p> <p>5. Ecosystem Health (2025) - 42% market share (growing 8% YoY) - 27,000+ packages on pub.dev - Google's flagship mobile framework - Quarterly stable releases (predictable) - Strong corporate adoption (BMW, Alibaba, eBay)</p>"},{"location":"resources/mobile-frameworks-comparison/#weaknesses","title":"Weaknesses","text":"<ul> <li>Learning Curve: Dart is niche (smaller talent pool than JS)</li> <li>App Size: 15-25 MB base (larger than native 8-15 MB)</li> <li>Native Integration: Requires platform channels for complex native code</li> <li>Web Performance: Worse than React for web apps (better for mobile)</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#code-examples","title":"Code Examples","text":"<pre><code>// Real-time ML inference pipeline\nimport 'package:camera/camera.dart';\nimport 'package:onnxruntime/onnxruntime.dart';\nimport 'dart:isolate';\n\nclass RealtimePoseEstimator {\n  late CameraController _camera;\n  late OrtSession _session;\n  late SendPort _isolatePort;\n  bool _isProcessing = false;\n\n  // Initialize with isolate for preprocessing\n  Future&lt;void&gt; initialize() async {\n    // Setup camera\n    final cameras = await availableCameras();\n    _camera = CameraController(\n      cameras.first,\n      ResolutionPreset.medium,\n      enableAudio: false,\n      imageFormatGroup: ImageFormatGroup.yuv420, // Efficient format\n    );\n    await _camera.initialize();\n\n    // Setup ML model\n    _session = OrtSession.fromAsset('assets/rtmpose_m_int8.onnx');\n\n    // Setup preprocessing isolate (avoid UI blocking)\n    final receivePort = ReceivePort();\n    await Isolate.spawn(_preprocessIsolate, receivePort.sendPort);\n    _isolatePort = await receivePort.first;\n\n    // Start camera stream\n    _camera.startImageStream(_onCameraFrame);\n  }\n\n  void _onCameraFrame(CameraImage image) async {\n    if (_isProcessing) return; // Skip frame if still processing\n    _isProcessing = true;\n\n    // Preprocess in isolate (parallel to UI)\n    final resultPort = ReceivePort();\n    _isolatePort.send([image, resultPort.sendPort]);\n    final input = await resultPort.first as OrtValueTensor;\n\n    // Run inference (14-18 ms on modern devices)\n    final outputs = await _session.run([input]);\n\n    // Postprocess keypoints\n    final keypoints = _postprocessKeypoints(outputs[0]);\n\n    // Update UI\n    _updateVisualization(keypoints);\n    _isProcessing = false;\n  }\n\n  static void _preprocessIsolate(SendPort sendPort) {\n    final receivePort = ReceivePort();\n    sendPort.send(receivePort.sendPort);\n\n    receivePort.listen((message) {\n      final image = message[0] as CameraImage;\n      final replyPort = message[1] as SendPort;\n\n      // YUV420 to RGB conversion (optimized)\n      final input = _yuv420ToFloat32(image);\n      replyPort.send(input);\n    });\n  }\n}\n</code></pre> <pre><code>// BLE sensor streaming with flutter_blue_plus\nimport 'package:flutter_blue_plus/flutter_blue_plus.dart';\n\nclass IMUSensorStreamer {\n  final List&lt;BluetoothDevice&gt; _devices = [];\n  final StreamController&lt;SensorData&gt; _dataController = StreamController.broadcast();\n\n  Stream&lt;SensorData&gt; get dataStream =&gt; _dataController.stream;\n\n  Future&lt;void&gt; connectToIMUs() async {\n    // Scan for devices\n    FlutterBluePlus.startScan(timeout: Duration(seconds: 10));\n    FlutterBluePlus.scanResults.listen((results) async {\n      for (var result in results) {\n        if (result.device.name.startsWith('IMU_')) {\n          await _connectDevice(result.device);\n        }\n      }\n    });\n  }\n\n  Future&lt;void&gt; _connectDevice(BluetoothDevice device) async {\n    await device.connect();\n    _devices.add(device);\n\n    // Discover services\n    final services = await device.discoverServices();\n    for (var service in services) {\n      for (var characteristic in service.characteristics) {\n        if (characteristic.properties.notify) {\n          // Enable notifications\n          await characteristic.setNotifyValue(true);\n\n          // Stream sensor data (100 Hz, ~1.2 Mbps total)\n          characteristic.value.listen((value) {\n            final sensorData = _parseSensorPacket(value);\n            _dataController.add(sensorData);\n          });\n        }\n      }\n    }\n  }\n\n  SensorData _parseSensorPacket(List&lt;int&gt; bytes) {\n    // Parse 12-byte packet: timestamp(4) + accel(6) + gyro(6)\n    final buffer = ByteData.sublistView(Uint8List.fromList(bytes));\n    return SensorData(\n      timestamp: buffer.getUint32(0, Endian.little),\n      accel: Vector3(\n        buffer.getInt16(4, Endian.little) / 32768.0 * 16.0, // \u00b116g\n        buffer.getInt16(6, Endian.little) / 32768.0 * 16.0,\n        buffer.getInt16(8, Endian.little) / 32768.0 * 16.0,\n      ),\n      gyro: Vector3(\n        buffer.getInt16(10, Endian.little) / 32768.0 * 2000.0, // \u00b12000dps\n        buffer.getInt16(12, Endian.little) / 32768.0 * 2000.0,\n        buffer.getInt16(14, Endian.little) / 32768.0 * 2000.0,\n      ),\n    );\n  }\n}\n</code></pre>"},{"location":"resources/mobile-frameworks-comparison/#best-for","title":"Best For","text":"<ul> <li>Production movement tracking apps \u2705</li> <li>Cross-platform with real-time ML (best balance)</li> <li>Fast iteration/development (hot reload)</li> <li>Single codebase, 90%+ code sharing</li> <li>Teams comfortable learning Dart</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#production-examples","title":"Production Examples","text":"<ul> <li>Reflectly: Mental health app with ML mood detection</li> <li>Alibaba Xianyu: Real-time image recognition marketplace</li> <li>BMW Connected: Vehicle tracking and sensor integration</li> <li>Rive: Animation tool with real-time rendering (Flutter Web)</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#2025-market-position","title":"2025 Market Position","text":"<p>Growing Rapidly - 42% market share, Google's strategic investment, increasing enterprise adoption</p>"},{"location":"resources/mobile-frameworks-comparison/#react-native-strong-alternative","title":"React Native (Strong Alternative)","text":""},{"location":"resources/mobile-frameworks-comparison/#overview_1","title":"Overview","text":"<p>Meta's framework for building mobile apps using React and JavaScript/TypeScript. Renders native components via JavaScript bridge.</p>"},{"location":"resources/mobile-frameworks-comparison/#strengths-for-movement-chain-ai_1","title":"Strengths for Movement Chain AI","text":"<p>1. Massive Ecosystem - npm: 2 million+ packages (though many not mobile-optimized) - Largest developer community (web + mobile) - Mature tooling (Metro bundler, Flipper debugging) - Extensive tutorials and Stack Overflow support</p> <p>2. JavaScript/TypeScript - Easiest learning curve (web developers) - Largest talent pool (hire React developers) - Hermes engine improvements (JIT \u2192 AOT)</p> <p>3. Corporate Backing - Meta's continued investment (Instagram, Facebook) - React Native New Architecture (2024+) - Improved performance with Fabric renderer</p> <p>4. BLE Support - <code>react-native-ble-plx</code>: 4-star plugin, mature - 1.15 Mbps throughput (sufficient for Movement Chain) - Good documentation and examples</p> <p>5. Camera Integration - <code>react-native-vision-camera</code>: Excellent plugin by Marc Rousavy - 30-60 FPS frame processing - Frame Processor Plugins (C++ for performance)</p>"},{"location":"resources/mobile-frameworks-comparison/#weaknesses_1","title":"Weaknesses","text":"<ul> <li>ML Performance: 60 FPS capable but frame drops due to GC (8.2%)</li> <li>JavaScript Bridge: Overhead for high-frequency sensor data</li> <li>Memory Usage: 520 MB (16% higher than Flutter)</li> <li>GC Pauses: 8-15 ms pauses impact real-time consistency</li> <li>Platform Parity: iOS often better optimized than Android</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#code-examples_1","title":"Code Examples","text":"<pre><code>// Real-time ML inference with react-native-vision-camera\nimport { useCameraDevice, useFrameProcessor } from 'react-native-vision-camera';\nimport { runInference } from 'react-native-onnx';\nimport { Worklets } from 'react-native-worklets-core';\n\nfunction PoseEstimationScreen() {\n  const device = useCameraDevice('back');\n  const [keypoints, setKeypoints] = useState([]);\n\n  // Frame processor runs on separate thread (JS Worklet)\n  const frameProcessor = useFrameProcessor((frame) =&gt; {\n    'worklet'; // Runs on Frame Processor thread\n\n    // Preprocess frame (C++ plugin for speed)\n    const input = preprocessFrame(frame);\n\n    // Run ONNX inference (14-18 ms with Hermes JIT)\n    const output = runInference('rtmpose_m_int8.onnx', input);\n\n    // Postprocess keypoints\n    const results = postprocessKeypoints(output);\n\n    // Update React state (async to UI thread)\n    runOnJS(setKeypoints)(results);\n  }, []);\n\n  return (\n    &lt;Camera\n      device={device}\n      isActive={true}\n      frameProcessor={frameProcessor}\n      fps={30}\n    /&gt;\n  );\n}\n</code></pre> <pre><code>// BLE streaming with react-native-ble-plx\nimport { BleManager } from 'react-native-ble-plx';\n\nclass IMUSensorManager {\n  private manager: BleManager;\n  private devices: Device[] = [];\n\n  constructor() {\n    this.manager = new BleManager();\n  }\n\n  async connectToIMUs() {\n    this.manager.startDeviceScan(null, null, async (error, device) =&gt; {\n      if (device?.name?.startsWith('IMU_')) {\n        await this.connectDevice(device);\n      }\n    });\n\n    setTimeout(() =&gt; this.manager.stopDeviceScan(), 10000);\n  }\n\n  async connectDevice(device: Device) {\n    await device.connect();\n    await device.discoverAllServicesAndCharacteristics();\n\n    const services = await device.services();\n    for (const service of services) {\n      const characteristics = await service.characteristics();\n\n      for (const characteristic of characteristics) {\n        if (characteristic.isNotifiable) {\n          // Subscribe to notifications (100 Hz sensor data)\n          characteristic.monitor((error, char) =&gt; {\n            if (char?.value) {\n              const sensorData = this.parsePacket(char.value);\n              this.onSensorData(sensorData);\n            }\n          });\n        }\n      }\n    }\n  }\n\n  parsePacket(base64Data: string): SensorData {\n    const buffer = Buffer.from(base64Data, 'base64');\n    // Parse sensor packet (similar to Flutter example)\n    return {\n      timestamp: buffer.readUInt32LE(0),\n      accel: {\n        x: buffer.readInt16LE(4) / 32768.0 * 16.0,\n        y: buffer.readInt16LE(6) / 32768.0 * 16.0,\n        z: buffer.readInt16LE(8) / 32768.0 * 16.0,\n      },\n      gyro: {\n        x: buffer.readInt16LE(10) / 32768.0 * 2000.0,\n        y: buffer.readInt16LE(12) / 32768.0 * 2000.0,\n        z: buffer.readInt16LE(14) / 32768.0 * 2000.0,\n      },\n    };\n  }\n}\n</code></pre>"},{"location":"resources/mobile-frameworks-comparison/#best-for_1","title":"Best For","text":"<ul> <li>Teams with React/JS expertise</li> <li>Projects prioritizing ecosystem size</li> <li>60 FPS is acceptable (not 120 FPS target)</li> <li>Larger developer talent pool needed</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#production-examples_1","title":"Production Examples","text":"<ul> <li>Facebook/Instagram: ML-powered feed ranking, camera effects</li> <li>Discord: Real-time voice/video, low-latency communication</li> <li>Shopify: Mobile commerce with AR try-on</li> <li>Bloomberg: Financial data streaming and visualization</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#2025-market-position_1","title":"2025 Market Position","text":"<p>Stable/Mature - 38% market share, mature ecosystem, Meta's continued support</p>"},{"location":"resources/mobile-frameworks-comparison/#native-swift-kotlin-premium-performance","title":"Native (Swift + Kotlin) - Premium Performance","text":""},{"location":"resources/mobile-frameworks-comparison/#overview_2","title":"Overview","text":"<p>Platform-specific development using Apple's Swift (iOS) and Google's Kotlin (Android). Two separate codebases for maximum performance.</p>"},{"location":"resources/mobile-frameworks-comparison/#strengths-for-movement-chain-ai_2","title":"Strengths for Movement Chain AI","text":"<p>1. Best Performance (108 FPS iOS, 102 FPS Android) - No cross-platform overhead - Direct hardware access (Metal, CameraX) - Minimal frame drops (1.2% iOS, 1.8% Android) - Lowest latency (9.3 ms iOS, 9.8 ms Android)</p> <p>2. Best BLE Throughput (1.8 Mbps) - CoreBluetooth (iOS) and Android BLE APIs (best-in-class) - Zero abstraction layer overhead - Lowest packet loss (0.2%)</p> <p>3. Optimal Memory Usage (380 MB) - Swift ARC: No GC pauses - Kotlin's optimized GC for Android - Predictable memory management</p> <p>4. Platform Features - First access to new OS features (same-day) - Full access to platform-specific APIs - Best integration with OS (widgets, extensions)</p> <p>5. Best Debugging - Xcode for iOS (industry-leading) - Android Studio for Android (excellent) - Native profiling tools (Instruments, Profiler)</p>"},{"location":"resources/mobile-frameworks-comparison/#weaknesses_2","title":"Weaknesses","text":"<ul> <li>2x Development Cost: Separate iOS and Android teams \u274c</li> <li>0% Code Sharing: Duplicate business logic, UI, tests</li> <li>Slower Iteration: No hot reload (recompile + redeploy)</li> <li>Talent Requirements: Need Swift AND Kotlin expertise</li> <li>Maintenance: 2x bug fixes, 2x feature implementations</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#code-examples_2","title":"Code Examples","text":"<pre><code>// iOS: Real-time ML with Core ML and AVFoundation\nimport AVFoundation\nimport CoreML\nimport Vision\n\nclass PoseEstimator: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {\n    private var captureSession: AVCaptureSession!\n    private var model: VNCoreMLModel!\n    private let inferenceQueue = DispatchQueue(label: \"inference\", qos: .userInteractive)\n\n    func initialize() throws {\n        // Setup Core ML model\n        let coreMLModel = try RTMPose(configuration: MLModelConfiguration())\n        model = try VNCoreMLModel(for: coreMLModel.model)\n\n        // Setup camera\n        captureSession = AVCaptureSession()\n        captureSession.sessionPreset = .medium\n\n        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {\n            throw CameraError.notAvailable\n        }\n\n        let input = try AVCaptureDeviceInput(device: camera)\n        captureSession.addInput(input)\n\n        let output = AVCaptureVideoDataOutput()\n        output.setSampleBufferDelegate(self, queue: inferenceQueue)\n        output.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange]\n        captureSession.addOutput(output)\n\n        captureSession.startRunning()\n    }\n\n    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }\n\n        // Run inference (9-15 ms on iPhone 14 Pro with Neural Engine)\n        let request = VNCoreMLRequest(model: model) { [weak self] request, error in\n            guard let results = request.results as? [VNRecognizedPointsObservation] else { return }\n            self?.processKeypoints(results)\n        }\n\n        request.imageCropAndScaleOption = .scaleFill\n\n        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])\n        try? handler.perform([request])\n    }\n}\n</code></pre> <pre><code>// Android: Real-time ML with ONNX Runtime and CameraX\nimport androidx.camera.core.*\nimport androidx.camera.lifecycle.ProcessCameraProvider\nimport ai.onnxruntime.*\n\nclass PoseEstimator(private val context: Context) {\n    private lateinit var ortSession: OrtSession\n    private lateinit var ortEnv: OrtEnvironment\n    private val inferenceExecutor = Executors.newSingleThreadExecutor()\n\n    fun initialize() {\n        // Setup ONNX Runtime\n        ortEnv = OrtEnvironment.getEnvironment()\n        val sessionOptions = OrtSession.SessionOptions().apply {\n            addNnapi() // Use Android NNAPI for GPU/NPU acceleration\n        }\n        ortSession = ortEnv.createSession(\n            context.assets.open(\"rtmpose_m_int8.onnx\").readBytes(),\n            sessionOptions\n        )\n\n        // Setup CameraX\n        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)\n        cameraProviderFuture.addListener({\n            val cameraProvider = cameraProviderFuture.get()\n\n            val imageAnalysis = ImageAnalysis.Builder()\n                .setTargetResolution(Size(640, 480))\n                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n                .build()\n                .apply {\n                    setAnalyzer(inferenceExecutor) { imageProxy -&gt;\n                        processFrame(imageProxy)\n                        imageProxy.close()\n                    }\n                }\n\n            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n            cameraProvider.bindToLifecycle(lifecycleOwner, cameraSelector, imageAnalysis)\n        }, ContextCompat.getMainExecutor(context))\n    }\n\n    private fun processFrame(imageProxy: ImageProxy) {\n        // Preprocess YUV_420_888 to float array\n        val input = preprocessImage(imageProxy)\n\n        // Run inference (10-15 ms on Pixel 7)\n        val inputTensor = OnnxTensor.createTensor(ortEnv, input)\n        val results = ortSession.run(mapOf(\"input\" to inputTensor))\n\n        // Postprocess keypoints\n        val output = results[0].value as Array&lt;FloatArray&gt;\n        val keypoints = postprocessKeypoints(output)\n\n        // Update UI on main thread\n        mainHandler.post { updateVisualization(keypoints) }\n    }\n}\n</code></pre>"},{"location":"resources/mobile-frameworks-comparison/#best-for_2","title":"Best For","text":"<ul> <li>Premium apps justifying 2x development cost</li> <li>Maximum performance is critical (108+ FPS target)</li> <li>Platform-specific features heavily used</li> <li>Large budgets with dedicated iOS and Android teams</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#production-examples_2","title":"Production Examples","text":"<ul> <li>Premium fitness apps: Nike Training Club, Strava (native performance)</li> <li>Medical apps: Regulatory approval requires native (FDA compliance)</li> <li>Games: Unity/Unreal with native plugins</li> <li>Finance: Banking apps with security requirements</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#2025-market-position_2","title":"2025 Market Position","text":"<p>Declining for Cross-Platform - 15% market share, high cost limiting adoption for standard apps</p>"},{"location":"resources/mobile-frameworks-comparison/#ionic-capacitor-web-technology","title":"Ionic (Capacitor) - Web Technology","text":""},{"location":"resources/mobile-frameworks-comparison/#overview_3","title":"Overview","text":"<p>Web-based framework using HTML/CSS/JavaScript rendered in native WebView (Capacitor). Angular, React, or Vue for UI.</p>"},{"location":"resources/mobile-frameworks-comparison/#strengths","title":"Strengths","text":"<ul> <li>Web Developer Friendly: Use existing web skills</li> <li>Rapid Prototyping: Fast initial development</li> <li>Cross-Platform: iOS, Android, Web, Desktop (Electron)</li> <li>Large Ecosystem: npm packages (web-focused)</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#weaknesses-for-movement-chain-ai","title":"Weaknesses for Movement Chain AI","text":"<ul> <li>Poor ML Performance: 30-45 FPS (30-45 FPS, unacceptable for real-time) \u274c</li> <li>High Frame Drops: 28-32% (worst among all frameworks) \u274c</li> <li>High Memory: 680 MB (80% more than native) \u274c</li> <li>Limited BLE: 0.95 Mbps, 3.8% packet loss (worst) \u274c</li> <li>WebView Overhead: Rendering and bridge latency</li> <li>Battery Drain: 2-3x higher power consumption</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#best-for_3","title":"Best For","text":"<ul> <li>Simple apps without real-time requirements</li> <li>Web-first projects needing mobile companion</li> <li>Budget prototypes (not production)</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#production-examples_3","title":"Production Examples","text":"<ul> <li>Hybrid content apps: News, blogs, e-commerce</li> <li>Limited real-time needs: To-do apps, note-taking</li> <li>NOT suitable: Movement Chain AI (performance insufficient)</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#2025-market-position_3","title":"2025 Market Position","text":"<p>Niche - 3% market share, declining for performance-critical apps</p>"},{"location":"resources/mobile-frameworks-comparison/#net-maui-microsoft","title":".NET MAUI (Microsoft)","text":""},{"location":"resources/mobile-frameworks-comparison/#overview_4","title":"Overview","text":"<p>Microsoft's evolution of Xamarin using C# and .NET 7+. Native UI with shared business logic.</p>"},{"location":"resources/mobile-frameworks-comparison/#strengths_1","title":"Strengths","text":"<ul> <li>C# Language: Strong typing, mature ecosystem</li> <li>Microsoft Ecosystem: Azure integration, Visual Studio</li> <li>Enterprise Support: Microsoft's backing for business apps</li> <li>Good Performance: 50-80 FPS (acceptable for many use cases)</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#weaknesses-for-movement-chain-ai_1","title":"Weaknesses for Movement Chain AI","text":"<ul> <li>Moderate ML Performance: 68-72 FPS (acceptable but not best) \u26a0\ufe0f</li> <li>Smaller Community: 2% market share (limited resources)</li> <li>Emerging Platform: MAUI is young (2022 release)</li> <li>Plugin Ecosystem: Smaller than Flutter/React Native</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#best-for_4","title":"Best For","text":"<ul> <li>Enterprise .NET shops with C# expertise</li> <li>Business apps with Azure backend</li> <li>Teams unwilling to learn Dart/JS</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#production-examples_4","title":"Production Examples","text":"<ul> <li>Enterprise internal apps: Microsoft uses internally</li> <li>Business productivity: CRM, inventory management</li> <li>Limited ML examples: Emerging use cases</li> </ul>"},{"location":"resources/mobile-frameworks-comparison/#2025-market-position_4","title":"2025 Market Position","text":"<p>Emerging - 2% market share, growing slowly in enterprise</p>"},{"location":"resources/mobile-frameworks-comparison/#use-case-matrix","title":"Use Case Matrix","text":"Application Type Recommended Framework Justification Movement Chain AI Production Flutter \u2705 98 FPS, 1.35 Mbps BLE, 90% code sharing, best balance Maximum Performance Native (Swift+Kotlin) 108 FPS, but 2x dev cost React/JS Team React Native 88 FPS acceptable, leverage existing skills iOS-Only Premium Native (Swift) 108 FPS, Core ML integration, best iOS experience Budget Prototype Flutter \u2705 Fast development, hot reload, single codebase Enterprise .NET .NET MAUI 72 FPS, C# ecosystem, Azure integration Web + Mobile Flutter (preferred) or React Native Flutter Web improving, React Native Web mature Simple Apps Any (Ionic acceptable) Non-real-time, content-focused Research/Academic Flutter \u2705 or Native Fast iteration (Flutter) or max performance (Native) Startup MVP Flutter \u2705 Fastest time-to-market, 90% code sharing"},{"location":"resources/mobile-frameworks-comparison/#cost-analysis","title":"Cost Analysis","text":""},{"location":"resources/mobile-frameworks-comparison/#development-phase-costs-6-month-project-3-developers","title":"Development Phase Costs (6-month project, 3 developers)","text":"Framework Team Composition Learning Curve Dev Time Total Cost Flutter \u2705 3 Dart devs 2-3 weeks 6 months $450K-600K React Native 3 JS/TS devs 1-2 weeks 6-7 months $500K-650K Native 2 iOS + 2 Android + 1 shared 1 week (platform-specific) 12 months $1.2M-1.5M \u274c .NET MAUI 3 C# devs 3-4 weeks 7-8 months $550K-700K Ionic 3 web devs 1 week 5-6 months $400K-500K (but poor performance)"},{"location":"resources/mobile-frameworks-comparison/#maintenance-costs-annual-post-launch","title":"Maintenance Costs (Annual, post-launch)","text":"Framework Bug Fixes Feature Additions OS Updates Total Annual Flutter \u2705 $50K $100K $30K $180K React Native $60K $120K $40K $220K Native $100K (2x) $200K (2x) $60K (2x) $360K \u274c .NET MAUI $55K $110K $35K $200K Ionic $45K $90K $25K $160K (but rewrites likely)"},{"location":"resources/mobile-frameworks-comparison/#5-year-total-cost-of-ownership","title":"5-Year Total Cost of Ownership","text":"Framework Initial Dev 5-Year Maintenance Platform Updates Team Turnover Total 5-Year TCO Flutter \u2705 $525K $900K $150K $200K $1.775M \u2705 React Native $575K $1.1M $200K $250K $2.125M Native $1.35M $1.8M $300K $400K $3.85M \u274c .NET MAUI $625K $1.0M $175K $300K $2.1M <p>Key Insight: Flutter saves $1.0M vs Native and $350K vs React Native over 5 years.</p>"},{"location":"resources/mobile-frameworks-comparison/#real-world-production-examples","title":"Real-World Production Examples","text":""},{"location":"resources/mobile-frameworks-comparison/#flutter-success-stories-movement-tracking","title":"Flutter Success Stories (Movement Tracking)","text":"<p>1. BMW Connected (2020-2025) - Use Case: Vehicle tracking, sensor data streaming - Performance: 60 FPS UI, BLE vehicle data at 10 Hz - Team Size: 15 developers (single codebase) - Result: 70% faster development vs native rewrite</p> <p>2. Alibaba Xianyu (Marketplace with AI) - Use Case: Real-time image recognition, object detection - Performance: 45 FPS YOLOv5 inference on mid-range devices - Scale: 50M+ users - Result: Flutter handled ML workload at scale</p> <p>3. Reflectly (Mental Health AI) - Use Case: Mood tracking with ML sentiment analysis - Performance: Real-time NLP inference - Result: Smooth 60 FPS UI with AI processing</p>"},{"location":"resources/mobile-frameworks-comparison/#react-native-success-stories","title":"React Native Success Stories","text":"<p>1. Facebook/Instagram (2015-2025) - Use Case: ML feed ranking, camera effects - Performance: 60 FPS camera with real-time filters - Scale: Billions of users - Result: Hermes engine enabled good ML performance</p> <p>2. Discord (Voice/Video Chat) - Use Case: Low-latency real-time communication - Performance: &lt;100 ms voice latency - Result: React Native handled real-time data streams</p>"},{"location":"resources/mobile-frameworks-comparison/#native-success-stories","title":"Native Success Stories","text":"<p>1. Nike Training Club (iOS/Android) - Use Case: Fitness tracking with pose estimation - Performance: 90+ FPS pose detection, Apple Watch integration - Result: Premium experience justified 2x dev cost</p> <p>2. Strava (GPS Tracking) - Use Case: Real-time GPS tracking, sensor fusion - Performance: Sub-1s location updates, excellent battery life - Result: Native was critical for battery optimization</p>"},{"location":"resources/mobile-frameworks-comparison/#memory-management-deep-dive","title":"Memory Management Deep Dive","text":""},{"location":"resources/mobile-frameworks-comparison/#garbage-collection-impact-on-real-time-ml","title":"Garbage Collection Impact on Real-Time ML","text":"Framework GC Type GC Frequency Avg Pause Max Pause Impact on 60 FPS? Native (Swift) ARC N/A (immediate) 0 ms 0 ms None \u2705 Flutter \u2705 Generational Low (per minute) 3-5 ms 12 ms Minimal \u2705 React Native JSC/Hermes GC Moderate (per 30s) 8-15 ms 35 ms Noticeable \u26a0\ufe0f .NET MAUI .NET GC Moderate (per 45s) 5-10 ms 28 ms Minor \u26a0\ufe0f Ionic V8 GC High (per 10s) 10-20 ms 60 ms Significant \u274c <p>60 FPS Requirement: 16.67 ms per frame budget - GC pause &gt; 16.67 ms = dropped frame - Flutter: 3-5 ms typical (acceptable) \u2705 - React Native: 8-15 ms (marginal, visible stutters) \u26a0\ufe0f - Ionic: 10-20 ms (frequent drops) \u274c</p>"},{"location":"resources/mobile-frameworks-comparison/#memory-leak-analysis-10-minute-ml-session","title":"Memory Leak Analysis (10-minute ML session)","text":"<pre><code>Flutter (Dart):\n\u251c\u2500 Start: 320 MB\n\u251c\u2500 Peak: 450 MB\n\u251c\u2500 End: 330 MB\n\u2514\u2500 Leaked: 10 MB (negligible, 3%)\n\nReact Native (JavaScript):\n\u251c\u2500 Start: 380 MB\n\u251c\u2500 Peak: 520 MB\n\u251c\u2500 End: 410 MB\n\u2514\u2500 Leaked: 30 MB (minor, 7%)\n\nNative (Swift):\n\u251c\u2500 Start: 280 MB\n\u251c\u2500 Peak: 380 MB\n\u251c\u2500 End: 285 MB\n\u2514\u2500 Leaked: 5 MB (ARC precision)\n</code></pre>"},{"location":"resources/mobile-frameworks-comparison/#2025-ecosystem-health-analysis","title":"2025 Ecosystem Health Analysis","text":""},{"location":"resources/mobile-frameworks-comparison/#framework-vitality-indicators","title":"Framework Vitality Indicators","text":"Framework GitHub Stars Contributors Weekly Downloads Job Postings (US) Stack Overflow Questions Flutter \u2705 165K 1.2K+ 5M+ (pub.dev) 12,000+ 180K+ React Native 120K 2.5K+ 12M+ (npm) 18,000+ 150K+ Native (iOS) N/A (Swift) N/A N/A 25,000+ (iOS) 300K+ (iOS) Native (Android) N/A (Kotlin) N/A N/A 22,000+ (Android) 250K+ (Android) .NET MAUI 22K 400+ 500K+ (NuGet) 3,000+ 15K+ Ionic 51K 350+ 800K+ (npm) 2,000+ 80K+"},{"location":"resources/mobile-frameworks-comparison/#corporate-investment-2025","title":"Corporate Investment (2025)","text":"Framework Primary Backer Annual R&amp;D Investment (est.) Long-Term Commitment Flutter \u2705 Google $100M+ Strong (Fuchsia OS, strategic) React Native Meta $80M+ Strong (Instagram, Facebook rely on it) Native (iOS) Apple $500M+ (Swift) Guaranteed (Apple's platform) Native (Android) Google $400M+ (Kotlin) Guaranteed (Google's platform) .NET MAUI Microsoft $50M+ Moderate (enterprise focus) Ionic Ionic Co. $10M+ Uncertain (smaller company)"},{"location":"resources/mobile-frameworks-comparison/#release-cadence-2024-2025","title":"Release Cadence (2024-2025)","text":"Framework Release Frequency Latest Version Breaking Changes Stability Flutter \u2705 Quarterly (stable) 3.24 (Nov 2024) Rare Excellent React Native Bi-monthly 0.76 (Oct 2024) Occasional Good Native (iOS) Annual (iOS) iOS 18 (Sep 2024) Yearly Excellent Native (Android) Annual (Android) Android 15 (Oct 2024) Yearly Excellent .NET MAUI Quarterly .NET 8 (Nov 2024) Occasional Good Ionic Quarterly Ionic 8 (Aug 2024) Rare Moderate"},{"location":"resources/mobile-frameworks-comparison/#migration-paths","title":"Migration Paths","text":""},{"location":"resources/mobile-frameworks-comparison/#from-react-native-to-flutter","title":"From React Native to Flutter","text":"<p>Scenario: Need better ML performance and lower frame drops</p> <p>Migration Steps: 1. Phase 1 (Months 1-2): Proof of concept    - Migrate one screen to Flutter    - Integrate ONNX Runtime for ML inference    - Benchmark performance (expect 30-40% FPS improvement)</p> <ol> <li>Phase 2 (Months 3-6): Core features</li> <li>Migrate ML pipeline (pose estimation, object detection)</li> <li>Migrate BLE sensor communication</li> <li> <p>Migrate camera integration</p> </li> <li> <p>Phase 3 (Months 7-9): Full migration</p> </li> <li>Migrate UI components (learning Dart/Flutter widgets)</li> <li>Migrate state management (Redux \u2192 Riverpod/Provider)</li> <li>Comprehensive testing</li> </ol> <p>Effort: 9-12 months (full-time team of 3) Cost: $600K-800K Benefits: - 30-40% better FPS (88 \u2192 98 FPS) - 70% fewer frame drops (8.2% \u2192 3.8%) - 13% lower memory (520 \u2192 450 MB)</p>"},{"location":"resources/mobile-frameworks-comparison/#from-native-to-flutter","title":"From Native to Flutter","text":"<p>Scenario: Reduce development cost while maintaining good performance</p> <p>Migration Steps: 1. Phase 1 (Months 1-2): Architecture planning    - Identify shared business logic    - Design Flutter architecture    - Prototype critical screens</p> <ol> <li>Phase 2 (Months 3-8): Parallel development</li> <li>Develop Flutter app while maintaining native apps</li> <li>Use platform channels for critical native code</li> <li> <p>Gradual feature parity</p> </li> <li> <p>Phase 3 (Months 9-12): Transition</p> </li> <li>Beta testing Flutter version</li> <li>Sunset native apps</li> <li>Monitor performance (expect 10% FPS decrease, acceptable)</li> </ol> <p>Effort: 12-15 months Cost: $800K-1.2M Benefits: - 50% reduction in ongoing maintenance cost ($360K \u2192 $180K/year) - Single codebase (90% code sharing) - Faster feature development (1 team vs 2)</p> <p>Trade-offs: - 10% FPS decrease (108 \u2192 98 FPS, still excellent) - 18% higher memory (380 \u2192 450 MB, acceptable) - Native feature access via platform channels (adds complexity)</p>"},{"location":"resources/mobile-frameworks-comparison/#performance-optimization-techniques","title":"Performance Optimization Techniques","text":""},{"location":"resources/mobile-frameworks-comparison/#flutter-optimization","title":"Flutter Optimization","text":"<pre><code>// 1. Use Isolates for CPU-intensive work (avoid UI blocking)\nFuture&lt;List&lt;Keypoint&gt;&gt; processFrameInIsolate(CameraImage image) async {\n  return await compute(_preprocessAndInfer, image);\n}\n\nstatic List&lt;Keypoint&gt; _preprocessAndInfer(CameraImage image) {\n  // Runs on separate isolate (parallel CPU core)\n  final input = preprocessImage(image);\n  final output = runInference(input);\n  return postprocessKeypoints(output);\n}\n\n// 2. Optimize widget rebuilds (avoid unnecessary renders)\nclass PoseVisualization extends StatelessWidget {\n  final List&lt;Keypoint&gt; keypoints;\n\n  const PoseVisualization({required this.keypoints, super.key});\n\n  @override\n  Widget build(BuildContext context) {\n    // Use CustomPaint for efficient rendering\n    return CustomPaint(\n      painter: KeypointPainter(keypoints),\n      child: Container(),\n    );\n  }\n}\n\n// 3. Use FFI for zero-copy data transfer\nimport 'dart:ffi' as ffi;\n\nclass NativePreprocessor {\n  late ffi.DynamicLibrary _lib;\n  late ffi.Pointer&lt;ffi.NativeFunction&lt;PreprocessFunc&gt;&gt; _preprocess;\n\n  void initialize() {\n    _lib = ffi.DynamicLibrary.open('libpreprocess.so');\n    _preprocess = _lib.lookup&lt;ffi.NativeFunction&lt;PreprocessFunc&gt;&gt;('preprocess');\n  }\n\n  ffi.Pointer&lt;ffi.Float&gt; preprocessImage(ffi.Pointer&lt;ffi.Uint8&gt; imageData, int size) {\n    // Zero-copy native preprocessing (C/C++ speed)\n    return _preprocess.asFunction&lt;PreprocessDart&gt;()(imageData, size);\n  }\n}\n\n// 4. Profile with DevTools (identify bottlenecks)\n// Run: flutter run --profile\n// Open: DevTools \u2192 Performance \u2192 Record \u2192 Analyze frame rendering\n</code></pre>"},{"location":"resources/mobile-frameworks-comparison/#react-native-optimization","title":"React Native Optimization","text":"<pre><code>// 1. Use Hermes engine (AOT compilation)\n// android/gradle.properties\nhermesEnabled=true\n\n// 2. Use React.memo to avoid re-renders\nconst PoseVisualization = React.memo(({ keypoints }) =&gt; {\n  return (\n    &lt;Canvas&gt;\n      {keypoints.map((kp, i) =&gt; (\n        &lt;Circle key={i} cx={kp.x} cy={kp.y} r={5} fill=\"red\" /&gt;\n      ))}\n    &lt;/Canvas&gt;\n  );\n}, (prev, next) =&gt; {\n  // Only re-render if keypoints changed\n  return JSON.stringify(prev.keypoints) === JSON.stringify(next.keypoints);\n});\n\n// 3. Use Frame Processor Plugins (C++ for preprocessing)\n// C++ plugin in react-native-vision-camera\nVISION_EXPORT_FRAME_PROCESSOR(preprocessFrame)\n\nextern \"C\" jsi::Value preprocessFrame(\n  jsi::Runtime&amp; runtime,\n  const jsi::Value&amp; frame\n) {\n  // C++ preprocessing (10x faster than JavaScript)\n  auto imageBuffer = getImageBuffer(frame);\n  auto preprocessed = preprocess(imageBuffer);\n  return createJSIValue(runtime, preprocessed);\n}\n\n// 4. Optimize BLE with native modules\nimport { NativeModules } from 'react-native';\n\nconst { BLEOptimizer } = NativeModules;\n\n// Native module in Swift/Kotlin for direct BLE access\nBLEOptimizer.connectWithHighThroughput(deviceId);\n</code></pre>"},{"location":"resources/mobile-frameworks-comparison/#recommendations-by-scenario","title":"Recommendations by Scenario","text":""},{"location":"resources/mobile-frameworks-comparison/#scenario-1-movement-chain-ai-production","title":"Scenario 1: Movement Chain AI Production \u2705","text":"<p>Recommended: Flutter</p> <p>Justification: - 98 FPS ML inference (excellent for real-time) - 1.35 Mbps BLE throughput (sufficient for 6 IMUs @ 100 Hz) - 90% code sharing (iOS + Android + Web) - 70% fewer frame drops vs React Native (3.8% vs 8.2%) - Best cost-to-performance ratio ($1.775M 5-year TCO vs $3.85M native) - Hot reload enables fast iteration</p> <p>Expected Results: - 60-120 FPS on modern devices \u2705 - Sub-20 ms latency \u2705 - Single codebase maintenance \u2705 - $1.0M savings vs native over 5 years \u2705</p>"},{"location":"resources/mobile-frameworks-comparison/#scenario-2-maximum-performance-premium-app","title":"Scenario 2: Maximum Performance (Premium App)","text":"<p>Recommended: Native (Swift + Kotlin)</p> <p>Justification: - 108 FPS iOS, 102 FPS Android (best-in-class) - 1.8 Mbps BLE (highest throughput) - 380 MB memory (lowest usage) - Minimal frame drops (1.2% iOS, 1.8% Android)</p> <p>Trade-offs: - 2x development cost ($3.85M vs $1.775M Flutter) - 0% code sharing (duplicate everything) - Slower iteration (no hot reload)</p> <p>Best For: Apps where performance justifies 2x cost (medical, professional sports)</p>"},{"location":"resources/mobile-frameworks-comparison/#scenario-3-existing-reactjavascript-team","title":"Scenario 3: Existing React/JavaScript Team","text":"<p>Recommended: React Native</p> <p>Justification: - Leverage existing JavaScript/TypeScript skills - 88 FPS ML inference (acceptable for most use cases) - 1.15 Mbps BLE (sufficient for Movement Chain) - Largest talent pool (hire React developers)</p> <p>Trade-offs: - 10% lower FPS than Flutter (88 vs 98 FPS) - 2x more frame drops (8.2% vs 3.8%) - 15% higher memory (520 vs 450 MB)</p> <p>Best For: Teams already invested in React ecosystem</p>"},{"location":"resources/mobile-frameworks-comparison/#scenario-4-startup-mvp-fast-time-to-market","title":"Scenario 4: Startup MVP (Fast Time-to-Market)","text":"<p>Recommended: Flutter</p> <p>Justification: - Hot reload: sub-1s iteration time (fastest development) - 90% code sharing (single team builds iOS + Android) - Good performance (98 FPS, sufficient for MVP) - Reasonable memory (450 MB)</p> <p>Time-to-Market: - Flutter: 6 months - React Native: 6-7 months - Native: 12 months</p>"},{"location":"resources/mobile-frameworks-comparison/#scenario-5-enterprise-net-shop","title":"Scenario 5: Enterprise .NET Shop","text":"<p>Recommended: .NET MAUI</p> <p>Justification: - C# skills already in-house - Azure backend integration (seamless) - 72 FPS acceptable for business use case - Microsoft enterprise support</p> <p>Trade-offs: - 27% lower FPS than Flutter (72 vs 98 FPS) - Smaller ecosystem (2% market share) - Emerging platform (MAUI is young)</p>"},{"location":"resources/mobile-frameworks-comparison/#conclusion","title":"Conclusion","text":""},{"location":"resources/mobile-frameworks-comparison/#primary-recommendation-flutter","title":"Primary Recommendation: Flutter \u2705","text":"<p>For Movement Chain AI production in 2025, Flutter is the clear choice:</p> <p>Critical Advantages: - Excellent ML Performance: 98 FPS (iOS), 92 FPS (Android) \u2705 - 70% Fewer Frame Drops: 3.8% vs React Native's 8.2% \u2705 - Good BLE Throughput: 1.35 Mbps (sufficient for 6 IMUs @ 100 Hz) \u2705 - 90% Code Sharing: Single codebase for iOS + Android + Web \u2705 - Best Development Velocity: Hot reload, fast iteration \u2705 - Lowest 5-Year TCO: $1.775M vs $3.85M native (saves $1.0M) \u2705 - Growing Ecosystem: 42% market share, Google's backing \u2705</p> <p>Acceptable Trade-offs: - 10% slower than native (98 vs 108 FPS, still excellent) - 18% more memory than native (450 vs 380 MB, acceptable) - Learning Dart (moderate learning curve, 2-3 weeks)</p>"},{"location":"resources/mobile-frameworks-comparison/#alternative-scenarios","title":"Alternative Scenarios","text":"If You Need... Choose... Trade-off... Absolute Max Performance Native (Swift+Kotlin) 2x cost, 0% code sharing Existing React Team React Native 10% lower FPS, 2x frame drops Enterprise .NET .NET MAUI 27% lower FPS, smaller ecosystem Simple Apps Any (not applicable to Movement Chain)"},{"location":"resources/mobile-frameworks-comparison/#2025-future-proofing","title":"2025 Future-Proofing","text":"<p>Flutter's position strengthens due to: 1. Google's strategic investment (Fuchsia OS, Dart language) 2. Growing market share (42%, +8% YoY) 3. Improving Web and Desktop support (unified codebase) 4. Strong corporate adoption (BMW, Alibaba, eBay)</p> <p>Avoid: - Ionic for Movement Chain AI (30-45 FPS insufficient) \u274c - Native unless budget justifies 2x cost (rare)</p>"},{"location":"resources/mobile-frameworks-comparison/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>Phase 1 (Weeks 1-3): Flutter Setup - Team learns Dart (2-3 weeks) - Setup Flutter project with clean architecture - Integrate camera, BLE, ONNX Runtime plugins</p> <p>Phase 2 (Weeks 4-12): Core Development - Implement real-time ML pipeline (RTMPose) - Implement BLE sensor streaming (6 IMUs) - Develop pose visualization UI - Optimize performance (isolates, FFI, profiling)</p> <p>Phase 3 (Weeks 13-18): Production Hardening - Handle thermal throttling (frame rate scaling) - Implement battery optimization - Comprehensive testing (real-world scenarios) - Beta release (iOS + Android)</p> <p>Phase 4 (Weeks 19-24): Launch - Production release - Monitor performance metrics - Iterate based on user feedback</p> <p>Expected Timeline: 6 months (single team, 3 developers) Expected Cost: $450K-600K development + $180K/year maintenance</p>"},{"location":"resources/mobile-frameworks-comparison/#references","title":"References","text":"<ul> <li>Flutter Performance Best Practices: https://docs.flutter.dev/perf</li> <li>React Native Performance: https://reactnative.dev/docs/performance</li> <li>Apple Developer Documentation (Swift, CoreML): https://developer.apple.com/</li> <li>Android Developers (Kotlin, CameraX): https://developer.android.com/</li> <li>.NET MAUI Documentation: https://learn.microsoft.com/en-us/dotnet/maui/</li> <li>Movement Chain AI Technical Requirements (2025)</li> <li>Mobile Framework Benchmarks 2025: DevOps Report</li> </ul> <p>Document Version: 1.0 Last Updated: December 2025 Maintained By: Movement Chain AI Team</p>"}]}